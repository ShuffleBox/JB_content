we were getting ready to start the show. And it kind of took like a philosophical angle. And I
said, hold on, save it for the show, Alex. You said you've come to a life decision.
So I think we should all prepare ourselves here on the show.
Well, a little bit. Yeah. I mean, we spoke to Matt as part of the Adventurous Way YouTube channel
a few episodes ago, and I've been watching his progress on his Vermont property about building
itself, building his house and everything like that. And, you know, I'm living in a very nice
house in suburbia in Raleigh. It's very nice. On paper, I have no reason not to like this house,
except for the fact that at least once a week, I get woken up by my neighbor's lawn crew at like
seven, seven 30 a.m. Such a first world problem. But when you work from home and stuff like that,
like, legitimately, I'm not always awake at that time, even with, you know, barely a one year old.
I mean, she does wake up often before that. But just the amount I don't think I'd really
appreciate it before I moved to suburbia, just how much yard work people do. Leaf blowers are
I'm developing a pathological hatred of them. Let's put it that way.
Boy, I got to say, I don't think I've been I don't think I've been happier to hear something
in a long time because I thought I was the only one and I kind of felt like a crazy person. No,
man, it's the worst. And when you're trying to record podcasts on a daily basis, I don't know
what it is, but it's like it's like people have gotten tool crazy and there's a competition in
my neighborhood to have like the loudest, most powerful, most gas powered tool possible to do
the job. It's those two stroke engines. And I know that even if we do move somewhere else and
this is the decision that we've come to is that we're probably some point next year going to look
towards buying a piece of land, maybe in the Appalachians somewhere, although that's totally
up for debate. But the decision we've come to is that in order to facilitate the land purchase,
we're going to buy an RV. Oh, I thought you're going to say Bitcoin.
Oh, I see. Like you're going to do like a tour, a land tour. Well, this is it. You know,
the Appalachians, the bit we want to live in anyway, is probably that right at the southern
tip, sort of Knoxville, Atlanta, that sort of that sort of side, you know, not too far from
the tail of the dragon, actually. And that's five, six hours drive from here. So if we wanted
to go and look at a piece of land that just came on the market, say, you know, that is a long day's
driving. Whereas if we had a van, we could park it down that that neck of the woods, leave it down
there. And then we don't need to worry about hotels. We don't need to worry about all sorts
of stuff. And then if and when we do buy the piece of land, we've got somewhere we can actually park
the RV and kind of stay a bit off grid and all that kind of stuff really is what we're thinking.
So, you know, I like it because you are a work from home guy. And and what work from home really
means is work online. And so, you know, one of the things we could totally do is deck your van
out or whatever it is with a solid internet connection and good Wi-Fi, because, man,
if that's not an area that I that I haven't spent years figuring out now,
I feel like I could launch a consulting business just on mobile connectivity,
but we could totally get your rig dialed in and then you could work from the road
and, you know, kind of make that kind of thing a little more doable so you don't have to take
vacation all the time when you're doing it, too. Well, that's just it. Yeah, we're not looking for
like a lady dupe size type thing, you know, like a huge bus type thing, but maybe a class C or
something like that. Like in England, it would be called like a camper van or something, not like
a sprinter size at the next size up, you know, medium size. So we could still drive it into
national parks and stuff like that in the future if we wanted to, you know, not take up half the
parking lot. I'll tell you what, you might want to get in the Starlink queue now. Yeah, that's what
I'm hearing. It sounds pretty bad. I'm pretty grateful I got it when I did. And then not only
not only am I grateful that I got in the queue when I did for Starlink, but did you see that
they updated the Starlink dish and now it it is smaller and I suspect uses less power, which
sounds really nice, but it doesn't come with built in Ethernet anymore. That's like a it's
an additional purchase and I think it might even be like a USB Ethernet deal now. So how does that
work? Isn't the whole point that it just plugged into a firewall or some description? I mean,
that's what made it huge for me, right? Because I'm a Starlink is one of four different simultaneous
possible Internet connections that I have. And so being able to manage that in one router is very
nice for. Yeah, you have four different Internet connections. I'm counting a Wi-Fi connection with
that. But yeah, but that is true. That's pretty baller. It doesn't mean they're all none of them
are faster. That's I have sometimes played around with combining all of them, but weakest link and
all that. But you know what I mean? Like it is really nice to just I run Ethernet from the back
of Dishi right into my Pepway. And for me, that is that was such a nice thing for me that they
that they did that because essentially that means that dish. Is not only doing all of the work of
connecting to the Starlink network. But it's also doing the DHCP relay. It's like it's doing the
modem business, too, and their router, while a pretty good little router. That ran open WRT
is is actually unnecessary unless you don't have a firewall already, but now they've made that
little router necessary. So there's one more component and I would not be able to manage it
as well. So I'm glad I got it from that from that point, too, but I do worry it means when they do
finally release their mobile RV slash boat dish, which they are working on, they'll probably have
a similar setup, which is probably what I'll have to switch to. Well, I'll buy the V1 stuff off you.
There you go. There you go. You heard it right here. Hey. All right. So you text me during the
week and I was I was out doing stuff and I got a text saying, hey, hey, Alex, hey, hey, hey, Alex,
a hedge doc is down. You know, that's what we use for all of our internal show notes and stuff like
that to collaborate. And you were just warming up to record lap when it is Tuesday morning here in
the Pacific Northwest. Busy morning for us. And of course, that's always the worst time to have
something go down. Right. Because everybody's coming online here. Wes and Brent and I are all
kind of getting online in the morning and we're looking at the doc and it's not working. And
this happens when you self host things. I mean, it's nothing's 100 percent when you use a service
that is hosted by a company either. Because, you know, when you think about it, all they're doing
is self hosting it, just perhaps, you know, putting a bit of a veneer in front of it for you.
And you would hope some standards and good practices and good organization behind the
scenes. Maybe. I don't know. I suppose. Yeah. You hope they have some infrastructure as code
and you hope they have a support system. I have all those things. You know, I am your SRE in
effect. And it still happened. Yeah. So what happened was we use on the node. We have an
external disk which I formatted to ZFS so that I can do all the remote replications from the cloud
of all the databases and stuff, backing up things like hedge doc and the pastebin that we use and
all the other self hosted stuff that's on that server. It comes back to my my disk. So I do
replication with ZFS. Turned out for some reason, my Gmail password that was doing the notifications
of disk being full had stopped working and I hadn't noticed. And so I logged in to have a look
at this disk on my phone via tail scale, by the way, that was a huge, huge boon for tail scale.
Like I hadn't set up the wire guard profiles on my phone after downgrading it to Android 10. I
talked about in the last episode and I was like, oh, no, how do I get access to this box? And
the tail scale saved the day. So anyway, the ZFS drive was full. So in the end, it was a simple
matter of logging into the Linode web interface, adding an extra 50 gig to that disk, which is just
so easy. I mean, I did it on my phone in, you know, I was watching my daughter in the tumble gym
through the glass. And I was trying to do it all on like juice SSH trying to do all these, you know,
extends FS, blah, blah, blah, blah, blah. And, you know, in the end, I sat down with a real keyboard
at home and 15 minutes later, the problem was solved. But let that be a lesson, kids. You've
got to set up monitoring, alerting, but also monitor your monitoring because if that ain't
working, if your alerting isn't working, then you're going to get a text from an angry Chris.
It's funny that like a couple of outages that we've talked about on the show, both were like
Google account related. That's not lost on me. That's kind of interesting. But the other thing
that I've been thinking is like, I wonder if we could convert the back end storage for HedgeDock
to object storage. That's what we've done with a couple of other things like next cloud. And the
only downside is you got to watch your disk usage because it'll just grow and grow and grow. And
the great thing is object storage will just grow and grow with it. But you don't want it to get
out of control because that could raise the cost. So there's but, you know, we're talking markdown
documents when it comes to HedgeDock. So you wonder there a little bit. But yeah, and we do
kind of take note. OK, yep, that's one more thing we need to add to the monitoring setup. OK, yeah,
we missed that one. You know, we were we were able to actually scramble and just use the public
instance of HedgeDock. I realized like, well, I guess I guess I could go use that. And by the way,
if you're not using HedgeDock and you you like the collaborative editing features of Google Docs,
but like something that does markdown, man, as a team, do we just use the crap. It's so solid as
well. I mean, there was nothing wrong with the app. It was just out of disk space. Yeah, I have
been on the other side. I haven't really had an outage so much. I mean, you could chalk this up.
This is truly maybe a bit of self-hosting regret. Like out of all the things I've self-hosted
recently, I pretty much feel like they were all a solid decision except for Matrix,
specifically the Synapse server. And I think that if I was just setting up a Matrix server
for the J.B. team. Or like my family or friends or a small project, it would be problem free,
practically, man, it'd be really easy. But that's not what I did. I set up with, of course,
the help of Wes, a Matrix server for the Jupiter Broadcasting community at Colony.JupiterBroadcasting.com.
So Matrix is this new open source implementation of. Well, gosh, it's hard to explain because
Matrix itself is not a chat. It has chat clients like we have web clients, right? So there's
Element, which is a Matrix client. And in the Element Matrix client, it's a lot like Discord
or Slack. If you're familiar with those kinds of group chats or IRC of days gone, but a little
more, a little more modern with a lot of the inline chat features that people expect. And
on top of that, there's other features like VoIP calling and file transfer and all kinds
of things. Matrix itself is a protocol with a series of functionalities. I mean, you could use
Matrix. It's a big thing though, isn't it, is that it's decentralized. Yes, and it's federated. So
there's a Matrix.org server. We have a Colony.JupiterBroadcasting.com server. There's a Fedora server.
There's a GNOME server, much like you'd have your own IRC server. You can run your own Matrix server,
much like we have a Discord server, right? But that Discord server runs on Discord systems and
it's managed by Discord. The difference with Matrix is you get all that Discord like functionality,
but you host it, you run it. But a federated chat system like this is no small task because it has
to be aware of what the rest of the federation is doing. You have to have user accounts that can
exist on both systems. And what ends up getting exposed is a lot of paper cuts for managing the
system. Like here's an example of just what happened this morning. And this isn't a big deal,
right? But this is just what happened this morning. I got a message saying, hey, Chris,
I noticed that the logo image, the PNG that you're using for your Matrix space,
it's too large for some servers. And so we're getting reports that your logo isn't displaying
for a fair amount of users. And, you know, that's like, oh, OK, well, I guess I'll upload a smaller
image, right? But there's really no standard there. Different clients have been different
restrictions. Servers have different restrictions. And it's a great example of a daily little task
you have to do, like some little small tweak or adjustment to participate in the wider Matrix
community that probably should just be handled by software. Like I should be able to upload a five
megabyte PNG and server side software resizes it. And maybe it resizes it into three different
sizes that are common sizes, right? Or it says it kicks back an error message on the client.
And it says, hey, man, upload a smaller picture. Here is some suggested file sizes. But you get
none of that. What you get is you upload a picture. Turns out that picture is too large.
What's the appropriate size? Don't know. Just depends on what different people have said. So,
you know, take a good guess and then just find out it doesn't work and then go back and fix it.
And it's just sort of this unrefined paper cut experience. And it's a thousand different examples
of that. And even with a team of moderators, Wes and myself, I still get I get looped into like
daily jobs to take care of. And some I've been postponing now for weeks because I just don't
have time. Meanwhile, we have three different Discord instances out there. You know, the self
hosted one, there's a JB one, there's an unfiltered one. And I probably I probably have to do two
things a year with those instances. Right. I mean, the same thing there. We have some community
moderators and folks like yourself, Alex, that are involved. But in terms of like my involvement,
it's like two things a year for each instance where with Matrix, it's damn near daily. And that
really just doesn't scale very well. And it clearly is an ongoing server maintenance task as well,
because there's quite a bit of storage that we were constantly compensating for because
we have to store images and videos and stuff that people upload and chat logs and all of that and
storage or whatever it might be. The Synapse server itself is under frequent development. So
even like right now, I need an update. I just updated it last week. There's an update right
now for it. Right. Not a big deal. In fact, it's been really smooth. It hasn't kicked out any
problems. And we have it all in Docker containers. And the upgrade has always gone really well to
its credit. And there are server side tools to help minimize the database and whatnot. But again,
you have to go employ those tools yourself on the command line manually. And it's just
disheartening because as a self-hosting advocate, it's so clear to me now that I've been running
Matrix for a little while, a Synapse server, that it is exponentially simpler to just go the
Discord or Slack route than it is the Matrix route. And when the core functionality of what
you're looking for is just an ability for you and your community to communicate, it's so much
simpler to use something else besides Matrix. And I hate saying that because I want the free
software, new standard thing to win. And I love that you can run your own and it's federated.
So I think what my advice would be for people out there that want to self-host a Matrix instance,
is most of the time what you want is something small for your team that uses your domain name.
Don't open it up to the world. For people that want to participate with you, but don't yet have
a Matrix account, have them just sign up at matrix.org. That's what I should have done.
I should have set up a Matrix server with a small team, just the JB team. And then all the rest of
the community just went out and got matrix.org, got accounts anywhere else they wanted, like they
could already, or their own Matrix server. Instead, we opened it up to the world because
I wanted to make it easier for the audience. And in doing so, we created something that has nearly
daily maintenance tasks required. And I mean, just not everybody wants to sign up for that.
Thankfully, we are willing to participate in that because, again, I think this is an important
thing. And I really think it's something like some projects could manage or companies could
easily manage. And there are hosted solutions out there as well. But I do regret the way we deployed
it. Looking back at it, the jupyterbroadcasting.com Matrix server should have just been for
jupyterbroadcasting.com staff. And then we would just connect using the federation system and have
everybody else participate. Because we already have people from all different kinds of servers,
their own, self-hosted, or hosted, and they're participating in our chat room. And I could have
just done that for everything. I'm not going to reverse course now. And I still appreciate the
ability to learn from it. But I do have some regrets. I think you just highlighted the exact
reason why I went with Discord. It was a bit of a dictatorship style decision that we took to say,
right, this is the self-hosted podcast, and yet we're going to make that compromise and go for
Discord. But I think you've just articulated very, very clearly that we made the right call
with that server. As I mentioned at the beginning of the show, we had an outage with some of our
production infrastructure that we used to make the shows this week. And the last thing I really want
to be doing is futzing around with that stuff when it's not a planned situation. An outage or
something like that. And if JB's primary audience communication community platform,
for the show is down, shows are down, then who's got to get out of bed and fix it? It'd be one of
us, me, you, Wes, somebody like that. There's nobody else to do that work. So I just, at the
end of the day, we are making podcasts. We aren't necessarily system admins. And it's just a
compromise. It's not a perfect situation. And I think you've just hit the nail on the head.
But on a happier note, Linux Unplugged 4.33, you, Wes, and Brent covered Jellyfin. And I'm sorry I
couldn't make it that day. There was just a lot going on with work and stuff like that. And it's
during the day for me, so I couldn't make it. But I have been trying Jellyfin for the last couple
of weeks. And I've got to say, it's really come a very, very long way. So thank you very much for
putting that episode out. And anybody in the audience that is a Plex diehard, like I am,
really, and has dismissed Jellyfin as not being fit for purpose, if you haven't looked at it in
at least a year, I would strongly suggest that you do because it's come a very long way,
particularly stuff like the Android TV client. It's not perfect. I've had a few crashes, a few
stutters here and there. But on the whole, I would say for people like me who are willing to put up
with that stuff, Jellyfin's pretty much there. So LinuxUnplugged.com slash 433 to go and hear what
the rest of the team think about that one.
Linode.com slash SSH. Go there to get $100 in 60 day credit on a new account. And you go there
to support this here show. Linode is fast and reliable cloud hosting. You've got to try it,
maybe for your next project or use it as a research and development lab like I have been this week.
And honestly, Linode, you got me because of the performance. I mean, when I first started using
Linux before they were a sponsor, I didn't have like an obligation or a sense of like, I got to
do it here. No, it was just I chose Linode because I compared it to other things and they had the
best performance and the best prices and the best dashboard. Now it turns out they also have the
best customer service and they love the technology all of this is built on. So that sort of permeates
the company culture. That's stuff I've learned now a couple of years later. It's really great
to to watch Linode go. They've been doing this for 18 years and they are not staying still.
They have been rolling out PCIe MVME storage and it is a game changer for certain types of
applications. If you think yours might be one of them, like a database or maybe an e-commerce site,
something that needs a lot of access to the disk, simultaneous connections, that kind of stuff,
MVME can be so great. Way more IOPS per gigabyte than traditional storage. So if you're a
performance hound or you've got an application that you think could really take advantage
of Linode's new storage, go to linode.com slash ssh and sign up. Get that $100 credit and now you're
a customer. Contact their customer support and start working with them on MVME storage options.
See what makes the most sense for your application. They have absolute experts there that can help
you figure this stuff out. And they have 11 data centers around the world. You're going to find
something that's near you, your friends, your family, or your customers. And they are their own
ISP. So the nice thing there is if you want to have something in multiple different data centers,
the connection between those data centers is screaming fast. Every single time I have a Linode
system and I do a system update, I just am so thrilled at the experience of downloading packages
because they do it locally. They cache the packages there. It downloads so fast, my terminal can
hardly keep up with the package updates flying by my screen. That kind of stuff, I just love it. And
they're just really simple to use too. Like you can go get something up and going like the new
next cloud release in like minutes. It's kind of ridiculous, really, considering how long it used
to take. Take it back in my day. Or you can do it all yourself. Then you can go to the next cloud
or you can do it all yourself. Then you can kind of have the back in my day experience.
Go learn something, go try it, maybe put it in production. Linode.com slash SSH.
Time for a Docker segment then. I don't think we've talked too much about containers and Docker
and that kind of stuff in a while. And you've been trying out Portainer. I just can't see the
point of a Docker GUI, but can you? I started to get it. This comes in all the time. I tried
two Docker GUIs this week actually. So my first one is Portainer. That's probably the most common
one that we get an email about. Or something goes by in Discord maybe. I hear about Portainer all
the time. And it clicked with me a little bit more this time, Alex. When I pointed Portainer
at systems that I haven't been using in a while, like somebody else maintains them,
and then I wanted to poke in and see what's going on. So from a discovering what's going on
and that kind of stuff, Portainer was pretty neat. Now, if you're not familiar with Portainer,
it is a web UI that sits on top of, it runs in Docker, and it sits on top of your Docker host,
or it can connect to a remote Docker system, either via the Docker API or their Portainer
agent, or just through direct Azure integration if that's your choice. And then what you get is
a management GUI that's pretty sophisticated. And it substitutes some common Docker parlances like
Docker compose files for Portainer stacks and things like that. And it gives you a way to
visualize everything going on Docker wise on your system. And you can look at multiple systems. So
that's one thing that's kind of nice is you can install Portainer like I did to my Compute Module
4 on my OpenSUSE Dumbleweed install. And then I added other remote Docker hosts to that Portainer
instance. And that's kind of nice, right? I like that. And I tried really hard to love it because
I thought, boy, wouldn't it be neat to just bring all of the JB systems into one Portainer management
instance and keep tabs on all of them. Because in total, I don't know, 30, 40 containers of
different applications that we're running. I would say 70% of those are for behind the scenes
software that we use to produce the shows, maybe 60%. And I don't know how diligent we are about
staying on top of those updates. We are good about the public facing stuff, but not so good about the
internal stuff. And so I kind of like the idea of just bringing it all under one management roof.
But man, you know, Alex, and I don't know, I know you don't have the love for Portainer either,
but I found it frustrating and tedious to use. You know, I have to say on my remote systems,
I have to enable Docker swarm. I have to install their agent. You have to have Docker of a certain
version. So immediately, that means some rail boxes are out. And I found the entire thing to
just be about 15 layers of lacquer on top of something that I just didn't need anything.
It was too complicated for what I need and also too restrictive at the same time. And it felt like
a lot of fuss to get it up and going. And I don't really see it scaling out itself long-term without
getting to like some paid version of Portainer, which I really have zero interest in doing.
And so I found myself sort of hitting the walls of using it within the first couple of days.
And I felt bad because we've gotten so many positive emails from the audience about it.
I really wanted to love it. I thought I'd figured out my angle on it. And at the end of it, I thought,
I'm not going to use this. I just don't like it. And I'm curious to know why it didn't stick for
you. Well, they do offer a paid service. You know, their business model from the beginning has been
their community edition is available for free. And then you have a business edition as well,
which probably has extra Chrome or support or something on it. Yeah, there seems to be several
features that are in the Portainer business only edition. But yeah, they have a community one that
does allow unlimited nodes. I think I'm still biased against them because in the very early
days of Linux server.io, I reached out to these guys and said, hey, you've got a whole bunch of
templates. Why don't you feature our templates on your like template store thing? And eventually
we worked with them and got the Linux server containers to show up in their official template
store. And then they just went dead on us. They just went quiet on us for no real reason that I
could ever discern and just quietly dropped support for all of our templates without telling us
anything. So I wasn't best pleased with them after that. And after a, you know, a project or a company
does something to burn you a bit like that, you know, they've got to go a long way to
restore the goodwill and good faith. And for me, I don't necessarily find a huge amount of value
in a Docker GUI anyway, because Compose does everything that I need to and some. Maybe that's
just making me out to be a bit of a curmudgeon troglodytes. I don't know.
Old man Alex.
Yeah, maybe. But you know, at the end of the day, it all it's doing is adding complexity,
you know, and this is true for a lot of GUIs, you know, Unraid is one of them, TrueNavScale is
another, OpenMediaVault, Portainer, a lot of these things that you can use to self-host different
services. You've got to learn their specific quirks and incantations of how that particular
developer expected you to go through their particular UX flow. And if you don't follow that,
you can very quickly end up in a situation where, oh, well, you should have just clicked this,
you should have just checked, or you could have, well, I didn't, okay? And now it doesn't work.
So, you know, the other angle is, it's not repeatable.
So this is what I was going to ask you, it's like, aren't we kind of solving the wrong
problem here with this? I mean, it's not to give you an opportunity for an Ansible rant, but
shouldn't we be solving this from a different direction?
Yeah, it's a solution looking for a problem, it really is. And you should have,
and this is, I'm just going to go in the closet over there and get my soapbox,
you should have all of your container definitions in a source control repository,
some kind of version controlled Git repository, to be honest.
There is no real reason why, even if all you're doing is just copying and pasting
your Docker compose file from your system that you manually go in and edit once a week,
once a month, whatever it is, and then just copy and paste that into GitHub and click save.
There is no reason why you shouldn't be doing that. And that will get you 99% of the way towards
doing what a lot of big companies do anyway. So, you know, I just look at Portainer and I lump it
in with a lot of other GUIs and I think, what are you doing that I couldn't do myself? And the
answer frequently is nothing. Let me see if this shifts your opinion. Alright, couldn't you make
that same argument for open sense? Like, imagine somebody who's super familiar with firewalls and
routing and all of that on Linux, like they just live and breathe it. They would probably look at
a solution like open sense and go, yeah, it's kind of the same thing. That's my opinion. See,
that's the perspective I was trying to take at it. Yeah, no, I totally agree. And it's for that
reason that I separated out my DHCP and DNS whenever episode it was a few months ago onto a
Raspberry Pi, which has been super solid and is living in version control. And now if I want to
update a DHCP lease, I just go in and edit the file and my continuous integration deploys the
stuff and off I go, you know, but that's not for everybody. You know, those skill sets, you know,
they do require a bit of knowledge about how, you know, CI systems work and all the rest of it. But
just editing a config file is pretty straightforward. And I think a lot of people would
be kind of amazed at how simple something like IP tables actually is to, to operate if they just
took the time to learn it. I just wonder if maybe the argument isn't spend that time learning
something like, like GitHub and Git or Ansible or some system instead of spending the time learning
Portainer. Not that Portainer is bad, but if you could learn some sort of configuration management
where you could deploy your systems repeatedly and you could make a central change and then deploy
that change. If you could take the time you spent learning Portainer and just learn how to properly
utilize something like change control and change management with Git, that would be a skill set
that would pay dividends for all your systems, right? And not just your Docker ones.
And here's another benefit, which may or may not apply to you in your specific situation.
But I wrote a role for Ansible that spits out a rendered Docker compose YAML file three,
four years ago, just, just about when I was emigrating actually. And since then, I've been
using that role almost daily to create new compose files, not just for my server, but for the servers
I have in England, the service I manage for JB, the service I manage for other people. And so I've
built essentially a self-hosting framework or a Docker compose kind of framework in Ansible.
And so for me, if I were to spin up a new server, I just transpose that role to that specific task,
insert my variables for that specific host. And I don't even have to think about all the other
stuff that I've set up over the last three or four years. That's a given, that's done. I never have
to think twice about what list of default packages I have on a specific box. Like I know that I've
got HTOP, I know that I've got, you know, all these other things on there. And that just truly is the
power of config management. I don't really expect to get onto config management in this
conversation, but here we go anyway. So this isn't a specific rant against Portainer in particular,
but more a rant about GUIs and non-repeatable UX flows and stuff like that. You know, if you have
to click through a wizard to set something up and then you don't get a repeatable experience at the
end of it, it's not for me. Backblaze.com slash SSH. Get peace of mind knowing your files are
backed up securely in the cloud with Backblaze. You guys know Backblaze. Now go get a free trial,
no credit card required, and support the show. Backblaze.com slash SSH. Unlimited computer
backup for your Mac or your PCs for just $7 a month. Your documents, your music, your photos,
your videos, your drawings, your projects, all of your data. And the nice thing is with Backblaze,
you can restore your files anywhere. They have web restore, they have an app for restore,
and if it's a lot of data, if you ever get in that position, they even have a restore by mail
program. You purchase a hard drive, they'll overnight it via FedEx. After you restore it,
you can return the hard drive for a refund. How great is that? And one of the things we've heard
from the audience when they started trying out Backblaze is the mobile app is just a great way
to get access to your files that you wouldn't normally have on quote unquote cloud storage,
and they're stored there securely. In fact, over 50 billion files have been restored for Backblaze
customers. How cool is that? So go get a fully featured, no credit card required trial at
Backblaze.com slash SSH. Visit Backblaze.com slash SSH so they know you came from here,
supporting the show, you know, all that kind of good stuff, and you get that 15 day free trial.
That's nice, isn't it? You guys know Backblaze. Now it's a chance to try it out 15 days for free.
Backblaze.com slash SSH. Go there, play around with it, and start protecting yourself from
potential bad times. Start today at Backblaze.com slash SSH.
All right, at the risk of my soapbox not quite being put away properly,
there was another GUI appeared on Reddit. I think it was this morning or yesterday.
Lazy Docker. Have you tried this one out?
I had to after I saw it take off in the Discord. I saw somebody going on about it and I thought,
let's go try it out because it's a simple Go app and it's a command line tool. So I put some
screenshots in the show notes if you want to see it on my CM4. And well, let's see,
where to go with this one. Alex, I actually kind of like Lazy Docker. First of all,
you can also get it pretty easily on the Mac or Windows. It's in Chocolaty and it's in Brew.
So that's really nice. And then, of course, if you're on GNU slash Linux, you just need to have
Go installed and you just run the binary. And what you get is an Ncurses style clickable interface
that lists your running services, containers, images, and volumes. And then so, for example,
you could click if you have mouse support in your console or if you're SSH from a desktop
that has a mouse, you click the running service, you can get your config information or you can get
your Docker compose file or you can get process information or you can even get like a top style
chart. I'm going to go and check this out. This looks really pretty cool, actually.
Yeah, I actually thought it was not bad because it's so simple because most systems already have
Go these days. You can just curl it right down to a box and run it. And what I got out of this that
I was hoping to get with Portainer, but what really got it with Lazy Docker was it was a
great way to jump on a system that I haven't been logged into for six to nine months and just get
an overview of what's going on, where the Docker volumes are at, what processes are in there.
And I have to say, when I did that, I discovered that one of the containers was constantly
restarting every few minutes and I hadn't noticed because it had been up every time I connected to
it and I wasn't actively looking at anything. I wasn't checking the logs for that container and I
hadn't gone in and looked at that particular one. I just was on the box and got an overview.
And thanks to Lazy Docker, I noticed that, oh, yeah, look at this one. This one's restarting
every few minutes. Let's go see what's going on. And I could see the logs and everything right
there. This is 100% the only use case for a GUI that makes sense to me. It's just that dashboard
aspect. You can look at a graph, you can look at whatever, and it gives you just that big picture
that a terminal kind of sort of sometimes gives you. But this has some nice extra features.
Yeah. And you could do it over an SSH connection, which I really appreciate.
Portainer, it's nice to be able to centralize everything and have it web-based and there is a
time and a place for a web-based tool. But there's also a very important time and place for something
that runs inside an SSH connection like you just recently did. Sometimes you just have SSH, you
don't have a lot of options and you need it to be there on the command line. And that's what Lazy
Docker did for me. And the fact that I discovered one of my containers was having an issue was just
kind of a bonus. And it's a great way to reconnect or I guess what, re-associate yourself with what
the hell's going on on a box if you haven't been on there in a little bit. I've got another one
for you that is part of Docker actually. Just log into a box that's running a few containers and
type docker stats. It's pretty much like htop or top, I suppose, for all of your containers.
It will show you the name of the container, the CPU usage, the amount of memory that's being used,
NetIO, BlockIO, all that kind of stuff. And also how many PIDs that particular container's created.
So I can see for example here that Plex needs 140 PIDs within that container.
Whereas say something like a SimpleEngineX container is like seven.
Yeah. Oh, this is great. I'm looking right now. Oh my gosh. Some of these are such hogs.
It's shocking how bad some of these are. The Java apps in particular. So the worst offender
for me, I've got BookSonic running and I've actually used Docker Compose's spec to limit
it to one gigabyte of RAM. So for those of you that don't know, BookSonic is an audiobook friendly
version of the subsonic Java-based server that is now like a decade old or more. And this thing,
it runs like a pig. I mean, it's using 900 megs of RAM to just sit there and do nothing.
You know what we need is, do you remember like there was the, maybe you don't, this might have
been before your time, Alex, but in the old init systems of Linux, of yore, there was like this
Xynet D. And what it was is it was like this master process that would open a port on behalf
of a service on your system, but to save resources, it would leave the service stopped until it
received a connection on that port. And then it would start the service in the background
and forward the connection and make it all transparent to the end client. So the first
time maybe it would be slow for the end user, but their connection would get forwarded once
the service came on and everything would connect from then on, it would be fast.
And what we need is something like that for Docker containers, because I have a similar
thing where I have containers that I use a couple of times a month, but they're running 24 seven.
Yeah. I mean, for me, that's stuff like invoice Ninja that I use to invoice clients. You know,
I do it on a batch once a month and then it's running for the entirety of the rest of the month
and there's really no reason for that. So I'm sure there are projects out there to launch
containers on demand. If you know of one, please let us know at self hosted dot show slash contact.
I guess maybe we could wrap up our Docker dazzle, if you will, with a talk about dozzle and a couple
of other things like the fact that Docker compose has some changes that come in. I mean, we could
just make it a whole Docker extravaganza. I don't know. Maybe we start with dozzle dot dev.
Yeah, this one's pretty great. Actually, it's a real time Docker container log viewer in the
browser. It doesn't require anything terribly special apart from mounting the Docker socket.
So usual provisos apply there in terms of security. You can mount it through a proxy.
That's probably the safest way to do that. But essentially what it does is it lists all
of your containers on the left hand side. It says you can see which ones are running,
when their last restart was, all that kind of stuff. But then you just click on the container
and it shows you the logs as if you were doing Docker logs name of service. And that's pretty
much it. That's all it does, but it does it really, really well. Yeah, and it helps you
search through the log and find stuff. It has this really nice real time search function that
is super useful. And after my experience I had earlier with realizing that one of my
containers was actually restarting frequently, kind of realized I could use something like this.
So I'm going to take a look at dozzle and and maybe, maybe I'll throw it on a machine.
I talked about this one on Linux action news, but it's worth noting here since we're kind of
on a roll. Docker compose is changing from like a standalone Python thing to inside Docker itself.
So I think the way it's going to work, Alex, is existing Docker compose commands will just kind
of map to the new Docker compose commands. But instead of it being like Docker dash compose,
the new proper syntax will just be Docker space compose because it's now inside the Docker app
and not a separate thing. Yeah, just as if Docker needed to be even more monolithic,
they are un-breaking out the separate project that is Docker compose and bringing it in house.
So I think this is a largely a bit of tech debt that they're addressing here.
If you go way, way, way back in the annals, Docker compose actually started its life as
a project named fig. And then it was renamed to Docker compose. And it became what we know today.
You know, in terms of this change here, not a lot is going to change besides the invocation that
you need to need to use. Although they do say that if you still use the hyphen, so Docker hyphen
compose, they're going to put in an alias that will effectively just it will just continue working as
you expect. And there should be full backwards compatibility with existing compose files,
although there have been some small changes to the Docker compose spec. So if you're seeing errors,
that could well be why. More than likely, though, it's probably your YAML indentation that you've
screwed up. So make sure you enable white space in your editor. It's still in beta. So you know,
take that for what you will, you know, here be dragons, all that kind of stuff.
But there are some nice things in it. You know, largely, though, like I say, it's a it's a tech
debt style, addressing tech debt style release. And there isn't much really in terms of new
features here for normal people. But the developers are very excited. Well, and it's really how it
always should have been. It is it does make sense. So for me, Docker compose was what made it click.
Because a simpleton like myself now has directories that are backupable that are just
loaded with YAML files that just describe my infrastructure. And that's it right there, man.
Like as soon as I went like, oh, and you know how damn handy it is to be able to just cat a file and
say like, where did I end up mounting? Oh, right. That thing points over here for storage. OK, well,
that's where I have to go on the file system. I do that crap all the time. So for me,
Docker compose was this massive upgrade. It took my occasional use to Docker from like every
now and then I'll fire something up to check it out to like this is how I deploy things now
because that Docker compose file, it is a plain text way for me to just visualize how an application
is built and what it uses. And I love it. And so I'm glad to actually see it become like an official
part of the main Docker app, actually, because it means it's going to be around forever and you
start to see support and things like Podman for Docker compose. So it's starting to become more
than just a Docker thing. And I think that's great, too. Now, if you want to hear more about
that, you know, the LinuxActionNews.com slash 209 where Chris and Wes break that down in a bit more
detail for you. I wrote a blog post a little while ago about how I can manage monolithic
Docker compose file. So I'm the type of guy that tends to slam my 30 containers all into one
massive file. I know some people have different preferences with when it comes to managing these
things through stacks, and what have you, but I found a feature that got added to Docker compose,
I'm not quite sure when, called profiles. And in essence, this allows you to address multiple
services within a monolithic file itself. I can give each container a specific profile, like a
tag in the YAML of like test or prod or something like that. And I can address all of my test
containers within one massive file, as if it were part of just a smaller stack. Probably limited
use if you're using stacks, and you know, what stacks do, you create a directory, you put the
YAML file inside that directory. And each compose file is in each directory is what's called a stack
and compose will treat that as like a separate entity. And one of the new features in Docker
compose v2 is that you can actually enter those directories and list different stacks, like PS
processes and LS and, you know, see what different containers are part of different stacks. That does
make compose even a bit more useful. But if you want to find out about the profiles, I wrote a
blog post. Good link will be in the show notes at selfhosted.show slash five nine. So as always,
a big thank you to our SRE subscribers, you make this show possible over at self hosted.show slash
SRE. We do every episode, we do a little post show where Chris and I talk about some
some fun stuff. What are we talking about this week? Well, Wendell sent you a KVM.
Oh, that's right. Yes. Yeah. Yeah, we're gonna be talking about that. I have such a such a goldfish
brain. Sometimes we talked about that half an hour ago. I got it right up here. But also,
we've got a live stream coming up as well, where Chris is going to start putting together his first
ESP based device with ESP home. Yeah, that's right. It's gonna be it's gonna be awesome. I've been
wanting to do something like this for a long time. And I was I was complaining Alex like, man, if you
lived here, I know you just be over here on we'd have this thing built like a weekend. And then
he's like, well, let's just jump on a call and do it. And then we thought, wouldn't it be awesome
to hang out with our members? We haven't figured out all the tech yet. But we'll have more
information soon, like in the post show, and we'll make a members post to to let people know what's
going on when we have it all dialed in. We'll do it before the end of 21. How about that as a
promise? I like it. That's a good idea. Goals, Alex goal. Alright, now if you want to send us in
your feedback, you can get a self hosted dot show slash contact. That's the place to go to get in
touch with us. And as always, you can find me on Twitter at ironic badger. There you go. I'm at
Chris LAS on there the podcast itself. That's at chill to add at self hosted show. What is it? I
don't know. I can barely say it. I'm kind of feeling like I'm done plugging the Twitter now
the jacks out. I feel like we should be out Alex. Alright, everybody, thanks for listening and not
going on Twitter. That was self hosted dot show slash 59.
