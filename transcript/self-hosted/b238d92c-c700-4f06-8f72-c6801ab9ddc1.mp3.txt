This episode is brought to you by the all new A Cloud Guru, the leader in learning for Cloud,
Linux and other modern tech skills. Hundreds of courses, thousands of hands-on labs.
Get certified, get hired, get learning at acloudguru.com.
This is a bit of an awkward one. Chris just messaged me and said that he's poured 15 gallons
of water all over his MacBook Pro for some reason. I don't know why he'd go ahead and do that.
So we have a backup host today. We have Morgan joining us from Florida. Hello, Morgan. How are
you? You know, a little hot, a little sweaty, but it's what I'm used to. Nothing compared to
the Pacific Northwest this week. Yeah, I can't complain about that. At least I have AC. Yeah,
yeah. Down in Florida, I mean, you kind of have to have it, right? So Morgan,
you last joined us on episode 18 to talk about doorbells. So does that make you our resident
doorbell correspondent? Well, seeing how the doorbell died on me and I think it was actually
the heat that killed it, maybe. All right. Well, let's talk about doorbells, shall we?
Last time you were on, you were testing one of the cloud-free doorbells for us. And how's that
been going? So a couple months ago, I would say back in April, I went to check to see if a package
was left at my door and I noticed that I couldn't connect to the doorbell. So I fired up the,
you know, Unifi app and I noticed it disconnected at about seven something in the morning the day
before. You check the actual doorbell itself, it still has power, it works,
but the camera doesn't work. So, you know, I go on the website, no real help. There's a forum,
because as you guys know, this is a generic relabeled camera. So I go on the website,
the EZViz website, and I start taking a look and there's like 10 or 12 different ways to save it,
to bring it back. Nothing. You know, I couldn't get it to come back. You know, we were super
concerned about heat and it just died on me. You know, I pulled out the SD card and I threw it in
there and, you know, I had it set to just record nonstop. And sure enough, like right at 7am in
the morning, it just died on me. You know, the SD card's fine, but it just died. To check that
it wasn't the transformer, I replaced the transformer, I replaced the electronic doorbell
itself. Everything was fine, just the camera. So I don't know if, you know, I just got unlucky.
You know, the model I had had an issue with it. I've seen other people talk about how they have
them in Arizona and they reach temperatures a lot higher than what I have, but unfortunately for me,
it died. Is yours in direct sunlight? It is in direct sunlight for the last hour or two of the
day. Luckily, my front door faces west and, you know, the houses are two stories. So with the
trees, usually the last two hours of the day and it doesn't get enough direct sunlight to get too
hot. But again, as we mentioned on the original show, it's hot to the touch, you know, so that
was a concern and maybe that's the reason why it bit the bullet. Well, I mean, on the flip side,
I've been using the EZViz branded version of this doorbell for the last year or so at my house.
And as you say, it runs a bit warm, but I've not had any issues with it dropping out or
stopping working or anything like that. I guess it's just based on your field research,
it's just a matter of timing. The downside is there's not a lot of options out there
other than these EZViz clones. RCA makes an EZViz clone and a lot of people are saying that the
firmware is actually a little bit better. You know, Wyze just came out with a camera and
unfortunately, you know, Wyze did a little bit of trickery. You know, there's a USB port that they
cover with a sticker that they don't tell you that's there. You know, there's a YouTube video
that shows you how to access that and it's actually really great for, you know, places like an RV,
but it's cloud storage only. You know, there's no SD card port. So you're in a situation where
you're really being forced into the cloud. And the only viable option that I can find is back to
the EZViz clone. And to be honest with you, I've run without a doorbell camera for a while because
I don't want to buy something that I know is not going to be good. So I may find myself again in
the next few days with yet another EZViz clone, or I may go with the RCA. The RCA one that seems
to be the one that everybody recommends. Yeah, it's interesting. Looking at the Wyze website,
they've started making pretty much everything. I've seen a smartwatch from them recently and
doorbell on top of the cameras and all that kind of stuff. So they're really expanding their
business pretty aggressively at the moment. And I'm not sure how to feel about it. Like they
obviously made their name with these really cheap, affordable $20 little cameras that most people
use. And I've still got a couple in this house that have got the RTSP firmware on them. They
work great. But the version threes, the company says the RTSP firmware is air quotes coming soon.
And that was six months ago and yeah, it hasn't come soon yet. So I don't know why is it kind of
on thin ice with me. So I'd be wary of going down that route just for that reason alone.
There's definitely a market right now for somebody to come in and make a doorbell that is just
bare bones. It does what you need, SD slot, powered either by batteries or powered by a transformer
and has the ability to be connected to stream. The future is getting you into that monthly payment.
And the more features they strip out, the better chance that you're going to make that monthly
payment. And it's really unfortunate and it makes home labs and self-hosting even harder.
I don't know what's going to happen if EasyViz stops selling the clones. I mean, that model is,
I think, a little over two years now. It doesn't look like they're looking to come out with a new
one. Planned obsolescence is a real problem. And I think one of the things we can do to kind of help
the environment in that respect from pointless e-waste is, I suppose on the one hand is consume
less, buy less stuff that we don't need. I mean, our doorbells have functioned fine for 30 years
as just a little switch with a wire going to a little bell in the hallway. The fact that we're
sat here talking about them having a camera in them and all sorts of circuitry and stuff when
there's a global chip crisis is very much a first world problem. But on the other hand,
I think these companies can do a lot more to help us help them really in having open devices and
open firmwares. And I saw some news today come out of the UK about this right to repair bill
that they've passed and how it's unbelievable. The right to repair bill they've passed doesn't
include laptops and phones, which are two of the most important devices that people upgrade
regularly. Fridges and washing machines and all that kind of stuff are important, of course,
because they're very bulky and refrigerant and all that kind of stuff. It's not endless to
manufacture this stuff. But yeah, I think there's some kind of ethical responsibility that these
manufacturers have to start playing the game. But it's all about where's the incentive? Let's take
Samsung, for example. If they make a smartphone and it lasts three years, they foot exactly
0% of the bill to recycle, to dispose of that item. They sell me the phone, I use it for a
little bit and I throw it in the bin. The transaction so far as Samsung is concerned is
I've bought the phone. There's no consequence for them if I throw it in the bin after a day
or after three years. I think until we have a way of getting companies attached to their
environmental footprint, if that's even the right word, the waste that their products generate by
failing prematurely, this cycle is just going to perpetuate. That's a good point. The other thing
you're taking into consideration is you have CEOs of major car manufacturers who are guilt tripping
cryptocurrency miners for wasting electricity without taking into consideration of how does
the electricity get produced that then powers the very car he sells.
I wonder who you could be talking about there. I think his name begins with E and ends with
Lon, right? Possibly. But that's another topic for another day. And I don't want to get attacked
on a Twitter account I don't have. Well, speaking of Twitter, Home Assistant announced this week
that they're going to be working on an official two year integration. This is pretty cool. So
two year the manufacturer are going to be actually writing software that is compatible out of the
box with Home Assistant. I've struggled to deploy Home Assistant. And it's not because I don't want
to use it. It's because I just can't make up my mind what product to go with. And I think the
biggest issue that I'm having is I want to host my own solutions. But I don't want to get vendor
lock in. I don't want to go with something that requires me to do a hack install. I want something
that's just going to be easy. I can take out of the box, I can connect it to my Wi Fi, I can wire
it into my device. And then I can go into Home Assistant, I click a wizard and it's done. And
I've talked to a lot of other people who are in the same situation. We have a lot of really
intelligent people who are out there that are very tech savvy, but they don't want the hassle
of having to configure XML files or JSON, or, you know, installing firmware, etc, etc. It is a pain
and it's the biggest roadblock that I see to get people to adapt. So to hear that somebody's going
to come out and design something that is going to work directly with Home Assistant is integrated
integrated into their firmware is phenomenal. What's really exciting about this piece of news
is that they say that they're going to do the cloud API first with a local API to follow.
Now, I love me a good local API, because I don't see why if I have a light bulb next to me on my
desk, why does it need to go all the way to a data center in outer Mongolia, and then come back to my
desk again? I mean, it's got to travel six feet. So why don't we just why don't we just do that?
Why do we need to go out and in again? So that's very exciting to me too.
Yeah, I totally agree. And it's interesting what you're saying about a local API.
When you got me hooked on Shelly's, and I wired a couple Shelly's into control some lights around
the house. The first thing I did is I timed it. How long does it take to tell Alexa to turn off
the light? Compared to how long does it take to open the app and I click the lights off and they
turn off. And these are seconds that we're talking about. But just the idea that it's instantaneous,
right? We're not having to transverse the internet. I have no idea who's looking at my packets,
what Amazon is trying to do with what I just said. But the idea that it's local, it's in your house.
If you lose internet, you can still control your lights. Can't do that when you're cloud connected.
We've probably got some people listening that are going, yeah, what was wrong with a light switch?
I mean, you could control your lights without the internet with a light switch.
But you can't automate a light switch particularly well. I suppose you could have a child and get
them to walk over and flick the switch. But that's quite an expensive solution really, isn't it?
Yeah, and I have six lights that are embedded in the ceiling where I watch our movies. And sometimes
I want four of them on, or I want the two behind me on. And previously, they were all wired to an
on and off switch. And I really don't want to be in a situation where I need to run six, eight
switches to get what I want. But I spent a day, ran some wires, installed some shellies. And
when I say turn off one, two, three, and four, they turn off.
Bada bing, bada boom.
A couple months ago, I finally decided that I was going to pull out the wallet
and make some purchases and build a home server. Now, I had a home lab that I previously talked
about before, but it was just that it was a lab. I tore it up, broke it a lot. It broke a lot.
So it wasn't something that I could reliably count on to host services that my kids
need every day to watch movies, TV, to listen to music, all that good stuff.
Really important stuff, right?
The things that keep them from driving me insane. So, Alex, you had a UNAZ that you had bought
and you didn't need anymore. So I graciously, for a decent price, took that off your hands.
And I set it up, got it all built, slapped in a bunch of spinning disks, set up merger FS.
Well, I didn't go to perfectmediaserver.com and I didn't read the directions. So I totally didn't
set up merger FS properly. And I found out that we really couldn't stream two or three
movies at a time because they just weren't handling the bandwidth. And I was like,
what did I do wrong? And then I realized, genius, everything's going to one disk.
You're trying to stream a bunch of movies at one time. People are trying to listen to music.
It's just not going to work. So I had to find something that would let me properly rebalance.
And off to Google I went. And so I fell upon merger FS-tools. It's over on GitHub, fantastic,
easy to set up, clone the repository, and within seconds, everything's being rebalanced across my
drives. Yeah, these tools are really great actually. And there's a few of them. It's
mergerFS.ctl.fsoc.dupe. And the first one is mergerFS.ctl. This is a wrapper around the
mergerFS ex-attra interface. Essentially what this lets you do is add and remove drives as well as
get info about the array that's in there. And print things like your version and your mount
point and stuff like that. Pretty useful if you need it. Probably most people won't need that one,
but it's there if you do. The next one is mergerFS.fsoc. So mergerFS, FS check,
audit permissions, ownership of files and directories in a mergerFS mount. And then
one of the great tools that I really like is mergerFS.dupe.
No, no, no, hold on, hold before we sweep this under the carpet, we've got to clear this up.
FSoc or FS check. If you have an opinion on this, let us know at self-hosted show on Twitter,
please.
I don't know. I think FSoc sounds a little dirty.
FSCK. All right, mergerFS.dupe. This one duplicates files and directories across different branches
in a pool. The selected file can be duplicated using the dupe option. So this could be useful
if you're not using something like SnapRaid for kind of parity type redundancy. This one would
literally physically duplicate a file across X number of disks.
And then of course, you have the follow up to that, which is mergerFS.ddupe. ddupe does as it
says, it finds removed duplicate files across pools. You can set up ignore, ddupe, and strict
options to target specific use cases. I can tell you this one I use right away because as I was
copying pictures over, I copy them from my Mac, I copy them from my Fedora box. And I knew that I
had them in multiple places because as they say, if you don't have three copies,
It may as well not exist.
The first thing I did is I ran the ddupe because this became my fourth copy. And there was no point
in having multiple of the same files everywhere. It was really quick. I was really surprised.
One of the things I like about the ddupe tool is by default, it doesn't actually delete anything.
You have to explicitly set execute mode, which for fat fingered people who don't read the docs
like me on occasion, that's a good thing.
And then again, as I originally mentioned, there's the mergerFS balance, probably the best feature
you're going to run into. It's going to make sure that your data is balanced equally across the
pool. You can set a defined range. I think the default is like 2%.
So the last one is mergerFS.consolidate. And this does the complete opposite of balance, really.
It takes things from multiple places and puts them and co-locates them all onto one single drive.
That one requires rsync. I think there's a couple of others that require rsync as well.
But obviously under the hood, it's just doing some rsync magic to copy that stuff around.
It's definitely great. Like I said, if you're running mergerFS in your home lab,
definitely check it out. I'm sure there's going to be at least one tool in there that's going to
help you out.
linode.com slash SSH. Go there to get a $100 60-day credit on your new Linode account.
Linode is the largest independent cloud provider, and it's the largest cloud provider in the world.
Now, no matter what skill level you're at, what technology stack you use, Linode can help your
ideas come to life on the web. If you run into any trouble getting set up, Linode comes with
amazing 24-7 customer support by phone or ticket, along with hundreds of guides and tutorials to
help you get started. Linode has an easy to use and powerful cloud dashboard, S3 compatible object
storage, cloud firewalls, simple one-click application deployments, super fast networking,
and so much more. Linode started in 2003 as one of the first companies in cloud computing,
three years before AWS and other enterprise providers. They're independently owned and founded
on a love for Linux. And I tell you, when you use Linode and you dig into some of their more advanced
stuff, by the way, did you know they will let you boot your own ISOs? Oh, yes. If you want to run
some esoteric thing from 20 years ago, you can try and do that on Linode. So go to linode.com
slash SSH and get a $100 60-day credit on your new Linode account. That's linode.com slash SSH.
One of the downsides of having a home lab and then a media server is I needed permission to
buy it and I needed permission to where to put it. So the downside is it's next to my desk and
it's in a small little hole. And I have my three monitors and my Mac book, and then I have my
desktop for work. And I have so many cables, and I'm constantly in a situation where
when I need to hook up to the UNAZ, I'm unplugging a cable, same thing with the ESXi box,
and I need a solution. But I need a solution that really works. I don't want the old style KVM
where you're running all these cables and you're pushing a button. I want something that works with
a modern monitor, HDMI, and something that just works. Well, I was actually talking with Wendell
about this the other night because he did a video on Pi KVM and I sort of messaged him afterwards
and said, you know what would be really great is if I could take that single Pi4 that I've got
acting as a KVM unit and control three or four systems with it. I'm like, well, surely this
should be possible with an HDMI switcher, but I tried a couple in the past and they need you
to actually be physically present to push the button on the front of the damn thing to change
the input from system one to system two, three, four, whatever. But the really nice thing about
the unit that I've put in the show notes, it's a four port HDMI switch. It was about $60. And this
thing lets you use a keyboard shortcut to change input. So I just press control, control one,
control, control two, and that changes the input on the KVM switch. So how I've got it wired up is
the HDMI goes from the server out into the input of input one on the switch. And then it comes from
the switch output port into the capture port on the Raspberry Pi4 through the CSI bridge. Now,
I've written a blog post about this, of course, because that's what I seem to spend my spare time
doing. And I put a link to it in the show notes with a little diagram in case that is a little
confusing to follow by my talking, but it takes about five minutes to wire up and put together.
And yeah, it just, it just works, Morgan. That's the really cool thing. I can now switch between
my server and my other server and my Raspberry Pi3 that's doing DNS. And they all just appear
in the browser. It's like magic. Does it have the ability to record? Can you record the output?
I don't think it does. You can record macros so you can record different keyboard, you know,
sequences and mouse sequences and that kind of thing, but I don't think it will let you record
the actual stream, although it's a WebRTC based system. So I would wonder if there's some kind
of a stream that we could hook into with VLC or something and maybe do it like that. If you know
the answer to that again, write in and let us know at selfhosted.show slash contact.
Now you're much more of a sysadmin than I am. I would say you've got a bit of a background
working in all sorts of different places, cruise ships and hospitals and all sorts of places,
right? So you've probably run into email gateway issues when servers are trying to notify you that
they've failed. That's always been one of the fun parts about working in a data center. Most of my
employment has been with exchange and you always run into these systems that just need an open
relay or as anybody knows, open relays are a little bit dangerous in the world. So you're
always looking for a good solution for an SMTP gateway that isn't expensive.
Or you can do what most people do and just copy their Gmail password around onto various
boxes and have it connect that way. I mean, what's the worst that can happen?
Yeah, when you're working from your home, but what happens when you're working in a data center?
You're probably going to get that blocked.
Probably. Yeah. So I have an app pick for you that might tickle your fancy. This one's called
MailRise and this is an SMTP gateway for notifications. Essentially it plugs into an
app we've mentioned before called AppRise. So this MailRise application kind of sits in the middle
listening for emails, you know, syslog emails, whatever it might be, failures coming in from
various different system services. And when it receives the email, it reacts to that and then
connects to AppRise. So you can talk to any of the 60 plus notification services that AppRise
supports. So, you know, Telegram, Slack, Discord, you name it. For those of you who are new to the
show or possibly missed that episode, AppRise is a great tool that allows you to send notifications
to almost all the most popular notification services today. Think Telegram, Discord,
Slack, Reddit, for those people who are really cool. It's a one notification library to rule
them all. And what's great is the tool MailRise, as Alex said, sits in front of that and is an
SMTP gateway. So there's a lot of applications that really don't have the ability to have that
newer feature and MailRise gives you that by allowing you to send emails to the SMTP gateway
and then you can send it off. So when your laundry is done, you can send an email and
post it on Reddit to let everybody know I now have clean clothes.
Hooray for that. And that, you know, we live in the future. We can monitor Morgan's laundry
schedule via Reddit. And speaking of the cloud, cloudfree.shop has some exciting news this week.
They have released their version two smart plugs. So these are really cool. They've got
energy monitoring built right in. And of course they run TAS motor out of the box. And you know,
the best thing about the company, it's a small family owned business run by one of our very own
listeners. Now you can go to cloudfree.shop and use the coupon code self-hosted so that they know
that we sent you and you also get a dollar off per plug as well. So that's cloudfree.shop
with the coupon code self-hosted. Mike writes to us. Hey guys, love the show.
I'm a long time Linux user and administrator, but I have to admit I've fallen a little behind
on the whole container craze. That's understandable Mike. I'm very comfortable with VMs.
They always made logical sense to me. You have a virtual disk image, probably something with
some metadata in it, and you get to run as a full fledged system. Something about the whole
Docker runs X and it just magically happens without specifying any parameters just struck
me as creepy. Regardless, I've used them a little bit inside of firewall for land services,
but I'm wondering if there are best practices that either of you use. For instance, when you're
running multiple containers with persistence, where would you put the compose file? Do you have
any tips for organizing container volumes on CFS? I've looked around and I haven't found any good
articles. Just lots of run Docker compose up and magic happens, which is disconcerting in a
production environment to me. That's the beautiful thing of Docker is it is just magic. For some
people, that's exactly what they need. For your average person running an Unraid box or a small
Raspberry Pi who's not working in this stuff day to day, being able to do Docker run Plex or
whatever it is without having to juggle half a dozen bash scripts and eight different versions of Java
and all this kind of crap. That's exactly where Docker is going to win. Also developers, if they're
trying to do multiple different versions of an application development lifecycle all at once.
I've got three different builds and I'm trying to figure out which one is causing my production
systems to go down. It's very easy to isolate and roll back a container that way because it's a
declarative build. I understand there are some more old school people around that don't like
that approach because it is a little more opaque than you've been used to. However, I would say
the reason it appears opaque is because perhaps you haven't quite taken the amount of time you
need to look into where that container originated from. Sometimes you can go and look at the Docker
file in the open source community and see basically a Docker file is like a recipe.
It's a glorified bash script essentially and say app install this package, create this user,
install this particular library and on the whole those containers work pretty well. This is a
problem that used to exist that I haven't seen really be a problem for quite some time now is
a lot of containers used to get uploaded to Docker Hub without a Docker file. You had to just trust
that they weren't going to run a crypto miner on you or something like that. That problem largely
seems to have gone away. There are lots of different places you can go. Linux server.io is one
of them. You can go to obviously Docker's website as well for some documentation there.
I'm going to say something that's probably not going to be popular, but I don't think
containers are solution for everything. And it's Alex and I know containers are what puts roof
over our heads. And I will honestly tell you again, I don't think containers are the solution
for everything. You don't always need to build a server and install Docker or Podman and run a
container on it. Sometimes you can just install the DBN, the dev files or the RPM files and configure
yourself. However, what containers do is it makes it easier to install services that work together.
If you have a service that goes out and queries, it does a search for a file,
injects that download file into the download service. The download service then downloads
the file. Another service grabs the file, reads the metadata, processes it. That's going to be
really difficult. How do we know that all three of those services require the same version of libc?
How do we know that they require a specific version of go, et cetera, et cetera. You don't
want to run into the situation where you're constantly looking at dependencies. And that's
what containers do. So if you're doing a standalone box or standalone application,
yeah, doing the fat install is perfectly fine. But whenever you want to make sure that everything's
going to work together and they're not going to butt heads, that's the real reason why containers
were created. Now, to answer your question about Docker compose files, the way I do it,
and I'm sure Alex does it his way, is if the services need to speak to each other,
they're in the same compose file. I run 10 or 12 containers on my home lab. And if the service does
not need to talk to each other, it gets its own compose. That's the way that I've always done it.
There's no reason for me to do a Docker compose up with Adguard in the same exact compose as Plex.
They don't need to talk to each other. They don't need to be on the same Docker network.
They can live in different Docker compose files.
I once had a colleague tell me that Docker was a solution looking for a problem.
Back then, this was like six years ago. I got really hot and steamy under the collar about this
because Docker was the new hotness and I could see that it was going to solve all the world's
problems. And with a little bit of hindsight, I think I kind of agree with him in some ways.
It definitely has its use cases, but there are also places where VMs are probably better.
Now, coming to another part of Mike's question, he asks about tips for organizing container
volumes on ZFS. I just create a data set per app, Mike. It's as simple as that.
You can set quotas. You can do snapshots using Jim Salter's Sanoid tool. I have it set to do,
I think, four hours worth of app data, snapshots, and then keep a daily snapshot and a monthly and
an annual snapshot. The only thing to be aware of there is if you're doing something like InfluxDB
or some kind of really chatty time series container or database that generates a lot of data,
those snapshots will take up a lot of space because of how copy on write works.
So just make sure you're familiar with that concept before you dive into that one.
Ivan writes in, do you guys host your own email server?
And what are your thoughts on this? This might be a good idea for the show.
I can tell you on a personal note, I love Gmail. Why do I love Gmail? It's easy. They read my
emails. They tell me what I want to buy. It works on my phone and they give me a bunch of storage.
But I can also tell you my private email is for my family only. I don't give it out to anybody
who isn't related to me by marriage or by blood. And I run, therefore, my own email server. It's
been $5 a month. It's on Linode. I recommend everybody use it. Do you, I want you to contact
me for a job offer? I have a Gmail. Do I want to sign up for FPNL? Gmail. Do I want to send pictures
to my mom, my kids, personal email? I know that no one's ever going to get a hold of it. I know
that I'm not going to have to worry about getting spam because again, that's what Gmail is for. It's
for spam filtering. My email that I host is for family only. It's for things that I want for me
and only my family. And I recommend everybody to do the same. Again, it's $5 a month. Super easy
to set up. I like Gmail too. It just works. So Thomas writes in with networking on his mind,
I wondered if either of you have touched VLANs on home networks. I just got two Netgear smart POE
switches and I'm thinking about how best to segment my network. I've got a NUC with Fedora on it and
a Windows laptop. I've also got a NAS running some containers and I make use of WireGuard and Plex
on there as well. I've also got a Wi-Fi network that has a Google Nest and a pair of smart scales.
Thanks for an interesting show and inspiration for my home network. Thomas. VLANs in the home network
is an interesting topic. I think for 90 plus percentage of the world, you don't need them
until you start considering IoT devices and how insecure they are and who can get access to them.
And the second you have IoT devices that are a huge gaping security hole, you need to segment them.
And that's where VLANs come in. And that's the number one thing I do. My Wyze cameras,
their own separate VLAN and SSID. My EcoVac smart vacuum on its own SSID and its own VLAN,
because I want to segment anything on my network that is going to potentially open me up to attack.
The other reason why I use VLANs is again, as I mentioned, I have a home lab. I have all these
services running. Sometimes I have multiple THCP that I'm testing, et cetera. I don't want that to
conflict with my home network. So that's where VLANs come in. I use VLANs for DMZ, things that
I want to expose outside my network. VLANs are really about segmentation. Take what you need,
that needs to talk to each other and put it on its own VLAN. And that's the way you should think
about it. Somehow I've managed to avoid the temptation of VLANs this whole time, because it
requires a bit of extra hardware or a bit more advanced hardware. So you've got to have a couple
of switches that support VLANs and something to configure them at the router level as well. So
it's not just a plug and play thing. You've got to have a bit more about you to do it. And I've
just swept that one under the carpet and just opened up a bigger subnet and put things in
different subnets and just prayed and hoped for the best, really. That's been my approach.
I'm sure that most people who are running home labs, they have Ubiquiti, Microtech. A lot of
the equipment that we already have can do it. That's a good point. I think the biggest issue
is that people look at VLANs and they're a little hesitant because they're afraid of what it can do.
I can definitely tell you I've gotten VLANs wrong and I've made a mess of things,
but that's what a home lab is for.
Break some eggs. All right. Well, thank you very much for joining us this week, Morgan. It was a
very, very last minute that we had to call upon you because we generally record the night before
the show goes out on the Friday morning. So we didn't have much time after Chris's laptop
got flooded. Also, a big thank you to our members over at selfhosted.show.sre. You can go over there
and support the show. Obviously, we do appreciate that because it goes straight into the JB coffers
to support the network directly, so we don't have to have as many sponsors on the shows.
Speaking of which, we have to thank A Cloud Guru, and you can find them just about anywhere on
social media. They are slash A Cloud Guru on Facebook, YouTube, Twitter, just about everywhere.
So for all the different ways to go and get in touch with us, you can go to selfhosted.show
slash contact is the place to go to get in touch with us, and you can find me on Twitter
at ironicbadger. And I don't have social media because I don't want to be canceled.
Oh, well, I know you troll people on Reddit on a regular basis, but you can also find the show
at selfhosted.show. Thanks for listening, everybody. That was selfhosted.show slash 48.
