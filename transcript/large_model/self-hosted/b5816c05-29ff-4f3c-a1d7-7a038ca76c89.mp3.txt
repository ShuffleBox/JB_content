Coming up on Self-Hosted 26, we have Mike and Wes join us to discuss the new Docker news.
We talk about freeing your robot vacuum from the cloud.
And Alex really goes for the hard sell on Terraform.
I'm Chris.
And I'm Alex.
And this is Self-Hosted 26.
This episode is brought to you by a cloud guru.
Are you looking to get a high paying career?
Maybe move into the cloud and make some good change?
Well, there's no better place to start than getting a certification.
ACG has helped more than 2 million people scale up on the cloud.
AWS, Azure, and Google Cloud Platform.
Head over to cloudguru.com and get started.
So being a Seattle guy, you probably follow the ins and outs of Microsoft, don't you?
The new Flight Simulator is out.
Oh, is it actually released?
That's why I'm seeing so many screenshots.
I just wasn't sure if it was like extreme hype cycle mode or what.
But I've been seeing people take screenshots of the real world and then compare it to Flight
Simulator.
The hype is real.
They've used Azure and Bing Maps to kind of do machine learning and all this kind of stuff.
And obviously the first thing anybody does in a Flight Simulator is go and try and find
their house, right?
And I'll tell you what, I was able to follow the roads from the local airport near my house,
follow the actual roads, the highways, the small roads, everything, and find my house
in a Flight Simulator.
I could actually see it with my car on the driveway.
It was, it's amazing.
Was it amazing or did it feel slightly creepy?
No, 100% amazing.
Because I remember, you know, Flight Sim 98, 2000, I think 2002 was another one.
And I remember trying to do those things, you know, fly with visual references and stuff
looking out the window and, you know, it was just generated low res garbage.
Whereas now I can actually see and navigate with my eyes.
It's, it's amazing.
Imagine how cool this could be fast forward into the future where cars are driving around
with LiDAR sensors and they're mapping things in real time in a 3D universe.
And then they somehow collected information safely because you know they're going to.
And then they could use that to inform game design.
Now my brain just exploded a bit.
That is in 20 years.
So if you look at the generational leap from Flight Sim 2000 to Flight Sim 2020, Flight
Sim 2040, oh boy.
It's almost as cool as robot vacuums, which I'm wondering how is the robot vacuum life
going for you?
It's great.
You know, there's just a base level of cleanliness that you get from having these machines, these
robots going around.
But what I wanted was the ability to dispatch the robo vac to a specific part of my house,
you know, clean up on aisle five type situation.
And with Valitudo, I've been able to not only free my robo vac from Xiaomi's cloud
clutches, I've also been able to program in some pretty cool stuff like room based
cleanups in Home Assistant.
And it's pretty sweet.
Wow.
I got to see this when we're traveling again.
I have got to see the setup and then I've got to get you to do it at the studio.
Well, lucky for you, Chris, I've written a blog post on this on the topic.
Really?
Well, how lucky is that?
If only there was a place we could link that.
Oh, I know the show notes, Alex.
Yeah, yeah, yeah.
Well, it's pretty cool.
So it uses an open source firmware called Valitudo.
And then you can create in the, it basically turns your robot vacuum into an Ubuntu Linux
computer.
You can SSH into it and all that kind of cool stuff.
But it also presents a web UI.
And that web interface lets you drag certain zones and create and name certain zones based
on coordinates.
And then you can reference those names in Home Assistant automations, create vacuum
cards and with icons and stuff.
So for example, my RoboVac tends to run overnight, but our bedroom doors always closed.
And so what I wanted was, you know, 11 a.m.
or something once we're definitely out of bed and stuff is for it to go and do the bedroom
and 11 a.m.
every morning, it doesn't do the rest of the house.
It literally just goes and finds its way to the bedroom, cleans the bedroom and then goes
back to its charging dock all by itself.
It's so cool.
I love the screenshots in your blog post.
Definitely worth checking out.
We wanted to chat a little bit about Docker today because they've been in the news recently
in a way that impacts the enthusiast and I think the small business market.
And it's something worth chatting.
So we wanted to bring on two prime individuals to discuss this topic.
Mr. Michael Dominic from the recently returned Coder Radio podcast is joining us and Mr.
Wes Payne from Linux Unplugged to help us go through all of this.
Well, hello, gentlemen.
Welcome to the Self-Hosted Podcast.
Thank you for having us.
Thank you.
So let's set this story up, just recap in case people aren't familiar with it.
As of while we were recording about a week ago, Docker announced some pretty significant
changes to their terms of service.
So Docker, everyone knows, world's largest container technology and also image repository
at the Docker Hub.
They currently store more than 15 petabytes of image data.
And they write, after a detailed analysis of the container images stored on Docker Hub,
we found that 4.5 petabytes of the data had not been pushed or pulled within six months
or longer.
We are making this move to optimize operations and make the Docker Hub service even stronger
for developers and development teams around the world.
So the new policy is after a certain amount of time, they will automatically delete your
image from the hub.
So as of this recording, the default retention time for free plans will be six months.
If you don't push or pull within six months, they're deleting it.
And then there's paid plans that have longer retention times, which will probably all change
over time.
So to keep this kind of evergreen, I'll just be vague.
But you can pay to have it retained longer.
So this is upsetting a lot of people who don't frequently update their containers.
I'm curious what your thoughts are, Mike.
Thanks for having me, Chris and Alex.
And Wes, good to hear from you again.
So I have solved this problem in the most neckbeard freedom way by simply hosting my
own Docker container registry.
It's great.
DigitalOcean will give you a droplet for like 20 bucks.
I'm very happy about it.
Having said that, I think there's a little bit of a GitHub problem here where some folks
who are maybe on the business side of things are confusing Docker Hub with Docker, the
technology itself, right?
Docker Hub is just a service to host your Docker images.
And just like you would in Git, be able to easily pull them and update them and whatever,
where there's absolutely no reason you cannot use the open source Docker project, spin up
your own server running there, again, open source software.
And well, frankly, self-host it, right?
It's the name of the show.
I would strongly recommend if you're like me, and for instance, I have a few legacy
containers that are like older versions of Rails that I have for clients that while I
don't use them often, certainly not every six months, I do use them every once in a
while, they want to upgrade their physical servers, right?
And they need a new copy or a new image rather.
So I would strongly suggest, especially the JP audience, it is just not that hard to run
your own Docker registry and go for it.
An interesting angle that you see in a lot of the commentary online is this reproducible
build situation.
And when you look at most of the images that are on Docker Hub, some, not all, I would
say the percentage has increased over the last five years or so, include a Docker file
alongside the image that's been published.
Many people argue that those Docker files represent reproducible builds, but I disagree
quite strongly because the moment you have something like apt-get update or apt-get install
package blah, without pinning it to a specific version, it's not a reproducible build.
And so running that Docker file today will give you a different result than it gave you
six months to 12 months ago and so on.
And I think a lot of the outcry from certainly some of the areas of the community is because
we're losing that kind of historical artifact.
Now that is kind of offset a little bit when you self-host a registry because obviously
you're in full control.
But the lens that I'm looking at this through is my former project, linuxserver.io, where
we have tens of thousands of pulls a day from Docker Hub.
The bandwidth cost alone, even from DigitalOcean, who are very reasonable, it's just not
sustainable for an open source project.
Yeah, I think this is kind of where things get confusing because Docker has been and
still is a lot of different things from the sort of Moby-ish nebulous backends of how
you make containers work at all.
But then also this sort of community aspect, this common area that you could all share
at.
And yes, you can often go rebuild things.
But that's not where people first meet Docker, right?
Sometimes it's go build your first Docker file and build it that way.
But so many people, and Alex, I think you're spot on there, especially coming from Linux
Server, using the great Linux Server images.
Well, I'm just going to go pull down that software.
And it's more of an application packaging format of these binary file system layers
than it is anything about playing a role in a CI-CD system.
And that's where it kind of gets confusing because there's just a lot of stuff under
this one Docker name.
That's true.
And I do think it's hard to divorce Docker's success from Docker Hub.
I think Docker, while obviously it's possible to host your own images and have alternative
hubs and third party repositories, I think it was the combination of Docker and Docker
Hub and being so easy to invoke images from Docker Hub with very little command syntax.
I think it contributed pretty significantly to the success of the project.
Like I often said, what happens to the default matters a lot because it will impact the majority
of users.
And just to clarify for those that aren't really familiar with what Chris is talking
about, when you do Docker pull image, there is some code, basically hard baked into the
Docker software, that substitutes Docker pull image for docker pull docker.io slash image
colon tag, like short code for that default tag is latest.
So by hijacking effectively that root namespace, Docker by default captured so much of the
traffic and they must get absolutely hammered every day.
I mean, what did you say at the beginning, 45 petabytes worth of images.
I mean, that's a huge amount of data and they're storing this stuff on S3 according to my research.
And we know that isn't going to be cheap.
And so from a businessman perspective, I can absolutely understand why they can't continue
giving this away for free, but I look at some of the parallels between this and say GitHub,
for example, and how they used to charge for stuff and isn't docker.com.
What's the purpose of docker.com or the Docker Hub?
I think originally it was kind of that the first hit is free and we'll get people in
to buy stuff for the enterprise and we'll subsidize docker.com and the hub through this
kind of insidious, you know, namespace hijack.
But Docker Inc is now owned by Mirantis, I think, and I'm not even honestly sure who
owns Docker Hub now.
I can't tell you if it's Mirantis or not.
I think you're right about that because it is, you know, there is still Docker contributing,
but they sold off their enterprise business to Mirantis.
And so in some ways, the, you know, the GitHub journey, it's the opposite where now they're
a part of Microsoft.
They've got this big backing of a giant corporation and now Docker Inc is left as this sort of
small progenitor of, you know, the Docker tooling and the ecosystem, but without all
those lucrative enterprise contracts and is apparently just trying to focus more on the
tooling side of things.
But yeah, still has to pay for petabytes and petabytes of storage.
And there's the historical perspective here way back when we had them on Coder, when they
were still dot cloud, Docker was just a way for dot cloud, the business to sell hosting
services, right?
Like they made their money charging you for running your applications.
You know, honestly, Alex, I didn't even think of the open source project perspective because
I'm, you know, I guess still evil, but I'm thinking more of a small ISV where, yeah,
I mean, my images are relatively small.
It's just easy to throw up an instance, right?
It's certainly true that now that their primary business is not in fact hosting your applications
that yeah, these, these petabytes of traffic are gotta be killing them financially.
And you touched on it there and I kind of bring it back to the original story.
I think, I think it is small businesses that are selling some sort of slow moving piece
of business class software to a client or a customer that are going to be hit the worst
by this change because it's entirely possible that a back office piece of software that
runs a small doctor's clinic or an accountant's office may not get changed in three years.
And so it could easily trip the six month window.
And I could see if you're a small shop, you're just packaging things up into a container
for your clients and you're publishing it on Docker hub because it's simpler that way.
Some scenario to that I could see impacting small business pretty significantly, but I
think it's probably pretty easy for Docker to make the argument that if you're using
this for professional services, you should have a paid account.
I think I agree with you.
So to work around this issue, a lot of the online commentary was, okay, I'm just going
to write a cron job, a bash script that's going to pull this image every 5.99 months
or whatever.
Of course.
Love it.
But if you look at the small print towards the end of the announcement, the free tier
limits you to 100 unauthenticated pulls every six hours and 200 every six hours that are
authenticated.
So again, this is coming at it from the Linux server perspective.
If let's say even just a few dozen of those users all ran scripts within that happened
to run the last or the first day of the month or whatever, are you not able to pull images
at all?
Do you just get sorry rate limits exceeded or what happens?
And I think a lot of that fear for me is really centered around open source projects that
rely on Docker Hub.
Because my day job is working in the enterprise on OpenShift, which has built in registries
and everything's hosted behind the firewall.
I'm not really too worried about Docker Hub from a business perspective because like Mike
said, you just spin up your own and it's super simple and it's become the, dare I say it,
the standard packaging format for the server.
Yeah, NGINX's image, that's not going anywhere, right?
There's a company.
But you're right.
I mean, here in the terms of service, they reserve the right to enforce if they want
to.
And that includes stuff like quantity of data, age of data, pull rate and number of image
auto builds.
So it'll be interesting to see which projects they actually choose.
Is this really meant to cull the images that are never used anywhere or will there be large
successful open source projects that have been using Docker Hub as the way to distribute
their project but can't afford a non-free plan?
You have to wonder if this is the beginning of a different, more services based monetization
strategy.
You look at Docker and you look at the history of the project.
So we've been tracking this thing since what, 2013?
And you saw a lot of really rapid innovation and security issues be discovered and fixed
and overall system level changes to accommodate containers happen really between 2013 and
2015.
That's when the OCI launched the Open Container Initiative.
And that was really kind of creating an open standards body for containers.
And we kind of got this normalized approach.
I mean, since really 2015, 2016, there hasn't been massive accomplishments or changes or
innovations in Docker.
They changed the name.
Yeah.
There's been a lot of weird company stuff going on.
But I wonder if this is a bit of a hands up approach saying, well, maybe this is how we're
going to make money.
You know, they were pushing swarm for quite some time, but clearly Kubernetes won that
race.
And that didn't take off.
And so now they're left having to try to make, what's the point of Docker?
We already have these images, right?
We know that there's standards outside of their control and all their sort of retooling
around Moby.
A lot of that was just to also piecemeal things out and use a lot of things like Run C and
Lib Container, all the stuff that's also out there in the community.
So their role is just shrinking and shrinking.
Yeah.
And we know right now, as around the time we're recording this, they're raising money.
They just got a $1.3 billion valuation.
So they're on the market looking for an investor.
I kind of expected Microsoft to buy Docker for quite a long time.
Yes.
Well, maybe.
I mean, if you're buying TikTok, why not?
Sometimes, you know, the trying to raise funds initiative turns into a getting purchased initiative
that has happened in the history of tech companies.
So am I the only one who's maybe a little too simplistic about this?
I mean, when they were at dot cloud and Chris, you and I spoke to them, their business was
hosting your application and making deployment super easy.
Why can that not be the business today?
Right.
Because they waited too long and now there's tons of places to host Docker containers for
cheap.
Sure.
Did Heroku exist back then?
It did, but Heroku was very expensive at that point, right?
It was before Salesforce dumped a wad of money into them and said, lower your prices.
I'd argue that that's probably the biggest one click app competitor to that kind of model.
Absolutely.
Yeah.
And also, Kubernetes wasn't a thing.
Yeah.
And I know I'm probably slightly biased, but people like Red Hat hadn't woken up to Kubernetes
and VMware now have got their own Kubernetes and anybody who's anybody has a Kubernetes
play now and Docker swarm is just not relevant.
And I think if you look at the history of Docker as a company, their largest misstep
was around that kind of 2016 timeframe when they kind of made an enemy of Red Hat and
then Red Hat decided to make Podman and Cryo and basically stop shipping Docker.
We liked your idea, but we'll do it our way.
Yeah.
Alex makes a great point.
I mean, AWS, Red Hat, all the big services.
I think even Asher offers Kubernetes support now, right?
Yeah, they do.
Yeah.
Interesting.
So they're getting an influx of $75 million.
These are all according to people familiar with the matter.
The deal is supposed to close at the end of this month and it looks like they're going
to use the funds to hire salespeople, marketing team, and have them go after corporate clients
according to people familiar with the matter.
So Docker apparently sees the future in corporate.
By the way, what was interesting during this entire process, they've only increased in
a billion valuations since they were last evaluated in 2015.
Interesting.
You know, there's also sort of a lag in a lot of this stuff.
You know, they're still maybe smaller or just lagging behind enterprises that haven't made
the shift fully into the containerized world or just starting to play in that space.
And I wonder, you know, a lot of the developers I know who maybe aren't super into the technology
but need to use these tools, they've recently tried Docker.
They've been using Docker.
I think we'll still see the Docker name being used at least on the command line for a while
to come.
But it'll be interesting to see if, you know, eventually just the other sorts of tools supplant
them as the default.
It's the Kleenex of containers.
Exactly.
So moving on to Terraform, let's have Wes stick around.
Thank you, Mr. Dominic, for joining us.
And shift gears to news.
This is a news-heavy episode and shift gears to Terraform 0.13.
Now most of you are probably wondering why on earth this is even a big deal, 0.13 doesn't
sound that important.
Well Terraform have revolutionized the way that I deploy infrastructure.
So for those that aren't even familiar with what Terraform is, it's a way to declaratively
define infrastructure as code, much like we would do with a piece of software.
We would say, here are our dependencies, here is what, you know, the various different interfaces
we have and stuff like that.
We can do the same with our infrastructure.
We can say, I want this, you know, let's say a droplet, for example, I want this firewall
rule to allow traffic on port 80 to this IP address, et cetera, et cetera, et cetera.
And the benefits of doing this stuff with Terraform is that you can store it as code
in a Git repo.
So you can version everything that's happening.
So let's say that I am part of a development team at work and we want to, you know, change
the size of our default droplets from the $5 to the $10 one, for example, I can go in
and make that change, but I have to commit it to Git in order for that change to be picked
up by my CI pipeline and then push those changes to production.
So it's basically a way of enabling a paper trail for infrastructure changes.
Now the reason that 0.13 got me so excited is because I use it a lot at home for doing
homelab stuff with OpenShift.
And with 0.13, you can do something which doesn't sound like a big deal until you've
tried it.
Terraform has this concept of count.
And what that lets me do is it says, right, I want three web servers, count equals three
on my web servers.
Traditionally, that only worked at the resource level.
So each thing that you create is a resource.
Now I can define a module.
So what that lets me do is reuse different bits of code from across the code base.
Now I've written a blog post about this because some of these concepts are a little abstract
to explain in a podcast without making it sound super duper boring and training-y.
No kidding.
So in the blog post, I talk about count and modules.
And so what this lets me do is write reusable chunks of resource definitions that I can
then call from the parent module.
So I can create one file that defines how all of my droplets are created.
And then I can recall or reuse that piece of code from anywhere else in my code base
and pipe in a bunch of variables in real time.
It's super duper cool.
And if you aren't using Terraform or any kind of automation to create infrastructure, I
highly suggest you take a look.
Now Wes, you use Terraform a little bit, don't you?
I do.
When Terraform first came out, I was so excited because, especially at the time, I was using
a lot of AWS.
And after you have a few people, maybe a couple different generations of teams that have all
gone into the console, which as people who use it now keeps adding more and more and
more buttons to click, you've got this infrastructure that, I mean, yes, you can document it, but
it's a real pain because there's all kinds of different systems, different networks,
different VPCs, how many databases, how many instances did you spin up?
Were those settings the settings you meant to?
Were those just the settings that you applied at the time and they don't make sense anymore?
And with Terraform, you've just got all of that package for you right there, let alone
the advantages you can have because, well, the AWS API is different than the Digital
Ocean one, but it turns out I've got infrastructure on each of those.
I don't want to have to wrap all those API calls up myself.
Well, Terraform took care of that so nicely.
And you're right, I think one of my biggest complaints about it, it's doing a great job
and I like the configuration language, HCL.
It's declarative.
You get to say like, this is what I want and instead of telling the computer how to do
it, well, Terraform has modules, has functionality built into it to go build those things out
for you in the real world, but I have noticed having to repeat myself a little more than
I would like and made me wish that I could reach for a little more full powered language
or something.
So these changes, huge.
It's one of those changes that when it came out or was announced anyway, I was like, oh,
this is the way it should have been.
It's one of those changes that you just think, yes, this is a good one.
This is absolutely needed.
And you touch on one of the most important things about Terraform for me is how it basically
abstracts you away from the underlying infrastructure.
So I can write code that will target VMware and Amazon and Digital Ocean and Linode and
insert all of the other cloud providers as well.
I think KVM recently got some updated stuff for Terraform.
I think Proxmox might have done as well.
So there is no real excuse, in my mind at least, not to be using these kinds of things
to automate infrastructure deployments because let's face it, we've all been there.
We've all installed a server and we're like, how did that get there?
Six months later, how did I deploy this?
Which image did I use?
Which firewall rule applies to this particular thing or whatever?
And by having your infrastructure as code, it's there right in front of you.
The answer is always there.
So what's it going to take to convince Chris to use it though?
That's what I want to know.
That's a good question.
Raspberry Pi support.
I can't help it.
I have to.
Well, the thing is about Terraform is that it's at the infrastructure layer.
So it's more about creating virtual machines and the ancillary stuff that goes around them.
So I would say for configuring the Pi's themselves, Ansible is probably a better bet.
And I tend to use Ansible for configuring the machines once Terraform has brought them
up.
Right.
I see.
We often see a lot of confusion about which tools should I be using because I mean Ansible
technically can do a lot of what Terraform does.
I find personally that the delineation between one tool to create the infrastructure and
another tool to quote unquote, configure the infrastructure, I find that separation quite
helpful.
And the two, you can call them from one another as well.
Yeah.
In a past life, I was managing a system that provisioned a whole bunch of EC2 resources
using entirely Ansible and moving the parts of that that were just infrastructure specific
and not all the OS configuration out to Terraform made it so much clearer.
And it was just simpler because honestly, Terraform does a great job of keeping up with
all the AWS changes.
Oftentimes they've got stuff even before CloudFormation does.
So it's just a better tool.
It's a funny story.
I actually found out that DigitalOcean were releasing VPC support by reading the Terraform
docs before it was announced on DO.
I found it in the Terraform docs, which I thought was quite cool.
Well, I know we got to get going, but before we do, I think you ordered something new and
I think maybe you forgot you got roles swapped here, Alex.
I thought I was the super cool small board computer NAS guy.
Well, don't tell my wife, but I might have impulse bought a single board powered system.
It's the Helios 64 from Kobold.io.
And this thing is the ultimate arm powered NAS.
It has four gigabytes of RAM, a built in battery pack.
It supports five, three and a half inch hard drives, has a USB three type C input.
So you can use it as a DAS as well as a NAS.
And I don't know, like I'm not super duper thrilled about only four gigs of RAM, but
I think for a few media apps and a little bit of file storage here and there should
do the trick really nicely.
What do you think?
Yeah.
I can't wait to buy it from you secondhand after you're done with it.
Yeah, maybe we'll see.
I mean, I saw it and I thought of you cause it's got a couple of gigabit NICs.
It's got a few USB ports.
I think it's got an HDMI out as well.
So one of the screenshots they show on their website is of it running Kodi on your TV.
So that could be an interesting use case.
I like reducing down a little bit, the NAS and the TV box all in one.
You know it's going to play back good in that scenario.
Yeah.
It's also got a USB-C and runs off DC power.
And like you said, that built in battery, that could be really handy too.
I can't wait to hear your thoughts on this.
It ships in August.
So we're still technically in August by a few days here as we record.
So I don't know, come on guys, get it, get mine out the door, please.
I want to get, I want to tell the good self-hosted people.
Well, that brings us to the end of self-hosted.
Well, you can find our sponsor on social media, Cloud Guru is at twitter.com, youtube.com and
facebook.com.
They're all just slash a cloud guru.
Couldn't be easier.
Thank you to Wes.
You can find links to Wes and Mike's Twitter accounts in our show notes.
And of course you can go get more Wes Payne on the Linux unplug.
Anything else you want to mention, Wes?
Thank you for having me.
I'm off to my own web.
Thanks for being here.
All right.
That's self-hosted 26.
