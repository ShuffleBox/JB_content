Would I be a bad man if I ran Arch on the server?
You can't do that. Why not? You don't really do that, do you?
I might. No, I don't, but I've often considered it.
I stream a lot of important things from that server.
I have often considered it. No, I use Debian primarily.
However, lately I switched out to Ubuntu for the ZFS stuff.
And this was after coming to Linux Fest Northwest in April.
I listened to Jim and Alan Jude, who are very persuasive gentlemen.
They are. They really can convince you.
Yeah, and Ubuntu has ZFS baked into the user space tool.
All you need to install is a user space tool.
And in future releases, it sounds like it's getting even easier.
For me, though, you just can't beat the stability of a Debian or a CentOS
or something like that. Yes, on the server.
On the server. Yeah, I agree.
I think actually if I was going to pick one true OS to rule them all
for my personal servers, I would pick Debian.
Why? Because it is very much so out of many of the distros,
the most set it and forget it. Even more so, I think, than CentOS.
It's just my personal opinion, though. I don't actually run Debian.
I get a good sense of stability whenever I run apps update or apps upgrade in Debian.
There's maybe a dozen packages a week that change, if that.
Well, I kind of feel like this question is sort of irrelevant too, in a way,
because in the past, I would avoid using Debian because it was so slow moving
that the packages would be horribly out of date.
And so it was just not really a contender.
That kind of moved me up to Ubuntu and things like that.
But now I'm really, to be honest with you,
of the opinion that it just doesn't matter.
Whichever one you are comfortable managing.
And what matters more is how you deploy and run the applications
and how you secure them and how you manage all of that.
That's like what I think matters a lot more.
And then the OS is an implementation detail,
and it just needs to be one you're comfortable with.
Containers changed everything.
Yeah, basically. Or VMs too. But for me, it was containers, really.
Because VMs, you still had to answer that server question.
Because VMs, you still got to pick an OS.
Whereas a container, okay, the maintainer has picked a base image to base it off of.
It could be an image based on Debian or Arch or Fedora
or some other esoteric thing, Alpine or something.
But when you're able to separate the application runtime from the base OS,
you end up in a situation where, like you say, the base OS doesn't matter anymore.
The only time it matters is when you want to have things like extra file system support,
like Ubuntu's ZFS support.
And Debian is still a bit old and crusty.
And that's what makes it great, in a way.
Well, okay, just going to throw this out to play devil's advocate.
To play devil's advocate, just to have a rounder conversation.
What about Windows Server?
Well, what about it?
Well, you get disk management in the typical Windows disk management stuff.
You get some decent features with NTFS.
You get shadow copy volumes for your users data to be safe,
which is kind of a nice backup.
You've got plentiful application availability.
You've even got the capability of running containers.
And Hyper-V isn't that bad of a virtualizer if you want to run VMs.
Containers, in inverted commas.
They're still running a Linux kernel somewhere.
I know, but it works.
So, I mean, what if Windows is the OS you're the most comfortable with?
Because from where I've just positioned myself,
I've kind of backed myself into that corner.
Because if you're running the applications in a container,
it doesn't really matter at the host OS.
I was just that guy.
To a point, here's my counter to that.
Learning Linux is an incredible enabler from a career standpoint.
Mm-hmm.
You're investing in yourself by learning these things,
by learning about what SSH is,
by learning about port forwarding and opening things in your firewall.
You're doing stuff that is being done in the enterprise,
that is being done in business,
that you go to your next job interview and you can be like,
well, hey, I've done X, Y, and Z in my home lab.
Mm-hmm.
I run these services for my family.
I run an XCloud instance.
And then I had to set up a reverse proxy
that means I don't have to open loads and loads of ports.
I can just have a let's encrypt certificate
that does this stuff automatically.
And you can talk authoritatively in an interview on these topics.
And that, for me, is the answer.
Yeah, that was a big revelation I had not too long ago.
I was using enterprise grade production deployed tools
to run my home lab stuff.
And I just thought for a second,
it's been a long time since I've done that.
Yeah, that is very much what learning Linux felt like back in the day.
So that's a great point.
And I can't argue it because the skill set of the future,
I made this point on a Linux Unplugged recently,
is, quote unquote, the cloud.
When you look at open job searches right now,
there'll be a couple thousand for a traditional sys admin.
And then there'll be like 10,000 for a, quote unquote, cloud admin.
Cloud.
Yeah, well, the cloud's made of Linux.
Just someone else's computer, you know?
It is.
Well, it can be your own computer in the case of our preference.
This is something I wanted to ask you, what does self-hosted mean to you?
For me, it's having something on a box that I control.
I think that's kind of the root of it,
because I consider servers that I run on DigitalOcean to be my boxes.
Maybe that's not a safe illusion to be under,
because in theory, they could just go in there
and probably get access to that image.
They have root access to the box they're all running on.
So maybe I shouldn't feel that way.
But I do right now.
My current frame of thought is services I've implemented,
data I'm responsible for, and applications and security
that's totally under my control.
Yeah.
And there are a number of considerations you've got to make
when deciding where to self-host a particular service.
Do I host it in my garage or my basement or a closet?
Or do I need more uptime than I can guarantee with that
and put it on a droplet or an EC2 instance or something like that?
And then obviously, you've got cost considerations
when you come on to cloud hosting versus local hosting.
So local hosting, you're going to be spending a lot of money upfront,
probably at least $1,000 to build a system
with half a dozen disks and multiple terabytes.
Whereas the draw of something like a cloud service,
what do you call it, a VPS,
is that you can get started in, what, 35 seconds, 40 seconds?
And maybe an entry price of $5 a month?
Yeah.
Which is much easier to eat than $1,000 upfront.
And there's no power bill, there's no heat considerations.
Because I built a new server in September last year,
and I have some regrets, you know.
Really?
Mm-hmm.
I'd like to talk about that, because I went through a phase
where I went, I'm going to try to cloud all of the things,
on boxes I run, and have only dumb, small appliances in the house.
And now I'm swinging back to big rig, lots of disk,
draws a lot of power, but also provides a lot of benefits.
And I'm not having any regrets.
Well, okay, so let me be more specific.
I love having 100 terabytes on my LAN.
Right.
I love being able to...
Just think of the cost to do that in the cloud.
Well, but you wouldn't do that.
It just takes it out as an option, basically.
I wouldn't do that.
I mean, I needed somewhere to store my drone footage,
all of my photography stuff.
Mm-hmm, same.
And I'm sure you've got production requirements
for your storage as well.
But for me, looking at my power bill every month,
my server draws, it's a dual Xeon, E5-2690v2.
So they're pretty powerful CPUs.
They score like 15,000 on PassMark each.
So that is the minimum you want for a 4K Plex transcode.
And do you find that your CPUs stay fairly active,
like you are using that much horsepower?
Because sometimes people, you know, they overbuild,
which then gets you down to overpower.
And that's where I thought I could go all appliance space.
I could have just little small boxes of Raspberry Pi here
and Nvidia Shield there, and then have all of the horsepower
up in the cloud.
That was my dream.
I mean, it's tempting.
It was.
I'm looking at my server load here,
and it's currently sat at 11.
So, you know, 40 thread, 20 core boxes, CPUs.
So the work is kind of bursty when those CPUs are actually in use.
And otherwise, they're kind of sitting there idle.
Yeah, like when a Plex transcode comes in,
it'll burst the first 10 minutes,
and then it will just sort of blip, blip, blip
every few seconds, the next bit of transcoding.
Well, that's a tough call, though,
because you do want to overbuild at least a little.
So it lasts a while and has plenty of headroom.
Well, so here's what I found.
Like Plex has this wonderful offline sync feature
where I can effectively download and transcode media
from whatever format Plex sees it in to my iPad
or to my phone at 8 to 10x speed on these CPUs.
I used to have an Atom C2750,
one of these old ASRock boards, Atom 8 core things.
And that was wonderful.
It sipped power, had like a 30 watt kind of maximum power draw.
And that worked great when it was just me doing Plex.
But my new house, I have gigabit upload.
And so I was like, well, let's share the love a little bit.
Let's let, you know, you have access.
My parents, you know, in England,
and my uncle in Canada have access to this thing.
And you think, well, I want to be able to do at least,
let's say, 10 streams at once.
And then when you start looking at that,
there's a whole rabbit hole you could go down
of doing hardware transcoding with this new Nvidia stuff,
NVENC, which has come popularized
because of Twitch streaming and that kind of thing.
But does have some limitations.
Yes, Nvidia arbitrarily limit their GPU drivers
to two streams on consumer cards,
like a 1080Ti for crying out loud.
That's capable of doing 20 or 30 streams
and they limit it to two.
And really not all codecs and bit rates are supported.
So if you want something that's more flexible,
you still end up on CPU.
So you still need to account for that in your CPU build.
Yeah. And so going back a bit to my power bill,
I look at these dual Xeons and they idle at around,
I mean, there's 15 hard drives in this box as well.
Yeah.
But they idle, that box idles at around 300,
box idles at around 300 watts.
And now some rough maths that I use
to calculate what that costs me.
North Carolina, my power is about 11 cents per kilowatt hour
and you can approximate that to roughly $1 per watt per year.
So that server costs me approximately $300 a year to run.
Now, if I was to look at what I could get in Digital Ocean
or Amazon for that kind of money, wouldn't come close.
Yeah. Not in horsepower and not in storage.
And also bear in mind that you're getting the benefit
of raw metal performance,
not a virtual machine up on a shared system.
So I run Proxmox as my base OS,
which is based on top of Debian.
And then all of my application workloads
are within an Ubuntu VM,
which I use pass-through on to send
a couple of disk controllers through.
And that works great.
And I don't notice any performance penalty for real on that.
But I can't help escape this thing,
particularly with this new Ryzen launch.
I can't help escape the fact that I could build a system
with similar performance that only drew 100 watts from the wall.
And I've still got the new Ryzen 3 Zen 2 stuff.
I could have the same performance
for less than half the energy draw.
Yeah, but there's no way that math works out price-wise.
Since you've spent the money on the server,
it would take you so long to make that up.
Yeah, I mean, a Ryzen build is going to cost me at least,
what, $1,000, $1,500?
To be honest for you, it's probably closer to $2,000.
Come on, you know yourself.
Yeah, and you know, there's another thing
that this old enterprise gear has
that you can't necessarily get cheaply
on the AMD side right now, and that's IPMI.
So I could log into my PFSense box from here,
bring up the IPMI interface,
and remotely see what is being output through the VGA port.
I'm in Seattle, my server's in Raleigh, North Carolina,
and I can just bring up the display as if I was there.
KVM over IP, it's amazing.
That is, especially when you have family members
that are depending on it.
That is really nice to have that.
We use the word depending in inverted commas as well, I think.
The whole thing is a big math equation to me.
I look at what I expect this box to do,
how core to my, how, quote unquote, dependent am I on this?
How core to my daily use of my network is it?
I.e., is it doing DNS?
Is it doing DHCP?
Is it doing Samba or NFS, some kind of storage?
So then how performant does that need to be,
and how dependable does it need to be?
And then from that, I try to derive
how reliable I need the server hardware to be.
And then the only other kind of two factors
I have to figure there is how much CPU and disk,
because often the other things will drive the RAM requirements.
And I often end up on a mix of x86 Xeon hardware
with 64 gigs of RAM was kind of like my standard build right now,
and then a ton of disk.
And it is the same thing.
It's drawing around.
I don't think it's drawing that much.
I don't have as many disks as you do, but it does.
It does have a big cost.
I look at it as a business expense
since it is also file services for our team and whatnot.
But it was a hard calculation to make
because I did really like when I could go in lower power
because I often went that also meant less noise,
less heat, less costs, less complexity.
So I do kind of want to encourage like when possible,
don't overbuild if possible.
I've kind of gotten away with that so far in the RV.
I don't have any hard work equipment
that requires a fan or anything like that running in the RV.
It's all appliance space so far, but I can feel it slipping.
It's slipping.
You're not tempted to put a Pi in the RV then?
Oh, I got like an idea for like three different ones right now.
The Raspberry Pi fits in a weird place in all of this
because it's perfect to run small stuff
that doesn't have much IO.
But the minute you want to attach
more than one SATA type disk to it,
you have to have lots of USB converters.
And maybe with the four, we should do some testing maybe
because they've separated out the ethernet bus
from the USB bus finally.
Yep.
The early benchmarking I've done shows
it really makes a big difference.
I'm sure.
So I think the four is the first Raspberry Pi
that's truly ready to be a home server.
And I think so regardless of what hardware you pick
and regardless of what OS,
even though I think we both kind of recommend
if you're starting out, people try out Debian
or Ubuntu therefore.
Well, there are others.
Yep.
CentOS would be another good choice, I think.
Well, we haven't mentioned FreeNAS or Unraid yet.
Oh, I feel like we could have a whole conversation
on those suckers.
Right?
Yeah.
I mean, if you're just starting out,
Unraid I would say is possibly the easiest thing around.
Yeah.
Is that good though?
It's a proprietary piece of software
which you have to pay a license for.
So it's not completely free and open source
like our Debian recommendation would be.
And that's something that I struggle with quite often
when speaking to people in Discord or on Reddit
is trying to gauge you as a person.
What are you trying to get out of this purchase
or this build?
Are you trying to learn skills
that will make you more marketable for a job
or are you just trying to solve a problem
which is how do I stream media around my house?
You could also be somebody who just likes to know
how the things work
and so you want to build it yourself, put it together.
You want to know all the parts of the recipe.
You know, what is the right box for you?
Is it a Synology maybe?
Is learning Linux the right thing for you to do whatsoever?
There are so many options in this space.
It can be bewildering.
Bewildering as a new guy.
It's like we need some sort of show
that has time to discuss and explore
all these different options and help sort all of this out.
Something to help you.
Is that what this is?
Yeah, maybe.
Maybe something to help you navigate those complexities.
What a great idea.
I think my other checklist
if you're considering self-hosting
is you need to be willing to take on
some personal responsibility for security.
Absolutely.
You know, be willing to use SSH key authentication
whenever possible, don't use passwords
for any kind of login.
Be willing to explore isolating applications and services
either through VMs, through containers,
whatever works for you.
Also, where possible, restrict your file systems to read only.
There's no read.
If you don't need to have write access
or if you can easily enable write access to make a change,
consider that and also stay tuned for future discussions
on setting up reverse proxies
because that's going to be a key part of this too.
You got to be willing to deep dive a little bit
into reverse proxy.
And if you're willing to do those kinds of
self-responsibility items that aren't related to the CPU
or the hardware or the operating system,
if you're willing to do those things in addition,
I think you're probably a good candidate to self-host.
But the other thing is, right,
if you were to just buy an off-the-shelf Synology
or a QNAP or any of the other kind of NAS in a box
like Drobo maybe,
you're still going to have to figure a lot of this stuff out.
The minute you, for me, what woke my mind up to all this
was five or six years ago, I bought a Drobo,
returned it because it was expensive.
I've owned a couple of them.
And then I bought a Synology.
And then I sort of thought to myself,
well, I've got this thing sat there with blinky lights
that's on, what else can I do with it?
And that single thought sparked my entire journey rabbit hole
into, I guess, being sat here with you right now.
Now, was that before you worked for Red Hat or was that?
Oh, a long time, yeah.
This was, I was still working at the Apple Store.
Oh, okay.
I was a genius on the genius bar at the Apple Store.
You have that moment, you say to yourself,
I want to solve this problem.
What would it take?
For me, it was a 1.5 terabyte Seagate hard drive,
which are notorious now.
They have a reputation.
I woke up one morning, I powered my computer off this.
I was still running Windows.
I'm sorry to say.
No, that's it.
Windows happens, man.
We're okay with that.
I was still running Windows.
I had a bunch of movies and TV shows on this 1.5 terabyte hard drive,
which I'd ripped from DVDs I had.
Oh, yeah.
Oh, that's a labor of love.
Yeah.
And they were all beautifully categorized.
Of course.
And cataloged.
You had a nice naming scheme for each file and everything.
And I woke up one morning, pushed the power button on my desktop.
Sure.
And that drive didn't show up.
And I'm like, what the F?
Where has this gone?
It was fine last night.
Double click on all my computer, no D drive.
And then you go in disk management, nothing.
And you go into the BIOS, nothing.
And when it doesn't show up in the BIOS, you're like, oh, crap.
So I had a USB external hard drive, 3.5 hard drive.
You can close your kind of thing to hook up.
It was like a hot swap.
I had one of those toasters.
It's like a USB toaster.
You put this full size drive in there.
I love those things.
Yeah.
So they fulfill a use case that I wish I had.
Right.
I just don't have it.
Never used.
I bought one at home.
And I bought one here at the studio.
And I thought, I'll synchronize my data that way.
I use it all the time, but nothing.
So this 1.5 terabyte Seagate drive just had a hugely horrific failure rate.
We're talking double digit percentages where the spindle one morning just decided
I'm not going to unlock.
Back when IBM made this, I had a series of, we call them death stars.
Same thing happened to me.
In an array, like several of them started popping.
That name has stuck with the desk star forever.
So I had a similar thing afflict me one time.
And then you start thinking, well, what if that had been the other drive
that had all my photos on it?
Yeah.
And then you think, well, OK, I need some redundancy here.
Because one, the phrase in the backup industry is once is nonce.
Right.
One is none, actually.
One copy is not a backup.
One is none.
Two is something.
And then three is-
A proper backup.
Yeah, that's what you want.
So nowadays I have, for all of my photos, for example, I have a full copy in Google
of everything, like Google Drive.
I have a few drives at my dad's house, which I send everything remotely to him,
encrypted using duplicati across the internet to his house.
How does that work?
It has this wonderful way of doing like snapshots.
So it does incremental backups.
It's a bit like ZFS send, I believe.
It only sends what's changed, but it works at the file level instead of ZFS's
block level stuff.
So in some ways it's better.
In some ways it's worse.
And do you go to the trouble of encrypting the data that you send up to Google Drive?
Google, yes.
Yeah.
My dad's no.
Yeah, right.
Sure, sure.
Because in the event of a failure where I need to get that data, the risk of having
to remember an encryption password on my dad's house might be the one thing that
costs me that data.
And it feels like if it's going to be safe somewhere, it's probably your dad's house.
Does the encryption you use to go to Google Drive, does it support incremental file changes
or is it whole files every time?
I think it must be whole files every time.
Yeah, that's something I think we could explore in the future too is how to leverage the cloud
for your self-hosted solution that is still secure for backups.
Yeah.
Private, I think that'd be a big thing.
There's a whole rabbit hole we could go into on just backups alone.
Oh man, totally.
We could have a total full conversation.
I'd love suggestions too.
We have a new Twitter account at self-hosted show, which you can tweet at Alex and I will
both be monitoring that.
And if you have something you want to get in the show, that's probably the best way
or hit us on Twitter or in the Telegram for Jupiter broadcasting, hashtag ask SSH.
What does the S stand for?
The extra S.
The super self-hosted show.
I think we should make the audience guess.
Oh, they should give us suggestions by doing hashtag ask because we just wanted to use
SSH.
And the winner will get a t-shirt with ask SSH on it.
Oh, great.
Yeah.
Sure.
Let's just do that.
And then if you have something longer form that you want to get over to us, you can
go over to us, self-hosted.show slash contact.
Self-hosted.show is going to be the landing page for this whole show.
Anything like links or contact pages or our RSS feeds, subscribe links, all of that is
at self-hosted.show.
So we're going to be focusing mostly on service stuff, like you've just heard.
Chris and I, we're building out infrastructure.
We're infrastructure guys.
Yeah, like right now I'm working on like a whole security camera system that's going
to be tied into this.
Right, yes.
Got to talk about that soon.
I can't wait to hear about that.
I really like it so far.
But we also have another show on the network called Choose Linux.
Yep.
Which complements this one really well.
Yeah, on the desktop side.
Yes.
And they're trying out desktop applications and distributions.
It's a great introduction to that.
It's just a fun journey of discovering new things in open source and Linux.
Just like we'll be doing on the server side, they do it on the desktop side.
So it's sort of a companion show in a way.
I like how that's worked out.
It's almost like we planned it that way.
Almost?
Hey, before we run, I've been watching something this week I want to tell you about.
I think you might really like this channel.
OK.
I came across it when I was doing research on security cameras.
And have you heard of the EufyCam E's?
No, I have not.
They're like a competitor to the Arlo camera system.
And Eufy is a division of Anchor, the people that make all of the.
My robot is by Eufy.
Yeah.
And so they've made some really nice cameras that are battery, wireless, the local recording,
which is encrypted and you still get remote access to it.
And the channel is just great where I watched the review.
It's undecided with Matt Farrell.
And he has just a great clean take on this stuff.
Nice level headed.
It's a good YouTube channel.
So I just I subscribed and I became a patron today.
I really I was that impressed.
Look at those production values.
This guy has good lighting.
That's what I'm talking about.
You can tell just in the first thumbnail how good a video is going to be.
And he's really he's well spoken.
He posts the script up on his website.
He does a really good evaluation.
And I just want.
So he's put these cameras into a Synology box, has he?
I think it's one of the many systems he has.
Yeah, that's great.
Check it out.
Undecided with Matt Farrell.
We'll have a link in the show notes at self hosted dot show slash one.
