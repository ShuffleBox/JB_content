Well, it was road trip time yesterday, Chris.
Sure, that was a heck of a road trip.
Yeah, we drove up from Raleigh, North Carolina,
and here we are in Lexington this morning, Kentucky.
Yeah, and we discovered something called the Snake.
The Snake.
It was the very, very windy, twisty,
was it Tennessee Mountain Road?
I think it was Tennessee.
It was the perfect road for us
that we just accidentally stumbled across.
Yeah.
Alex's tires are designed to stick to the road.
Let's just say that.
I mean, you just described all tires ever, but.
I know, I'm not a tire guy.
Anyway, the reason that we took this road trip,
a nine, 10 hour drive from Raleigh,
was to spend a bit of time with Wendell, who joins us today.
Hello, Wendell.
How's it going?
Pretty good, how are you doing?
Oh, not too bad.
Although it is a little weird, I mean, like,
podcast technology, I would think with, you know,
since Alexander Graham Bell,
you guys would not have to come here, but I'm glad you did.
That's true.
I had that realization on about hour three of the flight.
I said, well, you know, we could have done Skype,
but this is a lot more fun.
I mean, then we got to see the Grand Tour.
Yeah, you get to see,
we're going to do an office tour at some point
on the Level One channel very soon,
because the renovation is basically done.
Like, there's a few rough things we got to work on,
but basically the renovation's done.
That's pretty presentable.
There is something special about
actually connecting in meat space.
Yeah.
Which you just don't get over.
I mean, I've watched Chris,
I've watched your channel for a while,
and Wendell, I've watched yours probably for just as long,
going way, way back.
And I know you cover all sorts of technology
and stuff like that,
but you generally tend to focus on
the nerdier side of things.
I think it's fair to say.
So-
Look at this thing that I gave a large part of my life
into figuring out.
This was a terrible, terrible experience.
Suffer along with me.
I think that's an accurate statement
for a lot of what we're trying to do with this show, right?
Right, yeah.
Is, yeah, I've put some blood, sweat,
and a lot of alcohol into this project.
And I want you to come along for that ride with me.
So, yeah.
We thought it would be a good topic for our,
this is our second episode as we record.
We thought just the discussion of why self-host
is sort of a meta conversation.
And that's why the tour of your space here
was pretty perfect,
because there is clearly some stuff you've chosen
to spend the extra time to build local infrastructure,
put high-speed networking in, lots of disk.
And then there's some things,
which we don't know about yet,
that you alluded to that you've chosen to host in the cloud.
So I'm kind of curious what the Wendell take is
on why self-hosting is important.
I think that for the individual, self-hosting is
never more accessible than it's ever been.
And I think that going forward,
self-hosting makes a lot of sense,
because the reason that you don't want to self-host,
I think, is one of convenience.
So you look at Google and Gmail,
it's like Gmail is super convenient.
You just load a browser and there it is.
But the technology has moved on to the point
that that's basically become a commodity.
And we could talk,
I don't want to get super into the weeds here,
but things like containerization technologies mean
that individuals are better able to access Google levels
of technology running on a Raspberry Pi.
And so, well, running on a Raspberry Pi
might not be the best user experience,
but it's possible to have an even better user experience
than, quote unquote, the cloud,
even if you're rolling your own cloud
or doing a private cloud.
And because you're giving somebody money for the service,
they're not having to mine your data
in order to earn money.
Right.
Now, I did notice a lot of disk,
but you said some of that disk
was to actually backup the stuff
that you do keep in the cloud.
Yeah, so I'm always paranoid
that something's going to go wrong in the cloud.
Like, Microsoft had that major DNS outage in Azure.
And so it's like, is that going to happen?
There's a backup provider for dental,
like their specialization is like dental offices.
So you've got HIPAA and things like that.
And this was just last week.
And the organization that provides the backups in the cloud
was hit by ransomware.
It's like, oh, we all laugh.
It's like, oh, this is, you know, blah, blah, blah.
That's incompetence.
But- Happens though.
Yeah, well, I mean, when you have things like the OPM hack,
like the hardware was defective from the beginning.
So you know how you make sure it's safe
is you have an offline backup that's inaccessible.
Yeah, and there's something to being a smaller target.
If it's, you know, on your LAN,
you are one person amongst millions of nodes
on the internet.
But if you're Azure in this case,
or another large provider,
you've got a pretty big target on your back too.
So there's a different risk profile.
There's like a different set of risks.
Like you still have to keep your system secure
and make sure your firewall's, you know, set up right.
And you're not just letting anybody in,
but in a way you're also kind of just one of many.
You're sort of lost in the noise.
That's one of the things I like about containerization
is because it offers the possibility of somebody
to set up an automation system,
like just a very basic Docker, you know,
scrape and rebuild on a timer
would let you keep the patches and security
and still do local hosting
and probably be pretty safe
as long as the container that you're using
is maintained by people that know what they're doing.
Of course, like depending if the container
is not well-maintained
or there's like an open source supply chain attack,
then that's a whole other risk profile as well.
So what containers do you use like for backup?
Well, like we would have a replication
of like whatever the thing is.
So like if there was a very important website
that we were hosting,
we've actually been doing a lot of work lately
in taking traditional CMSs and making them headless.
So like with Drupal, for example,
it is very easy, not very easy,
but we can fairly easily add an interface to the JSON API.
So that Drupal, a traditional CMS,
which is actually kind of slow and kind of clunky,
especially version eight,
but we can take the content from that,
export it as flat files,
and then everything is perfectly fine to run
on like Amazon S3 and CloudFront.
And then when somebody has a content update,
it just regenerates the flat files.
And I think this kind of thing
with a service layer on top of it
is probably the future of websites.
So if you have a very large website
that needs to be very, very resilient,
you can have your traditional hosting thing,
but like your backup plan can be the series of flat files
or the main hosting can be a series of flat files.
And then it's like, oh, something went horribly wrong.
It's like, okay, well, let's just make the change in DNS.
And like Amazon could be on fire.
We don't care.
We could roll it out at Akamai.
We could roll it out at Linode.
We could roll it out, you know,
CloudFlare could provide services.
It's really, when you're dealing with it at that level,
the service provider no longer matters.
Are you not tempted to do like this new,
cause you just have a video come out about DevOps,
a DevOps workstation.
Are you not tempted to do stuff in Lambda?
You know, have these functions
that are just very single purpose or?
Yeah, no, you totally could.
It's funny.
We actually do have some things
that are running in Lambda.
So like a customer comes along and wants to do a job
and it'll just spin up in Lambda, do its thing.
And then the output is added to the collective.
And that's totally fine.
Like that's a perfectly reasonable way to design things
that functionally, I mean,
it's way different under the hood,
but functionally that's where a lot of DevOps is going
because like our local DevOps server,
Kubernetes is doing a lot of stuff.
So like a developer checks in a bunch of stuff,
a VM has stood up and a whole bunch of things happen
in Kubernetes and then all of that goes away
when they're done.
Well, actually it sticks around for like a week,
just in case we need it.
But then after a week,
like whatever that temporal thing was gone.
And this actually has worked out really well.
There's a particular project in mind
where the client is very hard to work with
and they've got their own like chef automation.
And so like, even if we just add an SSH key
to the very, like the two vCPU instances on Amazon,
which is not really enough for what they wanna do,
chef comes along and just clobbers that periodically.
And so we've set up continuous integration
and a Kubernetes thing so that it actually will spin up
something that works kind of like the Amazon EC2 instance,
but it has a way more CPU and memory horsepower.
It does all of the work that it needs to
in that temporal VM.
And really it's just a container in Kubernetes.
And then it pushes from that Kubernetes container
to that really just awful, terrible EC2 instance.
And so it doesn't matter if they come along
and clobber it later.
It doesn't matter if they come and delete the SSH keys.
It just doesn't matter.
So how do you decide which services get that treatment
and which services end up in your closet downstairs?
Well, a lot of, see,
I would say that that exists in both places.
Like it would not work without the closet
and it would not work without the cloud services.
You get sort of the best of both worlds.
If the cloud service goes away,
it's really easy for us to redeploy somewhere else.
And if the closet goes away,
we've got time to restore from offsite backups
and do whatever we need to do
without anybody that's in production
really realizing what's happened.
I think we need to rename the show, Chris, Closet Hosting.
It is, you know what, there's something to it there
because it's a nice, perfect sweet spot.
I really like that.
Cause you're not dependent on any one cloud provider.
And you're not absolutely dependent on your office space
being online 24 seven,
if you have to take something down
to install something new in the rack or move it.
Cause let's face it,
as experienced as you clearly are,
you're not a data center.
You don't have techs here 24 seven.
You don't have all the monitoring that they do,
the security protocols that they do to even get on the floor.
Although when you deal with people
actually in the data center, it's less than impressive.
True.
I've got a failed hard drive.
I need that replaced.
Do what now?
I can hammer the front of the server with a hammer.
Yeah, it's very frustrating experience.
So we're here at your space.
What about like your home set up?
You have storage systems at home.
Do you have a big like home server set up
or do you keep it lean at home tech wise?
Our home is very lean.
It's a similar philosophy,
but I have an ITX system that is in the fractal node 304.
And it's a four processor eight thread system right now,
but it's very soon going to be the,
as soon as I get a 3950X from AMD,
it's going to be a 16 core AM4 system.
Thought about replacing it with Threadripper,
but it's really overkill for what I do at home.
But the home system runs a number of VMs.
There's an IOT gateway because all the IOT devices
are on a completely separate network.
And the IOT devices, I run more homegrown IOT devices
than real like third party IOT devices.
What's an example of that?
The system for opening the garage door
is hooked into the lights.
So like where my house is,
there's a light that's like right at the road
cause there's a little bit of a driveway.
And then there are lights on the garage
and there are lights on the house.
And so there's a receiver at the light post
when you first entered the driveway
that picks up the Bluetooth Mac address of my phone.
And so when I drive by the light post,
it will turn the lights on if it's after dark
at the garage and a house.
So the IOT gateway for that cannot get on the internet.
Chris just for the listeners looked at me and winked
cause that's a phenomenal piece of engineering.
Yeah, it's great.
It's just an idea I wouldn't have thought of.
And are you battery powering that thing out there
or did you run power out to the remote?
It's a Raspberry Pi Zero in epoxy.
And so it literally is just doing its thing out there
with Bluetooth and the wifi from the house
is strong enough to make it out there.
That's great.
I mean, it's only like one megabit
but that's enough to send the packet
to be like, turn the lights on, it's dark.
It's all you need really.
That's a great solution.
And so you do have some level of presence awareness.
So the system knows specifically it's you.
Yeah.
That's really nice.
Did I hear you mention home assistant?
Yeah, I've been, so the home assistant conversation
is complicated.
It's broken right now, but historically I've had
the home assistant doing fun things
like scouring the internet for certain things
that I might be interested in that are temporal.
There's a Plex server that's a part of that.
So, you know, like Plex and historically,
although not currently, I've had an impact to capture box
that was hooked into the listings for cable TV.
And so like historically it's like, okay,
probably interested in Star Trek,
probably interested in all these things.
And I could get to that from my phone
and just tag shows that I might be interested in.
So like for Game of Thrones, it would DVR it for me.
But then that goes into Plex.
And so that's another like kind of cloud,
kind of not really thing.
Like Plex's model has changed over the years.
I'm not really super happy with their subscription model.
It's still probably worth it,
but it's super easy to run all of your media center at home
and actually stream it yourself
over your internet connection.
A lot of the internet connections here in America
are gimped in really weird ways,
but like upload is one of them.
And so it can be problematic to stream from,
like if you're traveling,
like stream your home media collection.
But Plex actually makes that really easy.
And I think that like movie studios and rights holders
are really nervous about that kind of functionality.
And I hope that it doesn't go away,
but being able to record those kinds of things
from traditional mediums,
like time and format shift it basically
has been really awesome.
But I don't really have a lot of time
to enjoy that kind of stuff anymore.
And Netflix is good enough that it has broken.
And it's like, okay,
I need to probably figure out why it's broken
or updated or whatever.
And I just haven't bothered.
But the thing about all of these streaming services
is, you know, Disney's coming out with a new one this year,
Apple, what's the one for Star Trek, CBS.
Yep, and Warner's got one coming out.
Yeah, I'm getting a bit fatigued by all this stuff.
I don't really like the idea
of not being able to get back to that either,
because like I have archived copies of Next Gen,
but I also have like the resampled whatever thing
of Star Trek Next Gen.
And I really like the idea of format shifting
from the newer formats
onto something that I can watch on Plex all the time.
Because like, what's on Netflix is not,
I don't think it's the resampled version.
It doesn't seem like it's the resampled version.
And, you know, things could disappear off of Netflix
at any given moment, like,
oh, House is here today, now it's gone.
Yeah, there was talk of them removing Friends
and people went crazy about that.
It's really all about the rights.
You know, each different one of these streaming services
wants to have that exclusive content.
And so if, for some reason, CBS did the calculation
and said, you know, we make more money,
if we take Star Trek off of Netflix, they could do it.
I don't know why culturally this has become okay.
It's like, oh, Catcher in the Rye is no longer available.
What?
Yeah, it's a weird, like it makes the product inconsistent.
And it seems like it's going to drive piracy.
Yeah.
It's a worse situation
than we were in with discs 10 years ago.
Yeah.
You know, because if I have a disc on my shelf,
I know that I'm gonna have that disc on my shelf.
Well, I now don't,
because I've ripped them all and put them in Plex.
Well, that's the person who does that
is the person who is the most advantaged now.
People who have pirated and created a huge content
or like me, I bought like all of the Star Trek stuff
on Blu-ray when it came out on Blu-ray,
bought it all on DVD and each time I've ripped it
and I'm just sort of done doing that too.
And just getting exhausted by it.
Yeah.
So you got a plague server.
How much storage is that on?
I think it's only about 20 terabytes, give or take.
So it's not, you know, exorbitant,
but there's also like production VM backups
and some other things there.
We need a leaderboard of different guests, Plex storage.
You know, like the top gear lap board.
Yeah.
We should have that with that.
That would be.
Well, if you don't mind software defined storage,
I could allocate a pool here.
I mean, we could probably go like up into the petabyte range.
Ooh.
That'd be pretty high quality board.
Okay.
Windows just shut straight to the top.
I love it.
It's gonna be hard to beat that for a while.
Just coming back to your Home Assistant,
when I say Home Assistant,
I mean the Python project that's all about home automation.
Oh yeah.
But you clearly mean something different.
Yeah, no.
Well, I wanna tie all of that in
because the Python Personal Assistant
is also about things like, you know,
for a while I had a scanner by the door.
And so like I could, like I got mail
and I could just feed it through the scanner
and it looks at the image.
Like I don't have to tell it what it is.
It knows, like it knows the time of day
and it knows this looks like an electric bill
and OCR is good enough
that it'll just categorize that online.
So like if you've ever used OneNote or Evernote
or something like that online
and you organize everything temporally,
you can just put that there,
tag it electric bill, whatever.
And then I don't have to think about organizing it.
It just does it for me.
Those kinds of things,
like a lot of people put a lot of work
into organizing those aspects of their lives.
Technology can do it,
but nobody's really doing it in terms of like a product.
I can see that being super valuable for you as an RVer
to have a PO box or something with an AI system
or machine learning system sorting this mail.
That's something I've thought about.
Cause there's some services where humans will do it,
but it's quite a bit.
It's quite a bit.
Yeah, my mother-in-law in England sorts my British mail.
See, I need one of those.
Yeah.
Just scan it, get it in the computer.
And that kind of stuff is really easy to automate
at the command line.
I mean, just a little bit of Python
and a little bit of image recognition
and even like the barcode recognition,
there's some really good optical barcode recognition
libraries out there.
You'd be surprised what you can bang out in a weekend.
Really?
And some of that though,
does sound like it relies on some service.
Like at some point you have to have something
that's intelligent enough to sort and store this stuff.
Well, you can still do all of that locally
because you can sort of tag it yourself.
I mean, it's one of the reasons why I have the Tesla V100s
is I wanna do that also with like the video component.
And I think that they're like OpenCV
and some of the open source libraries now are good enough
that you can train your own neural network
to do these things.
And I've got the old data set
so that I can just be like,
here is the last three years of electric bills.
This is what an electric bill looks like, just do it.
That is like that.
So there's compelling reasons in that argument.
There's compelling reasons to start capturing now
even if you don't have the recognition system in place yet.
Yeah, and Python is also really easy to automate.
And so like some of the same web technologies
we use for QA and QC, things like,
it's not Phantom JS anymore
because Phantom JS is on its way out,
but you can do browser automation and look for things.
And so you can like automate the login thing
for the electric company
because like you can get an electronic version
of your bill now, but it changes a little bit.
And sometimes the system is inaccessible
and sometimes you can't get more than six months of bills
and blah, blah, blah.
You can just automate that and have it log in and do stuff.
And then if it can't log in
cause they changed the website or whatever,
then you can have it notify you to be like,
hey, I couldn't download the bill or whatever.
Or I couldn't find the amount that was due
cause they changed the name of the div or whatever it is.
And that's not even AI.
That's just your good old fashioned
web dev quality control workflow.
Sure.
So you said, when I started asking you
about your home setup, you said you do keep it kind of lean.
Is that almost a philosophical thing?
Yeah, I don't really like data storage.
Like the data storage that I have,
I have files that go back to like the nineties,
but other than storage and replicas,
I used to have one full rack at home,
but by and large between virtual machine consolidation
and just not wanting to do the maintenance for it,
it's really become super, super consolidated.
Like all these little experiments
and all these other little things, they run in containers.
And right now my home setup is using Docker,
but everything at work pretty much is,
almost everything has moved to Kubernetes
cause Kubernetes on bare metal has also gotten a lot better
in the last six months or so.
And so because everything is containerized,
I have much better separation of the stuff that's mine
that I want to hang on to and stuff
that's just a transient thing that I put together
to try to deal with whatever it is.
Are you the sort of guy,
cause you mentioned a few of your servers
have Proxmox on them and a few of them have ESXi at home.
What do you do?
Are you bare metal at home with just containers on there
or VMs or?
At home, it's actually Beehive.
So yeah, I mean.
Okay, tell us why.
Cause Chris just did a double take on that one.
I wanted to experiment with it,
but I didn't trust it with production workloads yet.
Fair enough.
So.
Good way to do that.
Yeah, I mean, it's like, okay, will this actually run?
And I think that's part of why some of the things
are broken I think is cause it was like,
oh, I probably, I don't know if this was ready for that.
I don't know if maybe.
So Beehive is what the KVM equivalent for BSD?
Yep.
From, yeah, okay.
So that answers the OS you're running then.
Yeah, that was an actually that started out
as a free BSD installation that,
or I mean a free NAS installation that became free BSD.
And so that one became free BSD unwillingly.
Like it was free BSD or it was free NAS.
And then they did that upgrade that had like the web GUI.
That actually like, I feel really bad for that developer
because that was actually a brilliant interface
and it was great.
And it was exactly the right idea.
It was necessary too, but boy.
But the inertia of everything else, it was just impossible.
Yeah.
And I hope they bring that back
cause that was actually a good idea, but.
I do believe actually they are working on a new one.
That went wrong for me.
And then so then it became free BSD.
It's funny.
My free NAS systems are no longer free NAS systems either.
I went a little more extreme.
I moved them to Fedora, which was much more of a process.
The level one server, that's what we did.
Really?
Yeah.
I mean, once it's ZFS, the data is safe
and then you just got to get it working on Fedora.
I thought that's what you were going to say, by the way.
I thought you were going to say you had Fedora at home.
In some cases, ZFS on Linux is now ahead of the BSD fork.
Yeah.
Hasn't that been something to watch?
It has, yeah.
It's been a lot of fun.
Have we got memory compression yet?
That's the thing I'm waiting for.
If they do, it's not in any of my systems.
Well, Chris has a direct line to Alan Jude.
We can find that.
Alan, add that.
No, he's, yeah.
He is actually really involved with it these days.
I think that is a feature that he added.
I just don't know where it's available yet.
But the memory arc compression, data set compression thing,
that is going to be a game changer.
So you're all ZFS at home as well?
Yeah.
It's the only file system I trust.
Really?
Yeah.
So how do you square off?
Because at home, a lot of people, I mean,
I'm looking around this room full of hard drives.
And to paint a picture for people,
we're in Wendell's office today.
And it's just, it's Willy Wonka for nerds, this place.
It's like, it's amazing.
But for a lot of people, they want
to add one or two hard drives a year to their system
and stuff like that.
One of the biggest issues I face at home,
certainly with using ZFS, is that expansion,
the lack of flexibility.
What do you think about that?
So I ran up against that myself.
I mean, you remember, my home server
is an ITX system that's only got six three and a half inch
hard drive bays.
It's the Fractal Node 304.
And so I started out with, I think, three terabyte drives.
And then I bought three eight terabyte drives.
So it was two VDEVs of RAID Z1.
So one drive could die.
But I realized that when I was putting the eight terabyte
drives in, and that was when eight terabyte drives were
new and kind of expensive.
And it's like, I don't want to buy just one drive,
because losing eight terabytes of information
would be a huge pain.
And RAID 1 is not economical enough,
because I have to buy twice as much to have the redundancy.
But if I get three drives, it literally
splits the middle of cost and risk.
And so it's like, all right, I'll
get three eight terabyte drives.
So I got three eight terabyte drives.
And I was just going to add it as a VDEV
so that I could have six plus 16.
And that would be pretty good.
But then as I was doing that, I was like, well, wait.
I could probably just set up another zpool
and move everything off of the old one
and then have the free slots and just keep doing this three
drive square dance.
Because at home, at most, I would
have like four or five hard drives.
And so is it unreasonable economically for me
to buy four or five hard drives at a time
for doubling my storage every time?
And it's like, as long as the march of technology
continues, probably not.
I think that is the exact calculation I make with CFS.
If I'm willing to buy two or three drives at a time,
then it's a perfect fit for me for expansion.
And I think you've just got to make that math calculation.
And you've decided because of the rate
you want to add drives, it might be just too much.
And you also want to do mismatched drives,
don't you, like ones you take out of production
and put in there?
Well, that's an important consideration.
With the VDEV calculations and things like that,
you have to pretty much run the same size,
same make, same brand, ideally, performance-wise,
of drives in each VDEV.
So I use a merger FS.
And basically, it's JBOD plus SnapRaid for parity.
Because a lot of my data sets are just
very large static files that you write once, read a few times.
But I can imagine if you're doing bursty workloads,
like VM storage or any kind of database work,
you might run up against some limits.
Because you mentioned something about Optane today
whilst we were talking.
I mean, I've heard Linus Tech Tips talk about this.
And I just don't really know what it does.
Eventually, it should be cheaper than Flash.
It's one of those technologies that is really good.
It seems like it's less complicated than Flash.
But we're entering an era where you've
got like five or six bits per cell with Flash.
And so the density is going to be hard to compete with there.
But as I understand the manufacturing for Optane,
it's basically a 3D phase change thing.
It's just a couple of sheets of silicon
with a phase change layer between them.
I'm grossly oversimplifying.
But that type of manufacturing seems
like it would be orders of magnitude simpler
than NAND Flash manufacturing.
And then you add in the fact that the throughput is not
there yet.
Well, you can fix that by adding more devices.
But the latency is about halfway between NAND Flash and DRAM.
And so that's one of the reasons I
think Intel is pushing Optane as a DRAM alternative.
But the fly in the ointment for them
there is that AMD has shown up and said, we don't really care.
You want to run non-volatile memory?
No problem.
You want to run 8 terabytes of memory on a two-socket system?
No problem.
We don't care.
Whatever you want to do.
And so Intel's like, oh, man, we've
got these separate SKUs for more memory.
Darn, I mean, oh, we were going to try
to do some market segmentation there or something like that.
Because some of those large databases,
these companies will pay any amount of money
to make their database run faster,
because it literally translates into more money for them.
And so having the database run from Optane,
whether implemented as memory as DRAM
or implemented as a storage device,
enables those kinds of workloads in a way that the DRAM can't.
But Samsung and other companies have
done a lot of work on their NAND devices
to hide a lot of the latency, very efficient caching
algorithms, mixing flash types on a particular device,
and those kinds of things really solve
the problem for a lot of workloads,
but not the workloads where the companies are willing to pay
just obscene amounts of money.
And so I think that is one place where ZFS could improve
a whole lot is much more intelligent caching
for some of those workloads.
I mean, you get the ZIL and the L2 arc,
but it's still the case that just adding
a whole bunch more memory is generally
better than more L2 arc.
And so we're in a situation now where you can get Optane
or as a DIMM.
So you're literally, quote unquote, adding more RAM.
Or you can get Optane as a storage device,
and then you're adding RAM on disk.
I'm seeing videos where people are building entirely
flash-based servers now.
So the actual storage array is as fast now
as the caches used to be.
Yeah.
Which is insane.
Yeah, some of that, especially for video editing,
the random scrub ability is really good.
Although my little setup with its scrappy NAND flash
and 128 gigs of memory and spinning rust
with an intelligent and tiering cache policy,
I would put up against a lot of those flash servers.
A lot of the flash servers, especially the DIY servers,
like the jellyfish all flash server, for example,
I think that the software is just too dumb.
Because my old school NetApp disk
shelves with an intelligent tiering system for video
editing will match the performance of an all flash
storage while being orders of magnitude less expensive
for bulk storage.
Is that custom software they designed to figure that out?
No, I think a lot of the enterprise software
for tiered storage has already solved this problem
a long time ago.
We just haven't seen anybody apply it in this space.
I'm working with the Inmodus guys
to try to figure out a way we can demonstrate it.
Because the Inmodus product on Linux is actually quite good.
You can go in and tag files and be like,
this is part of the fast tier, this is part of the fast tier,
this is part of the fast tier.
And it'll do it.
So if you combine, like for a video editor,
if you combine your video ingest workload
with a little bit shell scripting, basically.
So I'm going to ingest videos from the camera.
It's like, oh, we probably need those
to be part of the fast tier.
And then you look at A time, and it's like, oh,
those probably should definitely be part of the fast tier.
Or you just give an editor control.
Like an editor can go create an empty text file in a folder
to be like, make this project hot.
And then you do that on a Friday, and you come in Monday,
and the system will have automatically,
it's like, oh, I need to make everything in this folder part
of the fast tier, and just do it.
That's like five lines of shell scripting.
But I'm 100% sure that that is going
to be as fast as a quote unquote all flash storage solution.
Interesting.
That makes a huge difference, it sounds like.
That is really cool.
So I've seen stuff like that.
My day job is working with OpenShift, a Kubernetes
enterprise distro.
And I see a lot of stuff in there around storage classes,
about how you can specify a flash array for this workload
and a spinning.
So this sounds like a really poor man's version.
Well, not necessarily poor man's, but yeah,
if it works, great.
Yeah, you can do it at the individual folder level, too,
which is really cool.
I want to implement some of this stuff for my Proxmox setup
now, so we'll have to share a Git repo or something.
It would be a lot of fun, yeah.
So I wanted to shift gears a little bit
and talk maybe a bit more about younger Wendell,
if that's OK.
I'm interested to know a little bit about what
got you interested in technology in the first place.
What was your first, can you remember,
what was your first computer?
It was probably a LaserPow 8086, or possibly
a Tandy 286.
So the LaserPow was like a Luggable,
and the Tandy was like a business machine,
but it had problems.
It needed to be repaired.
And so the thing that you have to understand
is where I grew up was super poor and 10 years in the past.
So a lot of the technology, I have fond memories
of the IBM XT, and the PC6300, and the AT&T PC6300,
which is really amazing, and the Tandy TL2, and even the TRS-80.
But a lot of these computers were not
current when I was using them, because a lot of businesses
would donate the, there were still tax loopholes.
And so it's like their garbage computers
would end up in poor schools and in the poorer
parts of the country.
And so I got to play with a lot of that stuff,
but also because it was garbage, I
wasn't afraid to tinker with it and break it worse.
But more often than not, I could actually fix it.
So there was a guy that ran a junk store,
and he knew that I had a knack for fixing things.
Not just computers, but other stuff.
And so he had somebody come and pick me up every weekend
and in summers and would bring me to the junk store.
And so I would work on fixing stuff at the junk store.
So he would buy the return truck from Sears.
You know the thing on Amazon now that's really popular?
It's like, oh, I bought $1,000.
They're just random returns from Amazon.
So you used to do that in the late 80s, early 90s.
You would do that tractor trailer at a time with Sears,
and you'd bid on it.
And so he would do that, and then there
would invariably be random technology on there.
And it's like, oh, look, I fixed this fax machine.
And he's like, great, let's sell it at the junk store.
He's like, oh, look, I fixed this other thing.
And it's like, oh, yeah, let's sell it at the junk store.
So between that and the school stuff, it was a lot of fun,
learning how to fix the stuff and put it together
and that kind of thing.
I got that there was a Tandy that
had bad sectors on the hard drive.
And so if you just did a regular DOS install,
the DOS install would fail because it
was near the beginning of the disk,
but it wasn't at the very beginning of the disk.
And so I had to figure out how to low level format the thing,
and there was no manual for it.
And it was just completely our local library had a book
called The Wyn Roche Hardware Bible, which had like,
I mean, I get a feel for the photographers.
It's like, you get the photographer for the book,
and this guy shows up.
And it's like, we need a super technical book
about component level repair, like the IBM PC XT.
And it's just like some old balding guy.
And it's like, OK, well, let's just
put a keyboard in their lap and take a picture of it.
And so that is the Wyn Roche Hardware Bible.
And so I carted that thing around for years.
Well, you'd love to get your hands on a copy of that again.
Still have it here somewhere.
Do you?
Yeah.
Good.
Good for you.
That's great, holding on to that kind of stuff.
That guy was clever to snatch you up.
Sounds like he made a good chunk of change off of you.
Yeah.
Yeah.
That was a lot of fun.
Later, I went to work for the company that fixed computers
for the school system.
So that was fun.
The thing that got their attention,
so IBM Model M keyboards, if you're really hard on them,
will develop a short in the cable.
So it's like foil wrapped.
I mean, typical IBM engineering, it's
like the rubberized plastic and then a hard plastic,
almost like a plastic bag plastic,
but the hard cellophane, and then foil,
and then individual wires.
And if you yank the cord, it will break the insulation
on the wires inside the foil.
And then they'll touch the foil and short out.
There's a green through-hole fuse
on the motherboard that will blow whenever
you have a keyboard that has that type of a short.
And so at a school, it's like, we
don't want to replace the keyboard.
This keyboard almost works.
And the keyboard will work as long
as the keyboard is in the right spot.
And then it mysteriously stops working.
And it's like, that's weird.
Let me try this keyboard on another computer.
It's like, oh, it's not working on this one either.
Now, both of those have a blown fuse.
So the replacement cost for a school that could not afford it
was like $900 for the motherboard.
Oh, my goodness.
And it was like, dude, we can just replace the fuse.
And they're like, one, if you're a student, what are you doing?
Two, OMG, WTF, BBQ.
And I was like, all right, well, just hire me.
Why can't I work for you?
And it was like lots of angry adults yelling at one another
and then like, all right, we're going to hire you.
Had a very similar experience in my school years.
Very similar to that.
That's great.
The adults were very uncomfortable with it at first.
They didn't know what to do about this kid that
was coming around fixing stuff.
Very, very uncomfortable.
There were two of us at the school, so I had a cohort.
And he's doing really well and all the stuff that he's doing.
But the really amazing thing with that is we deployed Linux.
By the time I was a senior in high school, Linux was just bare.
It was like 0.92.
What was the job it was doing?
Network address translation.
That's what I had my Linux blacks do, and also a proxy server.
So the school had a token ring.
And the new computers that were coming in were Ethernet.
And so there was just no internet access on the school with Ethernet.
Yeah, I mean, Ethernet was, well, it was kind of dumb
because they were like, well, 16 meg is faster on paper.
So that's got to be better than 10 meg Ethernet.
And I was just like, what?
What?
No.
Me and the other student, between the two of us,
we sort of figured out how we could do IP translation between a network.
It was called IP masquerading at the time.
And so the Ethernet computers could get on the token ring network
through NAT, basically.
And that was a lot of fun.
That was a crazy amount of fun.
And the guy that was in charge of all of the technology for the entire state
came, and he's like, this is good.
What you've done, it's fine, but without annoyance.
But we can't have an unsupportable solution in schools.
It'll cost too much money.
No one here has any idea how this works except for you.
And we can't do this.
We have to buy the proper equipment.
And it's like, all right, well, we can have a bake sale or something.
What is the proper equipment to do this?
And he explained what it was.
And I was like, all right, how much is that?
And he's like, well, I think the state has a contract for that for like $10,000.
And I was like, $10,000?
And I was like, if a couple of kids can do this with junk computers,
don't you think it's worth your time to learn how to do this?
Because you'd save $10,000 to school, and you've got a lot of schools.
His ears turned red.
Yeah, it's so funny.
And so we had a situation where the schools were very, very excited
to implement 802.11b, I think it was.
It was two megabits.
And that's how they did all of the interlinks between the schools.
Because otherwise, they have to pay for lines from the telcos,
which were just ridiculous.
And when they put the whole wireless network in,
nobody thought to segment up the different schools.
So it was one flat network.
Oh my god.
What could go wrong?
So like print jobs were routing through the central district building
and back out to the schools over a two meg link.
And so we built a series of Debian-based servers
to essentially solve the problem.
And Microsoft found out.
This is back in the days of Monopoly.
So they were rough players.
And they assembled a meeting with the adults.
And they came in playing hardball and said,
we want you to use this thing.
It's called NT5.
It's in development right now.
And it can do exactly what that Linux box is doing.
You want us to put a beta in when this is working just fine?
Yeah.
It was a really interesting time.
It was so early.
It was the same story.
It's an unsupportable product.
And now it would be just totally normal.
What's really funny is that all of those computers are IBM computers.
And IBM has literally become the poster child of Linux support.
Yeah, oh boy, there's some irony there too, huh?
Something, something, red hat.
Alex will say nothing for this segment of the show.
Jeez.
Yeah, really, when you look back at it like that,
you did have to kind of fight some misconception.
I don't know misconceptions is the right word.
But you had to push through some of that.
Yeah, they did eventually replace that system
with a solution based on Microsoft proxy server.
Was that on NT4 at the time?
Probably.
And it was garbage.
It was unadulterated garbage.
That's what they wanted us to use at first.
And we said, no, it's too much garbage.
We tried NT4 first.
And then they're, OK, we'll use the beta, use NT5,
which ended up being Windows 2000.
They never even called it NT5.
But back then, that's what they were calling it.
The people, like the state level people,
even came in to test it and oversee it
and make sure everything went smooth with the replacement.
And they had test systems set up side by side.
And it was like, here's the old system.
Look how much faster and better the new system is.
And just loading web pages was dramatically slower.
It was astonishingly slower.
Yeah.
Could you picture doing what you do now
in that era of the internet?
No.
I would have probably gone to like,
they would have had to put me away for homicide
because it was just like, this is like the dumbest
thing I've ever seen.
Yeah.
Yeah.
We've lost our ability to think clearly.
Yeah.
It's pretty incredible that the internet enables you
in quote unquote rural Kentucky to operate
an internet-based business that you have,
was it quarter of a million subscribers or something
on YouTube these days?
The technology really has changed the world.
And I think humanity is only really just beginning
to grasp what the internet's capable of with remote work.
Well, Arthur C. Clarke said that, and not in this generation,
but next generation, referring to us, I think,
that probably in our lifetime, we
will see a sea change reversal in the value of property.
City property will become worth less than rural property
because you could work anywhere.
As long as you have a good link.
Yeah.
And I feel like that is also, that meta story
is what is driving us to do this show now,
is the home solutions, like you touched on,
are as good as what Google can run in their data centers now.
Yeah.
That's mind blowing.
Well, I mean, it's a question of resources.
Like Google wants you to have the tiniest slice ever.
I mean, like even a serverless slice.
Like Amazon wants you to come up with a way
to do everything serverlessly because their margin
on the serverless thing is better even than EC2.
And their margins are 100% on EC2.
I mean, I've worked on some very large projects.
And trying to figure out the Amazon cost to the business,
when you've always got a background hum of customers,
and then you have like the burstiness,
and you're going to rely on Amazon for the burstiness,
your costs on Amazon is 100% of what you would
pay for bare metal hardware.
The question is just optimizing the curve
so that you're going to have a little bit overbuilt
in for your local infrastructure.
But for those things that are like two standard deviations
away from average, then you can rely on Amazon.
And it makes economic sense.
And yeah, you're paying Amazon money hand over fist.
But you didn't have to do the capital investment
for the other part.
So it probably makes sense.
And I think that's Amazon's, or I think
that's even Netflix's model.
That is the thing.
It's a CapEx.
It's not CapEx.
It's OpEx.
So people can write it off every quarter instead
of one big chunk up front.
And I've actually managed to get quite a lot of traction
with C-level people with that kind of math
and say to them, look, it's going to save you money.
You can write this off every quarter instead.
So just before we sat down, Wendell,
you mentioned something about Eric S. Raymond having
an invitation-only basement that he can do hacker projects in.
Yeah, this is always something that's really fascinating.
And something I've been on the lookout for real estate-wise
is I'm not in a position to do it now.
But he always tells us stories about people
that have come to stay with him and not really exactly
mentors, but people that would be
able to work on their projects in his basement.
So his basement is basically a separate apartment.
But you have access to Eric Raymond upstairs.
So it's like, I want to work on this thing,
or I want to learn about this thing, or I want to do whatever.
So it's a really sort of fascinating situation.
It's like a little mini maker's lab
with Eric Raymond upstairs.
Yeah, yeah.
I mean, that kind of thing is just fascinating.
So that kind of thing might work really well in terms
of doing some other stuff.
Well, before we go, Wendell, we should
do the obligatory pluggy plugs.
I bet everybody knows where to find you,
but where should we send people?
Level in the number one text.com.
And then you can get to Twitter or the YouTube channel
or wherever from there.
I mean, it doesn't really matter.
You can just Google level one Wendell, and it's fine.
It works.
I've tested that.
That's Tech Wendell on Twitter, I believe.
Yes, yes.
We'll put a link to that in the show notes as well.
Yeah.
Wendell, thanks not only for joining us,
but thanks for hosting us, too.
It's been a great tour.
Oh, no worries.
It's been interesting.
I hope it was worth the eight-hour drive.
I mean, ehh.
Well, thanks to the snake, it was.
The snake was.
The snake made it all.
Yeah.
Oh, you haven't seen Rule.
We could go about three hours off the beaten path,
and then you'll be like, holy smokes.
Really?
Yeah.
Could that be something?
It's scary.
Well, you can find us on Twitter at self-hostedshow.
Chris is at chrislas.
Thank you, sir.
What about you?
And I'm there on Twitter with at ironicbadger.
And don't forget to send us your show questions.
We'll get those into the show soon when we're done traveling.
Hashtag ask SSH.
Self-hosted.show slash two.
