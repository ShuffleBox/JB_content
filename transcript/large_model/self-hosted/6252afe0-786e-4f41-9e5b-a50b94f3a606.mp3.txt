Well, welcome to episode 73, everybody. My usual co-host, Chris Fisher, is unfortunately out sick
this week. We think he might have the Rona. So please send him your best wishes. But joining me,
I have a very special guest. I have Techno Tim. Hi, Tim. Hey, thanks for having me.
Well, thank you. Absolutely. Last minute, I messaged Tim yesterday and said,
yeah, Chris isn't feeling so good. Do you happen to be free tomorrow night? And serendipitously,
he was. So here we are. So this episode, we're going to be talking about all things Homelab.
Tim just passed 100,000 subscribers on YouTube. Congratulations. Thank you. Thank you. Yeah,
it's crazy. And as part of that, I think you launched 100 days of Homelab initiative,
which we'll come to in just a minute. But before we get there,
usual plugs for things like the Discord at selfhosted.show slash discord. We've got over
4000 people over there talking about all things self hosted, maker spaces, home assistant,
all that kind of stuff. You all know as well that we've got a UK meetup coming up in August.
The date did change due to some flight stuff with me. So the new date is provisionally August
the 5th. We're still trying to find a venue. And now if you have any ideas about where we might
host this thing in London, I've been ringing around a few places the last few days trying
to find a pub with a big beer garden or something like that so that we don't have to hang out on
like a village green or something. You know, the requirements are outdoors has toilets has beer.
I think that's probably a pretty good recipe for a good meetup. So if you have any ideas about
where we might do this in London general area, let me know. I'm on Twitter at ironic badger.
So remember, that's the provisional date is August the 5th. And I think that's about it.
So it's probably about time we start talking about the 100 days of homelab.
What madman came up with 100 day hour a day challenge?
Yeah, me, I guess, you know, 100 days of homelab is something that I've been noodling on for,
I'd say, about six months. You know, I'm a software developer, and software developers
have had a challenge for a while. It's 100 days of code, and it pops up in my my feed everywhere.
And I think what a great initiative. You know, you you form a habit by doing something once a day,
for an hour a day, with a goal in mind and march towards that goal for 100 days. And by the end of
the 100 days, you'll learn something. Maybe you'll learn how to program. Maybe you build a website,
whatever it is, maybe you'll learn that actually, you don't like the thing that you signed up to do
quite as much as you thought you did. That is true, too. Yeah, very good point. Yeah, you might
discover this is not for me. Like I thought programming wasn't for me in college, but now
it's programming for me out of college. You know, I had that idea of, okay, how can I get people in
the homelab community one together and motivated and excited about about doing stuff? I'm excited,
I know a lot of people are, but, you know, just kind of bringing people together. And the whole
entire landscape of a homelab, if you think about it, it's gigantic. But, you know, I kind of think
of it as networking, storage, infrastructure, automation, a little bit of DevOps, some hosting,
you know, it's a lot of different things to a lot of different people. And, you know,
that landscape is is rapidly changing, you know, especially in the last couple of years,
if you think of infrastructure as code or, or anything, or storage, storage, now in
Kubernetes, storage everywhere, it's all changing, you know, software defined networks. And so I
thought, hmm, I have 100k coming up, and what can I do that has, you know, has as related to 100.
And that's what I thought, I thought, well, maybe I'll launch something around then. Yeah,
I was six months out. And I thought, you know what, I'm just gonna do what I do best and
procrastinate for six months until this, you know, this 100k subs, if it ever comes, but
it came pretty quick. So I had to get on the ball. Well, that's a nice problem to have.
You know, I, I'm not sure if I've ever told this story on air, but I did a computer science
masters, I was originally a trained musician, and then did a few years at the Apple store and went
and did a comp sci masters. One of the friends I made on that course went and did a PhD straight
away afterwards. And his PhD was into defining DevOps, I don't know how he managed to swing that
with his supervisor, but he did, and he got paid to do it. And I think this dude spent four years
defining DevOps in his PhD thesis. Do you know what the outcome of the four years was?
What's that?
There isn't a definition. It's too nebulous.
I like it. I like it. I like it.
If only someone would pay me for four years to do absolutely nothing. I mean, sorry, sorry, Steve,
if you're listening, but it's a, it's an interesting thing that you raised seriously that
DevOps, in general, means whatever you want it to mean. It really just, you know, it's just
really, truly it does. You know, to some people, it means that you're a cloud expert. To other
people, it means that you can write Terraform code. To other people, it means that you know
all about networking, right? It just depends on the problem that you happen to need to solve
that's in front of you this week. It's almost like just like a buzzword, like a magic word
that management don't have to hire specific people to do specific tasks. We want a generalist
that knows a little bit about everything. And I think that's typically where DevOps comes in.
That's a that's a good point. And it's, you know, it's it can be lumped as anything from
getting code into production and whatever that means. It could be, you know,
that's a huge chasm to cross. And it could be many different technologies. And yeah,
it's a, you know, a jack of all trades type of role, but very needed.
Oh, you've you've used a database, have you? Okay, so that means you're a DBA now.
That's right. That's right.
So what kind of stuff have you got planned? I've seen you've done, you know, your launch video,
your 100k subs video was pretty cool. You had 12 of the biggest home labbing YouTubers as a
massive collaboration. You know, it was Wendell, friend of the show on there, a bunch of other
people, Jeff from craft computing. What else do you have planned for the hundred days?
I'm on day four today. You know, but for me, it was just to kind of get people motivated about,
you know, getting into homelab, whether they are already doing it or rekindling that, you know,
that passion they have for it. It was just kind of getting everybody on the same page,
like created hashtag for it. And what I'm realizing through this hashtag is, is,
is that one people are using it by my Twitter has never been so active. I don't have a lot of
followers. So I'm like, Whoa, this is how almost famous people feel. What is the hashtag 100 days
of homelab? It's small, but people are using it. And what I'm discovering through this is,
you know, these, these updates that people are giving are very similar to my daily stand up to
my scrum, you know, as a software developer, you know, if you do scrum, you stand up and say,
you know, what did I do yesterday? What did I do today? And, you know, do I have any roadblocks?
And it's, it's very awesome seeing everyone just kind of chiming in with what they're working on
today, what challenges they're facing, and then seeing other people join in and talk about how
you could solve a particular problem or, hey, how did that work out for you? It's just been very
awesome. So I don't know what the long term plan is. If I get to day 100, and someone is on day one,
feel like that's a success. That means that, you know, this has gone longer than my 100 days and
someone else's journey is starting now. And so, you know, there's a lot of people said, Do I start
with you? Don't I start with you? Start when you want, like, you know, just because my trains
leaving today doesn't mean your trains leaving today, your train could be leaving next week.
Choo-choo. That's right. That's right. And so, you know, I, if I get to day 100, and I see a day one,
that means this whole thing was a success, because it's bigger than, than my 100 days,
you must have some kind of an overarching goal. Because, you know, the time I think about when I
was really probably the most active in terms of development over the last few years was just
before I immigrated, actually, I was, I was pretty stressed about the move coming up, you know,
England to America, and I just needed something to distract me. So I used, I spent hours writing
Ansible playbooks to completely Ansibilise the deployment of my server. And at the time, it was
mostly Ansibilised. But I'd done it three or four years prior. So a lot of the stuff I'd learned,
you know, as a consultant, for a while, I'd learned some tips and tricks, and I've learned some new
stuff. And I thought, right, I want to do it properly. And my goal was to do as much as I could
through one or two commands to deploy the whole thing. Is there something like that at play for
you here? Possibly, I mean, I've done some Ansible automation, I think just a couple of months ago,
you know, I created an Ansible playbook to create a high availability Kubernetes cluster,
along with load balancers all in one, because I saw that as a pain point for a lot of the people
that were using Kubernetes. So yes, it is. And so I found a whole bunch of forks that were left
abandoned, and I made them work. And so I, you know, automated a lot of that. But with this,
you know, I'm, I'm not sure. I mean, for me, it was really supposed to be kind of a, you know,
a celebration video, and at the same time, get people involved. But long term, I honestly,
I didn't think it was going to be, you know, turn into something as big as it is now.
Well, you've got to stop coming on people's random podcasts and talking about promoting it.
That's true. That's true. I mean, it's a great initiative. I mean, if there was ever, you know,
some learning company that wanted to do something and help people in infrastructure do something,
I'd be all for it. But honestly, no, no long term plans. It was an idea that turned into a video,
I got a lot of awesome people on YouTube to help me out. And that's where it stopped. For me,
it's, you know, it's obviously still going. Sounds to me, like you had this bright idea,
and you didn't think, what happens when I release this into the world? Like,
do I actually have to follow through on this thing? That's true. I've been, in fact,
that's this show, you know, for me. I'm living it, baby. That's right. But I bet. I bet. Yeah.
Like, I didn't think, how does this scale? What, you know, what's my long term goal?
Right. Which is ironic for someone who's as deep into Kubernetes as you are.
Yeah. I mean, usually my, you know, my videos do okay over time. But this one did really good,
really fast. And that's, that's not, you know, my typical, you know, release cycle for videos.
I released a lot of videos and tutorials and how to set things up on deeper topics. So I get lots
of views over time. And, you know, usually I'll get some tweets and, hey, how did you do this? Or,
you know, or someone on Discord will ask, how do I fix this? And, you know, basically,
like async tech support. But with this one, it was like, no one needs my help. They're all doing
it themselves. And but at the same time, they're all joining in. So it's, it's really unique from
what I've done in the past. So at this point, I suppose it's worth kind of defining what a
homelab is. And maybe you could tell folks how you got into homelabbing to start with. And,
you know, that kind of thing. Yeah, it's a it's a huge topic. I even have a hard time describing
homelab to people at work or anything else, because I think that, you know, that the term
has kind of evolved into a lot of things. The way I look at it is it's, you know, when you think of,
you know, you went to school, you had a computer lab there where maybe that computer lab, you were
able to set up certain environments, and destroy those environments or build them up or do whatever
you wanted to tinker in those environments. And that's kind of the idea, I think, behind homelab
is that you can set up an environment a safe place where you can set up an environment to tinker with
tools or to explore new technologies or automate some some infrastructure or geek out on storage
and networking. It's even turned a lot into self hosting stuff at home to people, you know, I say
it to they, they've kind of coalesced. And sometimes I'll, you know, I'll say, hey, yeah,
like the website, I'm self hosting it in my homelab. But, you know, it's more than a lab
then at that point, you know, it's, it's, it's, it's borderline. Just make sure when you're
tinkering, you don't take Plex down because the wife will come and find you. Oh, yeah,
Plex or DNS. Yep, that's I hear faster than my alerts. It's true. How do I know the Wi Fi is off
the kids are shouting? That's right. That's right. All Plex must be done. Yeah, it's it's so many
things to so many different people. So it's really hard for me to kind of summarize it because to me,
it means something different. But and to someone else, it means, you know, something different. But
for me, for a long time, it's been just a place where I can spin up stuff and test stuff and tear
it down without the fear of destroying, you know, the company's production. And a lot of times,
if you work at a big company, you don't have access to a lot of the stuff, either security
wise or physically, you don't have access to a lot of the tools. And then on top of that,
a lot of the times, the architecture is just, you know, decided before you get there. So you're just
implementing stuff. Deleting a production VPC is a rite of passage. That's right. Yeah, our work
sometimes Kafka accidentally gets deleted in our environments. But the worst thing I've ever done,
I don't think I've ever said this on air is I deleted a production load balancer. Oh, yeah. And
I didn't know what I'd done at the time. And then suddenly, all the senior developers came out of the
break room and were like, why are all of our alerts firing? I'm like, I was working on the load
balancer. Oops. I've been there before where you're like, I did just make a change. But I hope it
wasn't that. Yes, we made some changes to the CI process after that. So I mean, you could say
that Alex is screw up. Save the company money in the long run. Here you go. Yeah. Yeah, I'll take
it. I'll take it. Yeah, I mean, you you raise another good point. Like, I think a home lab,
it means different things to different people, much like DevOps does, I suppose. I mean, to me,
a home lab is I don't have a set by me, I suppose I do, because I didn't sell the Juulzeon box I
retired a year ago yet. But really, a home lab should be separate from any kind of production
services. You know, and I use the term production loosely at home, although maybe it's not that
loose, because I have stuff like Home Assistant now, and Plex, as we talked about, and a bunch
of other stuff that I actually do rely on, you know, Home Assistant, particularly to run multiple
facets of my house. So a home lab, to me at least, is something, a space that I can just break stuff,
and it doesn't matter. And it can come all different shapes and sizes, right? I mean,
I mentioned I had a Juulzeon box that I could use as mine. People use Raspberry Pi's. What other
stuff? Oh, anything. I mean, people are using old broken laptops without a screen. Some people are
using old PCs. My recommendation most of the time when people say I want to build a home lab is,
well, just upgrade your current PC so you get an upgrade, you know, and on the machine you use the
most and use the one that's sitting over there for your home lab. You know, have that be your first
one. And you get two for one. You get two for one. You get an upgrade on your main machine,
then you get a pretty nice machine for your home lab. And that's probably going to outperform a lot
of the things that you would you would buy otherwise or spend a lot on. I was just thinking
about upgrading, you know, consumer grade gear. And the thing that runs out first in a home lab
scenario always is memory. You always run out of RAM first. And you know, you think about using an
old laptop because it's got a built in screen, a built in keyboard, a battery for the UPS, that
kind of stuff. But you can typically only have 16, maybe 24 gigs of RAM if you're lucky. Yeah,
that's not enough to do a whole bunch with. Yeah, no. But a lot of people getting into it are just
getting started. You know, it might be enough to run a hypervisor, you know, three, four Linux
virtual machines and enough to kind of tinker with with something else, you know, and not destroying
their own production machine. But, you know, it spans the gamut. There's used enterprise gear.
Some of my old PCs got converted into rack mount PCs, and now they're in my server rack,
you know, all the way to the new enterprise, which I've even bought before. Super micro servers
sometimes are pretty affordable. But then yeah, all the way down to Raspberry Pis two, I have four or
five of those. So it's I you know, I generally thinking of any any, it can be any computing
device, I think, can be used mostly and in a lot of the same ways. Now over at wiki.selfhosted.show,
we have an SSH guest storage leaderboard. At the top of the list, we got Wendell with a petabyte,
who's clearly just showing off and nobody's going to touch that. But I have to ask you, Tim, how
many terabytes do you have on your LAN of raw storage? Oh, good question. I have a lot of RAM,
my disk shelf alone has 40 terabytes. I forgot you had that disk shelf. I mean, there's one,
there's one of your videos guys goes back about a year or so now, I think, where you do an amazing
job of taking us through all the different boxes you have in your rack. And you've even got some
pretty cool LED lighting in that server room, if I recall. Yeah, yeah, it's it's it's yeah,
it's pretty wild. It's similar to it's overkill. But it looks cool. Yeah, it is. It is. You know,
when I first started making content, a lot of gamers were doing it. Now, you know, I play games
too. And I thought, we can't let all the gamers have fun. People with servers can have fun too.
And RGB is how they have fun. No terminals, the only video game I need, baby. Yeah, that's right.
That's right. Or the Chrome, the dinosaur when you're offline. But yeah, I think I'll stick with
40 terabytes. I mean, I know I have, you know, a handful and desktops around here and Macs and
laptops, but it's I can account for 40 right off the top of my head. There's no way I'm getting
close to a petabyte. And I'm sure Geerling Geerling is there now too. He has a petabyte.
That's right with this petabyte PI project. My goodness. That was a cool video. It was. Yeah,
incredible. Incredible. But yeah, I can't touch those.
lino.com slash SSH go there to get a $100 60 day credit on a new account and support the show.
Linode has been rolling out upgrades to NVME storage on their rigs recently,
which offers much greater performance density or IOPS per gigabyte than traditional storage.
If you're a performance hound or your application needs that level of storage throughput,
Linode's team can help you sort through the possibilities for accessing the power of NVME
and arriving at the optimal storage configuration for your environment.
The Linode support experience truly is one of the most remarkable things about Linode as a company.
No matter what time, what day it is, you can open a ticket with a node and know that they'll take
care of you. No matter how silly you've been, whether you've decided to try and install your
own custom operating system on one of their nodes that is not supported, they'll still try and help
you. I can't imagine opening a ticket with some of the other big hyperscalers asking how I'd go
ahead and install some random distro and actually get a coherent reply from a human. Linode's been
doing this for a long time, 18 years in fact, and they just keep getting better. Go try it for real
and see for yourself at linode.com slash SSH. Linode makes it simple, affordable, and accessible
to deploy and manage your customer's projects in the cloud. Linode also has an easy to use and
powerful cloud dashboard with S3 compatible object storage, bare metal servers, cloud firewalls,
DDoS protection, and so much more. In fact, we use Nextcloud here at the network to run the backend
for all of our show storage. So when our editors need our files, for example, they'll go and pull
it down from Nextcloud and that's backed by Linode's fantastic S3 object storage. It just
means as an administrator of a Nextcloud server, I don't have to worry about how much disk space
is free. I just know that Nextcloud can go and create a new object in the bucket and we're all
good. And with pricing 30 to 50% cheaper than the other major cloud providers, Linode can be part of
your multi-cloud strategy. Use our $100 credit to performance test your network so you can see just
how good Linode is for yourself. Go and grab yourself that $100 credit and 60-day free trial
over at linode.com slash SSH. That's linode.com slash SSH. Now I think the thing that I found you
for first was Kubernetes content. You were doing a bunch of stuff with Rancher, I think at the time,
and K3S. As an OpenShift guy at work, obviously my propensity is to use OpenShifty type stuff,
but actually I do like to try and keep my skills in the real Kubernetes world without a lot of the
OpenShift magic that goes on with the routing layer and all that kind of stuff. And so that
leads me down the path of looking at K3S. And I always, always, always find myself in this dichotomy
of I want to have something at home to learn on, but my goodness, is this complicated and overkill?
You know, it is. Well, it could be, but I ask, what is overkill? Is overkill 40 terabytes of
storage? Is overkill a V8 engine in your car? Is overkill half a terabyte of RAM? What is overkill?
So, you know, yeah, I've made a lot of K3S content and you hit the nail on the head. Like when I
built my Ansible playbook for K3S, it was to solve a lot of the complexity of setting it up, because
there's a lot of complexity in just setting it up, let alone everything you need to know about
Kubernetes later on. But I think K3S is a, one, it's a fantastic product. It's an easy lightweight
way, air quotes on lightweight, way to run containers on the edge. You know, with it,
you get a full, mostly full Kubernetes API. And you know, for a lot of things, it might
seem overkill, you know, Hey, I'm running one container of everything in my, you know, my,
my Docker stack or portainer or whatever you're using to manage your Docker containers.
But what happens if you want to run two? What happens if you want to make sure that they're
always up? What happens if you want to do that declaratively and, you know, create YAML for
all your deployments? So it's repeatable. How do you handle storage, you know, on, on your single
node, single nodes, pretty easy, but you know, how do you handle it if you have more than one?
And so, you know, Kubernetes asks a lot of those questions of containers and you're left to kind
of figure it out. But for the most part, once you get going with it, I think, you know, like,
like me when I caught the DevOps bug or engineering bug or software development bug, you know, it's,
it's something that you can go really deep on really fast and maybe never come back from.
It definitely is a lot of fun, but it, it, there's a lot of learning involved.
But I always find myself thinking, right, I've got two or three Raspberry Pis sat in the drawer.
And the whole purpose of doing this would be to have a highly available,
I dunno, Git server, web server, whatever it is, basic services like that. I don't think I would
do things like Home Assistant in, in Kubernetes cause it's, it's best suited as its own VM.
And we'll just, we'll forget about that. But there are certain services in the, in my overall kind of
self hosting world that would be pretty cool. You know, at the moment I run everything on
my storage server. So if a disk fails, I have to take that thing out and shoot it in the,
no, I don't do that. I take the disk out and the machine's offline for an hour or two,
or maybe longer if I'm doing some data transfer, I will actually stop all the containers on that
box so that nothing's reading and writing to mergerFS and doing all that kind of stuff.
And I find myself thinking in those moments, which admittedly is only
once a month for a few hours at most, I think, Oh, it'd be great if this was self healing and
that web service had just moved over here and its underlying storage had also replicated and
also done all. And it's just those, all those extra thoughts of, well, but then I need to solve
this problem and then I need to solve that one. And then I need a load balancer and then I need
to replicate the storage and all that kind of stuff. So what's the lowest barrier of entry
to a highly available Kubernetes set up in a home lab scenario?
Oh yeah, good question. So, I mean, you hit the nail on the head with all the challenges you'll
start to face. Those are the known ones. There are a lot of ones you don't know until you get into it.
But the lowest barrier of entry, I think the minimum available, there's a couple of ways
you can do it. With K3S, you can use etcd for your, your, your Kubernetes database,
or you can use a MySQL database, which is external. At the end of the day, you need at
least three nodes for quorum for them to vote. But if you're using the MySQL version, you don't
need quorum because the MySQL database acts as your database. So you, nodes don't need to vote
or it is the tiebreaker. So how does that work? etcd is explicitly designed for Kubernetes,
at least that's the way it feels. I know it wasn't originally, but
it's very lightweight. It's very good at maintaining quorum and the performance
at scale is excellent. MySQL, not so much. Yeah, no, I totally agree. So if you choose
the etcd route, it's going to be very chatty, but highly available. It's going to be replicating
all of the data across all of those nodes. It will have some kind of performance impact if
you're using Raspberry Pis with micro SD cards, probably not the best storage for something that
reads and writes often. But with MySQL, if you have it there, you can run that anywhere. It
becomes your state for where these nodes go and, and look up their state. And then your
database for everything in Kubernetes. But I think the performance is, is good enough,
good enough for, for nodes and good enough for, for k3s. I think otherwise they wouldn't
have chosen it. I suppose I get caught up sometimes in, you know, thinking about this
from my day job and thinking that I must do things properly. I must do it with etcd when
actually probably MySQL, certainly from your description there sounds actually like in some
cases it might be a decent choice. So what do you do? Do you put MySQL on it? You know, let's say
you've got three pies running k3s. Do you then have a fourth that's just dedicated as a single
node for MySQL? Well, if you, if you're doing MySQL, you could do two nodes with a MySQL database
anywhere in the environment that it can communicate with. It doesn't even have to be a
Kubernetes node. So you can have two Raspberry Pis and then your MySQL database wherever,
anywhere else, as long as it can reach, you know, MySQL over TCP. And how's the complexity of
setting up that replication? You don't have to do anything. There's nothing you need to do with k3s.
All of that is obfuscated from you in general, like with etcd or MySQL. You don't need to know
how to do that or how to set it up. Not saying that, you know, you might not have to troubleshoot
it sometimes. But for the most part, it's pretty solid. I'm a huge fan of the etcd way because,
you know, you can spin up nodes, add nodes. And I mean, you could do that with the MySQL version.
More industry standard too. So if we come back to one of the original goals of learning,
right, if you're doing things at home in a very custom way, you could argue that certain businesses
and certain shops will have a huge amount of custom code. Certainly older, more legacy shops
from, let's say, more than 15 years old, let's say, before the cloud was really a thing.
They'll have a lot of on-premise infrastructure that you'll go and you'll read the wiki if they
have one and you'll scratch your head and be like, why did you do it that way? Because 20 years ago,
there was no other way, mate. That's why. Yeah, exactly. So, you know, there is that to contend
with if you think about doing the MySQL route is it's not an industry standard way of doing things,
whereas etcd is. So, you know, you've got a few pies now running K3S with, let's say, etcd as the
backend. What next? So the next thing I highly recommend doing is going figuring out storage.
Well, there's two pieces. It's choose your own adventure. Remember, I said the terminal is the
only video game I need, baby. You see what I mean? Yeah, so those are the two things. And I recommend
people like figure that out up front. I know most people when they build a cluster, they don't even
have K3S in mind and maybe they do, but they're more focused on the service that they want to
run. They want to run WordPress or Ghost or Plex, maybe. And so they're really focused on that.
And sometimes I have to remind people, OK, before you do that, figure out storage and load balancing
because that's that stuff to figure out. I would probably say figure out storage too, because
almost every stateful application, so stateful applications and Kubernetes are ones that
write state or keep state in memory. But for ones that write to a volume to disk, you need to figure
out storage. And you can do the, hey, put it all in NFS. But then you're taking, you know, you're
taking this highly available service, K3S, and making it, you know, putting in a single point of
failure, which is probably your NFS. It's the same thing with MySQL too. And when we were talking
about that earlier, the reason why I don't choose that is because you're just, you know, you're
taking something that's highly available and all of a sudden you're making a single point of failure
be your MySQL server. And then you have to make your MySQL server HA to make that HA. And so it
just grows exponentially. It depends on how rigid you are about making things highly available.
And so you have a lot of choices, you know, but for storage, it's really going to be up to you
if you want to make it highly available. You could dump everything in NFS and that's fine.
Or you could choose things like Rook Ceph or Longhorn. There are options.
Why didn't I take the blue pill? That's often what I end up thinking at 2 AM when I've started one
of these ludicrous adventures down that particular rabbit hole. So let's presuppose that we now have
a running Kubernetes cluster with a load balancer with storage and everything's working. We've got
a completely empty cluster. Now what? Where do people find apps to actually run on this thing
that are compatible with Kubernetes? Yeah, good question. So most, most, I'll say most, air quotes,
most containers that are built on Docker are compatible with Kubernetes because Kubernetes
under the covers is now using a different container runtime. It's not important,
but it's compatible with Docker images and Docker containers. So one, anything you were previously
running in Docker most likely is going to run in Kubernetes and that's how it was designed to work.
Something you'll need to pay attention to, and I kind of hinted at it a little bit was,
was, you know, stateful applications. You'll need to make sure that that application you have can
scale. Everybody thinks like, oh, you know, I'm running Plex. The way to make it highly available
is spin the replicas up to three. It's not going to work. It's not going to work. Yeah. So if,
if things weren't built to be stateless, you're not going to be able to scale them. You'll get
some other benefits like they could bounce around on nodes, but you can only run one.
So it's almost a bit like RAID in that regard, right? It's not designed to increase your
resilience necessarily. It's designed to increase your uptime. So the whole sales pitch behind
Kubernetes that kind of got me excited about it in the beginning was, let's say you had a Plex
instance running on node one and node two and three are just sat there chilling out, doing nothing.
Node one has a hardware failure and Kubernetes is running a loop constantly checking the state
of these things. And every time that loop executes, it's saying right on node one,
this pod exists, Plex exists, and it matches the state declared in the YAML file that Alex put in
place. Cool. Everything's hunky-dory and it will carry on doing that loop. I don't know what the
frequency is, but it's many times a minute that that typically happens. Now what happens when node
one has a hardware failure or drops off the network or just crashes if the application crashes
for some reason? Well, Kubernetes is going to come around and do its health checks and make sure
that everything's tickety-boo. And it's going to say, hang on a minute, the desired state over here
doesn't match what I'm expecting. Well, what I'm going to do is I'm going to utilize a different
node that matches the node selector rule that you've put in here. Let's say it's a node with
quick sync for transcoding, for example. Not every node in your cluster might have a GPU available
to do that. The loop will go around and it will say, hey, okay, well, out of the five nodes in
this cluster, I can use these two. And now I'm going to take the Plex pod and I'm going to make
sure it's destroyed over there, but I'm going to spin up a new one over here. And then I'm going to
tell you that that happened in your log and alert you about it. That's, generally speaking, the
typical use case for a stateful application in Kubernetes. Like Tim was saying, it's not
to have three copies of Plex running at the same time because the database writes. And if you think
about how the data would flow in that transaction, you're coming in to watch video. Well, which
version of the Plex runtime are you hitting? And then that version of the Plex runtime has probably
got its fingers in the database somewhere. And how does the database know which one to listen to? And
it can get very confusing very quickly, which is why a lot of dev shops have to architect things
in a way called the 12 factor app. If you're interested, go and look at 12factorapp.net.
I think that's the website. Hang on. Yeah. Yeah. Good site. Good reference to 12factor.net
is the website. There'll be a link in the show notes. There are a few different ways to run
containers on Kubernetes. Like you were saying, OCI compliant containers, typically that's mostly
Docker containers. There are a few others under the covers as well. There is a project called
Kubernetes at home, which there'll be a link to in the show notes. Kate's at home. And this is a
fantastic resource. If you're not familiar with it, go check it out. You can go over there and
download Helm charts and all sorts of other stuff to run applications on top of your Kubernetes
cluster. And a lot of other smart people have done a lot of the legwork for your thinking about
how do I run an application that wasn't designed for the Kubernetes world to make it run in the
Kubernetes world. Stuff like user management. We're all familiar with the group and user ID stuff
from a normal Linux Docker host. There's some tweaks you've got to make in the Kubernetes world
to translate that stuff across multiple nodes because it's not just typical Linux permissions.
There's an extra layer on top. And there's lots of other small gotchas like that, like Tim was
saying. There's plenty of stuff that you don't know what you don't know until you find out you
don't know it. And it's a deep rabbit hole, but it's one that if you've got any interest in,
I highly recommend you give a look to Tim's channel, as well as the Kubernetes at home
stuff that's linked in the show notes. Yeah. I've worked with Kubernetes at home folks,
a couple of them there. I'm in their community. That's how I got bit by the flux bug.
And declaratively defining your whole entire Kubernetes clustered through manifest.
Yeah, that's a great call out. The Kubernetes at home Helm charts are fantastic,
especially for people self-hosting because they went after a lot of the services that people are
self-hosting. And if you just want normal, I shouldn't say normal, but publicly available
Helm charts for services like NGINX and all these enterprise services, those are out there. Those
Helm charts are out there. But what the Kate's at home Helm charts are, they're going after Plex,
they're going after Sonar, Radar, all these services that people like to run at home
and building charts for them. And if you even search some of the Helm chart repository
aggregators, they reference their charts too. So yeah, they've been a huge help in getting me
on the flux, which is a totally different topic, but it's pretty far down the rabbit hole.
I got absolutely hooked by these guys when I found out I could run a Factorio server
on my Kubernetes cluster. I just thought it was the coolest thing in the world.
So talk to me a bit about GitOps and flux and that kind of thing. We've talked to,
obviously I rambled a lot a minute ago about declarative state and how there's this loop
in Kubernetes that is constantly checking the state of things. GitOps takes that to another
level, right? It does. So GitOps is a lot more defined, I guess, than DevOps. So GitOps is this
idea that you define your cluster state or your environment state in manifest 100%.
And the way that you influence the state of a cluster or infrastructure is by doing it through
Git. So for example, I just went through this exercise. You know, I needed to get, let's just
say an Nginx container. I would add an Nginx manifest, whether I'm using Helm or Kubernetes
manifests, I would create that manifest. I would create that manifest. I would commit it to Git,
and I would push it up. And then there are services within Kubernetes that say,
Hey, I just got this manifest. I'm looking at the current state. I'm looking at the desired
state and I will apply it. And so GitOps basically says that you, the only way you can influence
state is really by influencing Git, but they say through a pull request could be anyway.
But now my, my whole entire cluster at home is that way. And I think the benefit of that is,
is that I can reproduce my whole entire Kubernetes cluster by just saying, you know,
kube control apply or use flux to do it all over again and rebuild my whole entire cluster.
Now data is a different story. I would have to do some restores and data to get those,
those persistent volume claims back. But at the end of the day, I have my whole entire playbook
for how to build my cluster. You know, if you're running your own cluster,
you can do one-offs and do all these weird stuff. You know, you know how it is to tinker.
That's just it. That's just it. GitOps at home might seem like massive overkill,
just like ansible-izing your server might seem massive overkill when it's just you.
That's right.
But if you take these principles into the workplace,
I guarantee you it's going to make you more employable, you'll earn a bigger salary.
But as a team, it just makes your life so much easier because you're not like,
who's done this to the load balancer? Alex, was it you? No, it's in Git. You can go look at Git
blame and you know that it was Tim that broke the load balancer, not Alex this week.
Yeah. Don't let, don't let him. Yeah. And it's, it's, it's nice because, you know,
places I've worked at, sometimes developers have full access to Kubernetes, which,
which is fine. That's a choice made by the company. But anyone can then go kubectl apply
or kubectl delete everything. And then it's gone, you know, and so GitOps basically says,
no more of that. We're separating our concerns. You know, if you want to get things into Kubernetes,
you do it through Git and then you use, you know, a controller or service to apply those to
Kubernetes. So it's, it's very interesting. It's very, it's very bleeding edge. And a couple of
places are doing this now for a couple, you know, flux is one Argo CD is another. And I'm sure
there's a handful of others that are coming up. Even GitLab themselves does it or Tanner does it
for Docker. I mean, this is a hot topic, but it's, it's really awesome. But at the end of the day,
now as a developer, I'm like, okay, I can't, I can't make changes to Kubernetes directly,
which I'm fine with. I'm fine with process. It has upsides and downsides. The upside is,
like you say, everything is declarative and it makes rebuilding stuff really, really easy.
The downside is you have to make every single change that way, no matter if it's a one
character change to a config file and whatever your peer review process is, you know, in a home
lab, it's likely to be you going, yes, yes, mash, mash, mash, yes, yes, yes. Whereas at work, you
know, you might have to get your team lead to approve it and, you know, explain why you missed
the semi-colon off the end of a line or whatever it might be, you know, that kind of thing. So it
has its upsides and downsides. I think for me, the upsides do outweigh the downsides simply because
of everything we talked about. Yeah, I agree. My notes section that I used to have on how to
reconfigure my Kubernetes cluster in the case of an event is gone. It used to be a long list of
helm commands that I used to run, kube control, you know, commands that I run along with manifest,
manifest, and now that's just gone. It's just, no, my documentation is the code and I hate it when
people say that, but it's true. Now my code is the documentation and documentation is the code.
There's no other way to do it. Well, it's the source of truth. It's the most up-to-date version
of what's in production. I mean, the only other more truthful source would be actually production,
but if you've been doing it all through GitOps anyway, they should be the same.
That's right. Yeah, yeah. It's a pretty strict principle, but I enjoy it. You know,
Argo CD is another one that I've been wanting to play with. It's pretty cool. It's a lot more
visual. You know, even before I was doing give up GitOps, I would still deploy stuff through
Kubernetes. I'd still use CICD, you know, pipelines, commit manifests, and then have,
you know, kube control apply during CI. This is a little bit different and it's pretty awesome.
Pretty awesome. It's fun to explore.
tailscale.com slash self-hosted. Go there to get a free personal account for up to 20 devices and
support the show. Tailscale is one of my absolute favorite discoveries of the last 12 months. It's a
zero config VPN. You can install it on any device in minutes and manage the firewall rules for you
and it works from anywhere. I've been able to close all the ports in my firewall, thanks to
the outbound NAT punching that tailscale does. And this means that no matter where I am in the world,
I can connect to my LAN as if I'm sat in this chair. And it is so great. I can hardly explain
to you guys. I can also, thanks to their amazing subnet router technology, connect to the Synology
box running at my mom's house or the server I have running at my dad's house as if I'm on
their LANs as well. For me, one of the best parts is every device on my network gets a stable IP
and auto-assigned domain that stays consistent no matter what network the device is on.
Devices only connect after signing in through your existing identity provider. This means you
can more easily enforce multi-factor authentication and deauthorize those who you've shared things
with who you perhaps wish you hadn't. You can try it for yourself for free for up to 20 machines
at tailscale.com slash self-hosted. And just imagine, what if this is how the internet worked?
What if every machine had a static IP and a DNS name and that address migrated around the world
with you and it was always encrypted and you never had to worry about certificates and all of this
just happened automatically? That's Tailscale. So go and try it out for yourself for free for
up to 20 machines at tailscale.com slash self-hosted.
Jerry writes in, hey guys, in response to the Wi-Fi enabled E Ink device mentioned in episode 72,
I thought you should check out pine64.org slash pine note. Hey, Jerry, I just want to say thanks
for writing in with this one. I mean, I know the conclusion that Chris and I came to in the last
episode was that for the most part, wherever you're going to want an E Ink display, there's
going to be power and therefore a cheap tablet might make more sense. But if you really, really
do want an E Ink display, you can go ahead and look at this pine note developer edition. It is
$400. So what we were saying about E Ink displays being expensive, definitely holds true with this
device, but it does look cool. And obviously, you know, supporting the pine project is a noble
endeavor. Those guys do great work over there. So you can go ahead and take a look at the link
in the show notes to the pine note. And Joshua also writes in, I've been running Linux since
high school and I've been self-hosting almost as long. I am currently getting my masters in
cybersecurity engineering and I've been using the skills I learned to try and make my systems
more secure. One tool I learned about recently is Linus. I think that's how you say it anyway,
L-Y-N-I-S. This scans your configs and gives you suggestions to improve your security.
Do either of you run any tools to help ensure you aren't making any obvious
security mistakes? Now, Tim, I know that you do some stuff like this in your day job. I
wondered if you had any suggestions for Josh. Yeah, great question. I was actually working
on this today. There are a lot of tools you can use to do analysis on the things you use. For
example, I was setting up a container image scanning today to scan Docker containers to look
for vulnerabilities, known vulnerabilities that are higher critical, and if they were
addressed or not. So there are lots of things you could do there like that. You can scan them either
at rest in a container registry, or you can scan them even during runtime if they're in
Kubernetes. And then if you are writing code, there's a lot of static code analysis tools you
can use too as well to scan and look for vulnerabilities in either your code or
dependencies that you're using for your code. Trivia is one that I was using today to set up,
and it's really, really awesome. And it's open source, and they do a lot of scanning of a lot
of different types. They're kind of an all-in-one now where they can scan code, dependencies,
and containers. So now when we see a video come out from you in a few weeks time on that,
I'll be like, hey, you heard it here first. You know, I think for me, cybersecurity is one of
those things. It's obviously a buzzword in certain areas, but just not being the tallest nail is the
name of the game. Don't do silly things like open ports in your firewall you don't absolutely need.
I mean, for me, since discovering Tailscale, I've actually been able to close every single port in
my firewall. I don't have anything open anymore, not even WireGuard like I used to, because
Tailscale does all the outbound NAT punching that I need to get past my own firewall so I can connect
to my LAN as if I'm here wherever I am in the world, which is just great. So stuff like that,
you know, don't open ports in your firewall, I think is a super basic but really important
principle. There's also stuff like using SSH keys or certificates if you want to, using TLS to make
sure it is actually your website that you're connected to through Let's Encrypt. There's
really no excuse these days not to, but I think beyond that just don't be the tallest nail. You
don't use a silly stupid password like password123. At least make some basic effort, that kind of
thing. Use a password manager, that kind of stuff. I think that's all you need to do really.
So Jaqen writes, I love self-hosted and all the rest of the JB shows. I started listening
exactly on the very last last episode and have been a fan ever since. Jaqen, I almost shed a tear
on the last episode at last, I've got to be honest with you. I know Chris isn't here, but I was a huge
fanboy of JB for many years before starting this show and I owe a lot of what I can say and talk
to about Linux, I think to JB as a whole. So I feel you there man, I feel you there. Now he continues,
on episode 71, a listener asked about thoughts on alternatives to the Raspberry Pi.
I think the Orange Pi makes a decent low-cost board. They sell for around 24 bucks and you can
get them directly from the manufacturer. Now Tim, I know you're a bit of a Pi fiend as well as I am
and Chris too. Have you ever heard of these Orange Pis? You know I've heard the name but I can't tell
you how they differ. I can't. I have lots of Raspberry Pis, I even have a Turing Pi 2 and
the only non-Pi device I have is an NVIDIA Jetson, but I haven't heard of them. I need to look into
them especially if they have a better supply than Raspberry Pis right now. Well that's just it isn't
it? Raspberry Pis are unobtainium, so no matter how much we might wax lyrical about them, if you
can't find them then it's pretty much no good to anybody. So yeah, maybe these Orange Pis are the
way to go. They look like a pretty cool board. They're ARM64 based which is, you know, they can
be a bit of a problem sometimes to find applications to run on these things but for a small little
headless box, maybe they'll do the trick. Who knows? If you have been running an Orange Pi
in anger, please write in and let us know at selfhosted.show contact. Now I want to say huge
thanks to Tim for stepping in at the last minute here to help me co-host the show. Thank you Tim.
Yeah thanks for having me. Huge fan. Is there anywhere else you'd like to send people? I mean
I imagine you've got some channel on YouTube. Yeah, yeah, channel on YouTube. Yeah, just Google
Technotim or use your favorite search engine and look for Technotim or just Technotim.live. That's
an easy way to get a hold of me. Very good. Thank you very much for joining us. Now I want to say
also a big thank you to our site reliability engineers, our SRE subscribers. You make the
show possible over at selfhosted.show slash SRE. You can also go and support the entire
Jupiter Broadcasting Network over at Jupiter.party. Don't forget as well that we have the London
Meetup coming up on August the 5th. More details to follow on that soon and again if you have a
venue recommendation please I would love to hear it. As always you can get in touch with us at
selfhosted.show slash contact. That's the place to go to get in touch with us and you can find me on
Twitter at ironicbadger. I'm at Technotim live on Twitter. Very good and thanks for listening
everybody. That was selfhosted.show slash 73.
