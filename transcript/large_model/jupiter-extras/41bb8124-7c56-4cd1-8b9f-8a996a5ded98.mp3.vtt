WEBVTT

00:00.000 --> 00:03.360
So we have a very special guest coming on the show this afternoon.

00:03.600 --> 00:06.960
And he is the developer behind Merger FS.

00:07.320 --> 00:11.960
But joining me in studio today is the host of Choose Linux, Drew.

00:12.400 --> 00:12.920
Hello, hello.

00:13.080 --> 00:15.880
And I also have the one and the only Brent.

00:16.040 --> 00:16.880
Well, hello.

00:17.040 --> 00:22.560
And the reason we have these two folks is Brent has never used Merger FS before.

00:22.920 --> 00:24.480
He is a curious mind.

00:24.480 --> 00:30.640
He is looking to build the ultimate photo backup storage solution.

00:30.640 --> 00:31.120
Is that right?

00:31.520 --> 00:33.840
I don't know about ultimate, but I need something that really works for me.

00:33.840 --> 00:35.720
Yeah, that pile of external hard drives.

00:36.200 --> 00:37.880
Don't tell them too much about that.

00:39.400 --> 00:40.000
Oh, my goodness.

00:40.000 --> 00:44.440
I was watching a video with Linus Tech Tips and the guy behind, what was it?

00:45.240 --> 00:46.160
Smarter Every Day.

00:46.960 --> 00:52.160
And Linus went to this guy's house and he lives in Alabama near the, near

00:52.160 --> 00:53.520
the like rocket city, what's it called?

00:53.520 --> 00:54.960
Just outside of Atlanta anyway.

00:55.440 --> 00:58.400
And this guy had huge plastic totes.

00:58.400 --> 01:02.600
I'm talking, he had over a hundred terabytes worth of external hard drives, all

01:02.600 --> 01:03.440
neatly labeled.

01:03.840 --> 01:06.280
And this guy went to a spreadsheet and looked at, right.

01:06.280 --> 01:08.000
I want to reference this project.

01:08.000 --> 01:09.800
I want to look for Apollo landings.

01:10.040 --> 01:12.280
I need to go to hard drive Q37.

01:12.280 --> 01:16.600
So you go to box Q, get out hard drive 37 and copy that folder to his.

01:17.000 --> 01:19.720
Was he a librarian before he started doing this?

01:19.760 --> 01:20.560
I have no idea.

01:20.560 --> 01:22.440
Did he use the Dewey Decimal System?

01:22.440 --> 01:27.840
It's a heck of a system when we have projects like Merger FS in the world.

01:27.840 --> 01:31.680
So, Drew, what's your, what's your background with Merger FS?

01:31.920 --> 01:37.280
I've been using it since I found your blog post talking about Merger FS and

01:37.320 --> 01:40.080
it's joys and I've been loving it.

01:40.080 --> 01:45.640
I run it on a Ubuntu 18.04 server with a bunch of Docker containers and

01:45.680 --> 01:46.840
Linux containers you mean?

01:47.320 --> 01:47.960
Well, yeah.

01:48.000 --> 01:49.000
I mean, okay.

01:49.000 --> 01:52.280
So yes, they are kind of like, they're like, they're like,

01:52.280 --> 01:54.320
containers running under Docker.

01:54.320 --> 01:55.960
I call them Docker containers all the time as well.

01:55.960 --> 01:56.960
I'm just as guilty.

01:57.120 --> 01:58.600
Well, I'm using it with Docker.

01:58.600 --> 02:01.280
If it were Podman, then they'd be OCI containers.

02:01.280 --> 02:01.800
Correct.

02:02.480 --> 02:05.320
So the funny thing is before the show, actually, we were just talking and I

02:05.320 --> 02:09.320
showed Drew my blog post from 2016 and he went, I've read this before.

02:09.320 --> 02:09.720
Yes.

02:09.880 --> 02:12.880
Before we, long before we met, cause we first met at LinuxFest Northwest this

02:12.880 --> 02:13.600
year in April.

02:13.720 --> 02:14.080
That's right.

02:14.400 --> 02:19.440
And, uh, it's just funny how the internet is so, it's such a small community

02:19.440 --> 02:21.000
that you never know who you're going to run into.

02:21.160 --> 02:22.040
Yeah, it's a little world.

02:22.040 --> 02:26.480
But yeah, it's, uh, that got me up and running with, uh, MergerFS and SnapRate

02:26.480 --> 02:29.800
on top for Parity and I am a big, big fan.

02:29.800 --> 02:31.920
So I'm really excited to talk to Antonio here.

02:32.160 --> 02:32.960
And what do you do with it?

02:32.960 --> 02:34.160
Is it mainly media files?

02:34.520 --> 02:39.000
It's media, Nextcloud, mail, uh, you know, I got a little bit of everything going

02:39.000 --> 02:39.200
on.

02:39.200 --> 02:42.600
It's a big, big server with lots of RAM and lots of storage.

02:42.600 --> 02:45.400
And so you're not running databases off this thing then?

02:45.680 --> 02:47.760
Um, well, there's a Nextcloud database.

02:47.760 --> 02:48.320
Does that count?

02:48.960 --> 02:49.720
Maybe not.

02:49.920 --> 02:50.720
Yeah, maybe not.

02:50.720 --> 02:52.400
I mean, nothing, nothing high power.

02:52.400 --> 02:54.760
For your container app data, how do you handle that?

02:54.760 --> 02:55.920
Do you have a separate mount point?

02:56.080 --> 03:00.400
Yeah, so I found that MergerFS wasn't very good with databases, so I ended up

03:00.400 --> 03:06.360
moving all of my databases and what I call control for my containers over to an

03:06.360 --> 03:12.320
SSD and then it's just data, just dumb, dumb data living on the MergerFS pool.

03:12.320 --> 03:13.800
And it does a great job.

03:14.160 --> 03:15.440
Write once, read many.

03:15.440 --> 03:16.400
That's where it excels.

03:16.520 --> 03:17.240
Exactly.

03:17.240 --> 03:21.040
Large, static, often unchanging data sets.

03:21.040 --> 03:24.560
And so we've heard from Drew and Brent, uh, I'd like to pull in Antonio at this

03:24.560 --> 03:29.160
point, who is the main developer, I think the only developer of MergerFS actually

03:29.160 --> 03:30.160
looking through the GitHub.

03:30.160 --> 03:36.400
He has 69,000 lines of code to his name and the rest of the world has about 70.

03:36.400 --> 03:38.600
So Antonio, it's great to hear from you.

03:38.800 --> 03:39.640
Thank you for having me.

03:39.960 --> 03:40.920
Well, thanks for being on the show.

03:40.920 --> 03:47.960
So first of all, I wanted to, uh, ask you, what is MergerFS in your own words?

03:48.280 --> 03:51.920
It's a, uh, union file system.

03:52.680 --> 03:53.160
I don't know.

03:53.200 --> 03:58.760
Uh, I ha I think my, uh, tagline on, uh, on GitHub right now is a

03:58.880 --> 04:01.600
featureful, uh, union file system.

04:02.200 --> 04:09.840
It's just a simple way of, of taking a union of all the directories and files

04:09.840 --> 04:15.040
that you may have on a, uh, across a number of file systems or even the same

04:15.040 --> 04:17.280
file system, but from different, uh, paths.

04:17.840 --> 04:26.320
So it's more or less just a proxy with some smarts for routing where the, uh, the

04:26.320 --> 04:32.040
specific functions of a file system, uh, get, uh, dispatched to.

04:32.520 --> 04:37.560
So it's, it's really just, uh, like any other proxy, uh, mostly, I mean, there's

04:37.560 --> 04:42.240
some additional features that are relevant for, uh, the file system space.

04:42.360 --> 04:45.600
But if you think of a web proxy or anything else, it's

04:45.600 --> 04:47.440
largely doing the same thing.

04:48.000 --> 04:54.880
And, uh, the purpose is just to simplify that, that workflow, that, that behavior.

04:54.880 --> 04:59.680
There's many use cases for unionizing, uh, existing file systems.

04:59.920 --> 05:04.800
And, uh, there are many different implementations and I created MergerFS

05:04.800 --> 05:11.240
due to a, I think, uh, a technical limitation that existed in, in the

05:11.240 --> 05:14.520
different popular, uh, implementations that were out there at the time.

05:14.960 --> 05:16.520
Yeah, I use it a lot.

05:16.560 --> 05:20.840
And, um, the way I would explain it is it allows you to take just a bunch of

05:20.840 --> 05:24.880
drives and make them appear as if they're an array of devices, if you like.

05:24.920 --> 05:31.320
So you could have five, 10, 20, uh, different hard drives, each with an

05:31.320 --> 05:35.520
individually readable file system on it and present that under a single mount

05:35.520 --> 05:37.640
point for your applications to use.

05:38.040 --> 05:41.400
Is there a limitation out of interest on the number of devices that

05:41.400 --> 05:42.920
we can put together under Merger?

05:42.920 --> 05:44.160
Not a specific one.

05:44.560 --> 05:48.320
Um, 65,536, I guess, right?

05:49.200 --> 05:53.000
Not even that it'd be the amount of RAM that you probably have.

05:53.040 --> 05:57.520
Uh, I think you're going to run out of, uh, connections before you run out of,

05:57.520 --> 06:01.520
uh, like physical connections to mount drives before you run out of, uh, the

06:01.520 --> 06:03.160
ability to, to merge them together.

06:03.520 --> 06:06.520
Uh, I had a question about, um, you mentioned there was a feature that was

06:06.520 --> 06:10.920
missing, uh, in most of the popular, um, system alternatives that you were

06:10.920 --> 06:11.920
looking at previously.

06:11.920 --> 06:15.560
Um, can you, can you mention that specific feature that kind of made you dive in?

06:16.200 --> 06:20.040
Well, I think the biggest, this might not be a feature per se, but support

06:20.280 --> 06:26.280
was a big one, um, MHTDFS, which I think a lot of people, uh, used to use and

06:26.280 --> 06:31.800
I find, uh, a number of people still use, um, the author had abandoned it quite

06:31.800 --> 06:32.640
some time ago.

06:32.640 --> 06:35.880
That was one of the primary reasons I started looking into this space.

06:36.480 --> 06:42.200
And, um, when I did, I recognized quickly that there was a lot of, uh, security

06:42.200 --> 06:48.040
issues with the software and, um, lack of configurability.

06:48.080 --> 06:54.680
So I think one of the biggest features for MergerFS versus MHTDFS is the ability

06:54.680 --> 07:01.960
to choose what I call a policy, um, in the, the way in which it figures out which

07:01.960 --> 07:05.520
drive or which branch to work on within the union.

07:05.920 --> 07:10.160
So this is a really interesting feature of MergerFS that I absolutely love.

07:10.280 --> 07:12.360
And this is the create policies.

07:12.840 --> 07:16.240
So how, how would you explain it to the layman, right?

07:16.240 --> 07:21.040
Cause the, the, the first, uh, and the default setting is existing path, most

07:21.040 --> 07:21.760
free space.

07:21.760 --> 07:23.840
So, I mean, how would you explain that to the layman?

07:23.840 --> 07:27.720
Uh, so most free space should be somewhat obvious.

07:27.760 --> 07:31.400
You know, each drive has X number of bytes.

07:31.440 --> 07:37.080
And if we look at the more simple version of that, those, that policy is just most

07:37.080 --> 07:44.360
free space at the time a create request comes in, it simply looks at all the drives

07:44.360 --> 07:50.560
at that time, takes a snapshot of those values of how many bytes are free and just

07:50.560 --> 07:51.640
picks the one with the most.

07:51.640 --> 07:53.200
Uh, it's a very simple algorithm.

07:53.520 --> 07:57.920
Most of the algorithms are quite simple and it's a snapshot in time, unless it's

07:57.920 --> 08:03.400
being cached, there's some caveats there, but, um, the existing path part in

08:03.400 --> 08:07.000
retrospect, I actually regret making that the default because it leads to a lot of

08:07.000 --> 08:12.760
confusion, but the intent was, uh, so existing path, there's a behavior in

08:12.760 --> 08:15.080
mergerFS called path preservation.

08:15.080 --> 08:23.360
So imagine you have four drives and you want, uh, media of some sort on one, say

08:23.360 --> 08:28.640
movies, uh, TV shows on the second drive, and you have a lot of eBooks and you want

08:28.640 --> 08:34.400
them on the other two drives and, uh, the path preservation would allow you to sort

08:34.400 --> 08:41.480
of manually go in, create the paths, uh, in each of those drives that you wish to

08:41.480 --> 08:46.680
have that particular data on. So you might make a movies directory on the first one

08:46.680 --> 08:50.880
and the TV directory on the second one and, and books on the third and fourth.

08:51.360 --> 08:58.280
And so the path preservation will filter out in effect, the drives when it's making

08:58.280 --> 09:01.720
decision based on the path, the relative path of that file.

09:02.040 --> 09:07.000
So it's kind of, basically it does a, uh, a, uh, der name.

09:07.000 --> 09:12.640
If you were thinking of it from like a command shell, uh, perspective and looks

09:12.640 --> 09:15.000
for that path across all the devices.

09:15.080 --> 09:22.120
And if it finds it, it is included in the kind of secondary filter, which is most

09:22.120 --> 09:22.920
free space.

09:23.280 --> 09:27.960
Um, and there's other filters too that happen at that time, but so it's kind of

09:27.960 --> 09:30.920
creating sub pools based on the directory hierarchy.

09:31.640 --> 09:35.000
Could you see this as almost like a hybrid approach in a way that it's

09:35.000 --> 09:38.840
kind of, you know, the defined folders that you've preset on those specific

09:38.840 --> 09:44.200
drives are sort of, um, chosen ahead of time are sort of the legacy way of

09:44.200 --> 09:46.320
storing data as we typically see it.

09:46.640 --> 09:50.920
Uh, and then everything else that you pull on those drives is just sort of, um,

09:50.960 --> 09:52.840
dealt with through the algorithms.

09:52.960 --> 09:54.880
Would that be a way of seeing it?

09:55.240 --> 09:55.560
Yeah.

09:55.600 --> 10:00.760
I mean, most people, you know, grow out their collection or of devices, their

10:00.760 --> 10:03.160
storage devices, pretty organically.

10:03.160 --> 10:05.040
Unless you're a business or whatever.

10:05.400 --> 10:11.200
And, uh, you tend to kind of just put a, this drive's going to have X and this

10:11.200 --> 10:15.080
drive's going to have Y and that drive's going to have said, and then you have to

10:15.160 --> 10:20.080
remember which drive is where, which one contains which data sounds familiar.

10:20.640 --> 10:21.000
Right.

10:21.040 --> 10:25.400
This data manages that, or this behavior, I should say, manages that for you.

10:25.480 --> 10:27.800
Um, it does require a little overhead.

10:27.800 --> 10:35.040
It does require you to either seed those paths, um, or to, at some point, you know,

10:35.040 --> 10:38.360
in the future to manage them, if a drive becomes more full or not.

10:38.840 --> 10:43.080
Um, and, and that's why I was saying that there tends to be confusion because as

10:43.080 --> 10:50.160
default, if you follow through step-by-step of, if you have a pool of, of a number of

10:50.160 --> 10:53.840
drives and they're all empty, then the first time you create a directory, it's

10:53.840 --> 10:57.440
going to end up on one of those drives with the defaults.

10:57.760 --> 11:02.320
And so a lot of people get caught up and they're like, Oh, I just copied over a huge

11:02.320 --> 11:05.440
amount of data to my pool and it's all going to one drive.

11:06.000 --> 11:12.320
And that's because the very first maker happens on one drive and they all had the

11:12.320 --> 11:14.240
same amount of space free effectively.

11:14.960 --> 11:20.160
And then that one, that one path is then used for, you know, all the sub directories

11:20.160 --> 11:24.840
are associated with that path.

11:24.840 --> 11:29.720
So it's something I can't really change now, uh, because it's the default, but, uh, if

11:29.720 --> 11:32.240
there's a merger FS three, I might change that.

11:32.440 --> 11:37.240
If you were to change it, what would be your preferred default now?

11:37.320 --> 11:39.160
Just, uh, most free space.

11:39.160 --> 11:43.640
I think the, the average person using merger FS is that's what they're expecting.

11:44.320 --> 11:49.800
Uh, some people would prefer a percentage wise or least free space or whatnot, but

11:49.800 --> 11:54.480
I think, um, most free space is kind of the, the common expectation.

11:55.280 --> 11:56.400
I think that's wise.

11:56.440 --> 12:02.920
Uh, I'm, I'm a merger FS user myself, and I was, uh, a little taken aback when I

12:02.920 --> 12:07.640
first discovered that it was not using least free space or, uh, most free space,

12:07.640 --> 12:12.920
excuse me, and ended up, you know, switching to that and then balancing the

12:12.920 --> 12:16.200
drives, uh, which leads me to another question.

12:16.200 --> 12:20.560
Is there a particular reason that you decided to keep the merger FS tools

12:20.560 --> 12:24.000
separate instead of including them in the base package?

12:24.200 --> 12:28.360
Mostly the release cadence, um, and the language.

12:28.400 --> 12:34.560
So merger FS is C plus plus and the tools they're written in Python three.

12:34.640 --> 12:38.000
And the reason there is just, they're doing very simple things.

12:38.000 --> 12:39.960
It's, it's kind of easier to get off the ground.

12:39.960 --> 12:44.920
And I wanted the tools to be more easily manipulated by third parties,

12:44.920 --> 12:50.600
cause it really started as examples of how to accomplish certain things out

12:50.600 --> 12:53.560
of band so that they didn't have to be in merger FS.

12:54.160 --> 12:59.640
The concern I have is that, you know, I'm the file system or merger

12:59.640 --> 13:03.360
FS is your file system, or at least a proxy for your, your data.

13:03.840 --> 13:08.440
And if I ever screw something up in merger FS, I can screw up your data.

13:08.720 --> 13:13.360
Uh, worst case, I could corrupt it silently, uh, best case, you know,

13:13.360 --> 13:19.640
maybe it crashes or it, uh, you know, deletes a file, hopefully that's.

13:20.320 --> 13:24.000
You know, in a, in an obvious way or whatever, but there's a lot of subtlety

13:24.000 --> 13:30.120
there and granted merger FS is way, way, way more simplistic than most file

13:30.120 --> 13:35.240
systems, but fuse, uh, historically has not always been the most stable.

13:35.240 --> 13:39.280
So I was kind of bounded by the stability of lib fuse and the kernel.

13:39.280 --> 13:44.200
And for any of those who've followed a fuse or merger FS over the years,

13:44.200 --> 13:48.080
they've seen that there's a number of issues that are outside of my control.

13:48.080 --> 13:54.040
And, uh, so with that in mind, I've tried very hard to limit what I put

13:54.040 --> 13:59.320
into the main product so that if there are any issues, these things can be

13:59.320 --> 14:04.960
managed out of band, um, or, or, or at least like even if merger FS isn't there.

14:04.960 --> 14:08.760
Cause one of the nice things, one of the appealing aspects of merger

14:08.760 --> 14:14.480
FS that a lot of users, uh, say is why they like it is, is that you can always

14:14.480 --> 14:16.520
take it out and you can always remove it.

14:16.520 --> 14:20.320
And there's no merger FS isn't really a file system.

14:20.560 --> 14:25.080
I mean, technically it is, but in terms of the traditional sense where it's

14:25.080 --> 14:29.360
controlling the blocks on the device, it's not doing that.

14:29.400 --> 14:34.120
It's just a proxy and as a result, you can always remove it and all your data

14:34.120 --> 14:36.920
is going to be totally accessible.

14:37.240 --> 14:38.800
Um, and that has other side effects.

14:38.800 --> 14:43.120
Meaning if you have a drive that dies, you only lose that one drive, your stuff,

14:43.120 --> 14:46.160
full access in real time to all your other data.

14:46.720 --> 14:53.680
Um, but making everything the separate repo, separate apps, uh, it allow it,

14:53.680 --> 14:58.680
it's, it's in that vein so that, uh, if you want a balance tool and I didn't

14:58.680 --> 15:01.040
write the balance tool the way you want, it's Python.

15:01.040 --> 15:04.040
There's a lot of people that know Python, uh, go at it.

15:04.320 --> 15:06.640
So you touched on the balance tool just then.

15:06.720 --> 15:09.960
Um, which of the other tools do you think that people need to know

15:09.960 --> 15:11.280
about that they perhaps don't?

15:11.760 --> 15:14.080
The dupe tool can be useful.

15:14.440 --> 15:24.160
Uh, so it, uh, it allows you to point at a directory or a file and give like

15:24.160 --> 15:29.160
a number of copies that you want and it will find the drives with the most

15:29.160 --> 15:34.360
free space and make, uh, uses rsync under the covers because I'd rather, rather

15:34.360 --> 15:39.800
than trying to create all that very solid logic of copying files, uh, safely.

15:40.080 --> 15:45.360
I just leverage rsync, but, um, Python is orchestrating that and it will go and.

15:46.600 --> 15:52.440
Uh, find a drive, make end copies, and, uh, it'll just kind of recursively go

15:52.440 --> 15:58.200
through the path that you have instructed it to that's quite useful if you're

15:58.200 --> 16:05.520
if, uh, I know the drive pool has the ability to replicate across, uh, a number

16:05.520 --> 16:11.040
of drives mergerFS doesn't, and in some ways that's purposeful, but, uh, this kind

16:11.040 --> 16:17.240
of allows you on a schedule to do something similar, uh, there's the FSCK

16:17.240 --> 16:21.880
tool, which I don't think I've updated in quite some time, but, uh, a lot of issues

16:21.920 --> 16:26.760
with mergerFS come down to permissions and that tool can help suss out some

16:26.760 --> 16:28.120
of those permission issues.

16:28.480 --> 16:32.320
For instance, if you have two relative paths on two different drives that have

16:32.320 --> 16:38.040
different, um, permissions or, uh, owners, then it can help you find that.

16:38.400 --> 16:40.040
I ran into that issue the other day, actually.

16:40.480 --> 16:42.000
So I now know how to fix it.

16:42.000 --> 16:42.440
Thank you.

16:43.080 --> 16:43.440
Yeah.

16:43.480 --> 16:49.000
Uh, one of my coworkers, uh, has for long has, uh, for a long time, uh,

16:49.040 --> 16:53.800
complain that I allow there to be inconsistencies on the underlying drives,

16:53.800 --> 16:58.360
but the, the problem is with it being a file system, you really don't have a good

16:58.360 --> 17:03.320
API or mechanism for reporting that you found these inconsistencies.

17:04.000 --> 17:10.360
And while I could in effect, just when I notice it, report it in some log

17:10.360 --> 17:16.880
somewhere, uh, why add that to the main product and complicate the code when

17:16.880 --> 17:20.960
it's just as simple as right, uh, to, to write something in Python that just

17:20.960 --> 17:22.200
doesn't audit once in a while.

17:22.200 --> 17:26.800
Uh, usually the only time you need that is when you're mucking around underneath,

17:27.040 --> 17:30.600
uh, merger FS or you're adding in preexisting drives.

17:30.960 --> 17:34.800
So it tends not to be something you need that often, but I think it

17:34.800 --> 17:36.160
was the full first tool I wrote.

17:36.480 --> 17:39.000
I guess the other question there is, could the file system just be doing

17:39.000 --> 17:42.560
that since it has, you know, maybe that's a job it should be doing as well.

17:42.960 --> 17:45.640
So merger FS really needs to be running as root.

17:45.640 --> 17:52.080
And certainly I could go and change permissions, um, or change, uh, owners

17:52.080 --> 17:56.840
or whatnot and try to make them consistent, but what's the heuristic, right?

17:56.840 --> 18:01.760
So if you have four drives and three of them say it's owned by nobody and, and,

18:01.800 --> 18:05.920
uh, the fourth one says it's owned by root, w what do you choose?

18:06.480 --> 18:10.640
Do you choose the one based on the policy for stat?

18:10.640 --> 18:14.520
Do you do it based on a different policy for a different function?

18:14.960 --> 18:20.680
It's not really clear and to try to make it configurable adds like a whole

18:20.680 --> 18:23.880
another level of configuration to the product that would almost

18:23.880 --> 18:25.520
never be used by most people.

18:26.120 --> 18:30.840
I wanted to touch on the fact that you say that it's safe to run as root.

18:31.280 --> 18:35.880
And I was just wondering, um, if we could, if we could talk about

18:35.880 --> 18:39.480
the security of it a little bit, I'd like to know a little bit more about

18:39.480 --> 18:42.400
what safeguards you have in place for that.

18:42.960 --> 18:43.400
Sure.

18:43.440 --> 18:47.760
I mean, there's a section in the docs, which I try to recommend everyone

18:47.760 --> 18:49.080
look at the merger FS docs.

18:49.120 --> 18:55.320
Uh, I have a long career in software development and documentation tends

18:55.320 --> 18:57.080
to be very crappy for most products.

18:57.080 --> 18:59.480
And I spend a lot of time working on docs.

18:59.480 --> 19:02.000
So when in doubt, first check the documents.

19:02.240 --> 19:06.200
From a, from a user to the developer, I'd just like to say a huge thank you.

19:06.200 --> 19:10.280
It makes such a difference when someone puts this much effort into the docs.

19:10.280 --> 19:12.800
So yes, please, if you're listening and haven't read them and are

19:12.800 --> 19:14.320
interested, go take a look.

19:14.400 --> 19:18.840
And these are some of the best written docs I've seen in any

19:18.840 --> 19:20.560
project in a long, long time.

19:20.560 --> 19:23.320
They are really fantastic and robust.

19:23.560 --> 19:24.960
So yes, thank you.

19:25.000 --> 19:25.440
Thank you.

19:25.520 --> 19:29.240
Uh, that said, um, I am biased.

19:29.280 --> 19:33.920
It is my software and I've been using it for a long time and I've, I'm very

19:33.920 --> 19:35.880
intimate with systems level development.

19:36.120 --> 19:41.480
So if there's anything that is unclear for some, uh, Linux newbie, which that

19:41.480 --> 19:46.720
tends to be the, uh, the, the issue, um, feel free to contact me.

19:46.800 --> 19:49.720
There's all the co all my contacts are also in the documents.

19:49.720 --> 19:53.800
So I'm happy to try to reword anything to make it more clear, but, uh,

19:53.840 --> 19:55.160
back to the original question.

19:56.040 --> 20:01.960
So there is a section in there where I go over this relative to M8, uh, MHC

20:01.960 --> 20:05.320
DFS, so MHC DFS could be run as root.

20:05.480 --> 20:08.080
And in fact, generally expected you to run it as root.

20:08.480 --> 20:14.320
If you were, if, if you were only ever running stuff as one user ID, then you

20:14.320 --> 20:18.000
probably wouldn't ever notice if you didn't run it as root, but in the code,

20:18.000 --> 20:23.960
there are very explicitly, um, assumptions that it has the ability to

20:24.200 --> 20:26.600
shown or chmod any random file.

20:26.920 --> 20:30.440
And the way it worked was let's say it creates a file and it's

20:30.440 --> 20:32.360
was, let's say it creates a file.

20:33.240 --> 20:37.640
It would create the file as root and then change the owner to whatever

20:37.640 --> 20:42.080
it was supposed to be based because the way fuse works, you get an instruction

20:42.080 --> 20:48.480
or command from the kernel and a part of the payload is to say, uh, this is

20:48.480 --> 20:52.320
the PID it came from, and this is the user ID and the group ID and some other

20:52.320 --> 20:52.920
metadata.

20:53.280 --> 20:58.040
And so, you know, who has made this request on the other side from the client

20:58.040 --> 21:05.080
app and, uh, MHDFS would simply shown or chmod the file after creating it as root.

21:05.280 --> 21:08.760
Now, from a security perspective, that's not actually a bad idea.

21:09.160 --> 21:15.360
However, that leads to non-posix standard behavior, because if you're root,

21:15.960 --> 21:23.240
uh, you cannot, uh, well in traditional file system APIs, you couldn't, you

21:23.240 --> 21:30.440
couldn't really replicate the permission checking that would happen at, in the

21:30.440 --> 21:32.760
same way that would happen by the current via the kernel.

21:33.200 --> 21:38.560
So your root, you can, uh, ignoring capabilities and such, you can basically

21:38.560 --> 21:43.360
open any file and in certain circumstances, it would allow you to, so

21:43.360 --> 21:46.680
you would be able to do things through MHDFS running as root that you

21:46.680 --> 21:47.840
shouldn't be able to do.

21:47.840 --> 21:54.240
Uh, the proper behavior, uh, is to change before you're do any of these, uh,

21:54.280 --> 21:59.600
commands to change to that user, to that effective user or effectively change

21:59.600 --> 22:04.280
that user user, I should say, and, uh, that's what merger FS does.

22:04.320 --> 22:10.200
So anytime a request comes in where the user ID is relevant, because not all, um,

22:10.240 --> 22:12.840
there's a overhead to changing user IDs.

22:12.840 --> 22:17.640
And so I don't do it if it's unnecessary for like a read and a write, um, the

22:17.640 --> 22:20.080
kernel doesn't actually check the permissions.

22:20.120 --> 22:27.640
So if you chmod or chown a file while you're reading and writing it, um, it's

22:27.720 --> 22:31.600
that's irrelevant to, to, uh, the situation.

22:31.640 --> 22:35.080
And so I don't actually change in those for those commands, but anything else

22:35.080 --> 22:42.600
like a stat or create, mkdir, et cetera, you, it, the only solution that I

22:42.600 --> 22:47.960
use secure way to do that is to become that user and then to, uh, take that

22:47.960 --> 22:50.440
action just like you would normally.

22:50.920 --> 22:56.120
And so that's, that's the biggest security change, um, relative to the,

22:56.240 --> 23:01.000
uh, MHDFS and some others, similar file systems out there.

23:01.000 --> 23:04.000
It's in effect, the same thing that, uh, Samba does.

23:04.000 --> 23:05.840
And I use the same tricks that they do.

23:05.840 --> 23:12.160
So in Linux, or I should say POSIX in general, uh, says that every thread within

23:12.160 --> 23:17.920
a process needs to be the same user ID, but Linux doesn't work the same way as

23:17.920 --> 23:18.840
traditional Unix.

23:18.880 --> 23:24.880
And it uses for those who are familiar, the clone system call, which is a much

23:24.880 --> 23:28.560
more flexible version of fork basically.

23:28.560 --> 23:35.240
And every thread it can have its own, uh, user ID and its own credentials.

23:35.400 --> 23:41.480
And so I abuse that, uh, by calling directly into the kernel so that as a

23:41.480 --> 23:45.840
multi-threaded application, merger FS can simulate being multiple users at once.

23:45.840 --> 23:49.280
And this is a behavior that, uh, Samba also uses.

23:49.680 --> 23:50.920
That's super interesting.

23:51.480 --> 23:55.520
Uh, I always wondered why there was so many processes showing up in top or

23:55.520 --> 23:56.560
something, and now I know why.

23:56.560 --> 23:58.480
The number of threads are configurable now.

23:58.520 --> 24:05.040
Um, that was something that I, when I vendored the libfuse library, I, uh, made

24:05.040 --> 24:08.320
that more configurable because you do lose some throughput when you increase the

24:08.320 --> 24:12.960
number of threads, because there's more contention on the pipe going into the

24:12.960 --> 24:18.320
kernel, but, uh, generally, yeah, that's, that's why you, you by default will get

24:18.320 --> 24:22.800
one thread per core and then the, there's a primary thread.

24:23.280 --> 24:24.800
Um, let's shift gears a little bit.

24:24.800 --> 24:28.600
What, uh, what considerations do people need to make in terms of CPU?

24:29.040 --> 24:34.400
Because traditionally one of the limitations in fuse has been it's high,

24:34.680 --> 24:36.680
quote unquote, high CPU usage.

24:37.880 --> 24:40.880
What sort of CPUs are people going to need to, to sustain a

24:40.880 --> 24:42.320
gigabit transfer, for example?

24:43.000 --> 24:44.920
That's hard to say.

24:45.160 --> 24:54.440
Um, for those who don't know fuse is a serial protocol that, uh,

24:54.440 --> 24:58.920
that the kernel exposes to user space.

24:59.440 --> 25:04.080
And it allows an app, a user land application to serially communicate

25:04.120 --> 25:07.760
over, uh, a device dev fuse.

25:08.440 --> 25:15.920
And every time, uh, say your client app is, uh, LS every time it's making some

25:15.920 --> 25:21.480
system call that's, that's a file system API, it's got to go into the kernel when

25:21.480 --> 25:27.400
the syscall say an open is, is called, there's the open from the app gets

25:27.400 --> 25:32.880
translated, there's a wrapper for the C library, then that calls a system call

25:32.880 --> 25:37.400
into the kernel and with fuse, it's got to see, Oh, okay.

25:37.400 --> 25:40.160
You're talking to a fuse file system.

25:40.480 --> 25:44.640
So I have to look up who's on the under other end of this file system,

25:44.880 --> 25:48.080
transfer that through this, uh, character device.

25:48.640 --> 25:51.120
And that's kind of where this overhead comes from.

25:51.120 --> 25:58.240
So you have to go into kernel space, into user space from user space back at well,

25:58.240 --> 26:00.720
as a proxy, I'm going back into kernel.

26:00.720 --> 26:04.760
Like, so merger merger FS is going back into the kernel to ask the question like,

26:05.200 --> 26:09.080
okay, I need to start this file, whatever the original app was asking.

26:09.680 --> 26:14.360
So that's another round back into the kernel, then back into user space and then

26:14.360 --> 26:16.080
back into the kernel and then back to the original app.

26:16.080 --> 26:20.280
So it's, it, there's a lot of latency there because it's serial.

26:20.280 --> 26:24.160
It's, uh, copying this data back and forth into the kernel and that there's

26:24.160 --> 26:31.640
a lot of overhead to that and it's CPU intensive, not in the traditional way.

26:31.680 --> 26:33.840
So it's, it's more IO intensive.

26:35.160 --> 26:39.760
And because it's a user land app, you're seeing the CPU utilization.

26:40.080 --> 26:48.200
So if you're using ZFS or butter FS or similar, those can take a lot of compute

26:48.200 --> 26:52.960
as well, but because they don't represent themselves in user space in quite the

26:52.960 --> 26:59.880
same way, uh, you don't see the CPU grinding away doing block deduplication

26:59.880 --> 27:04.440
or in ZFS or, or the raid behavior or whatnot.

27:05.040 --> 27:09.440
Um, and so in some ways, I think fuse gets a bad rap because of that, because

27:09.440 --> 27:15.800
you're seeing the actual compute that it needs that said there is this extra

27:15.800 --> 27:19.400
latency introduced because there's these extra hops going in and out of the

27:19.400 --> 27:26.560
kernel and the speed at which you can do that, um, Is it doesn't seem

27:26.560 --> 27:28.320
dependent entirely on the CPU.

27:28.400 --> 27:33.960
So I have had people use raspberry PI twos and no problem whatsoever.

27:33.960 --> 27:39.080
Fully, you know, uh, getting full throughput as they would expect either

27:39.080 --> 27:44.200
from the Nick or the drives they have connected and minimal CPU utilization.

27:44.200 --> 27:48.720
And I've had people on Xeons say that their CPU utilization is through the

27:48.720 --> 27:53.720
roof and I haven't been able to figure out why that is, uh, I've had similar

27:53.720 --> 28:02.120
systems, same kernel, same OS, uh, different drives and HBAs or controllers

28:02.360 --> 28:06.080
for the drives and had different behavior between the two.

28:07.080 --> 28:09.040
And I'm still trying to figure out why that is.

28:10.080 --> 28:11.440
I don't know if it's a driver issue.

28:11.440 --> 28:13.200
I don't know if it's a drive issue.

28:13.200 --> 28:17.760
Um, if it's a, there's a bunch of other crap going on on the machine that's not

28:17.760 --> 28:20.960
being accounted for issue, uh, it's hard to say.

28:21.120 --> 28:26.080
And, uh, what I really need to do is there's all these variables and arguments

28:26.080 --> 28:31.360
that I allow to be modified in merger FS that I would like to hide from the user.

28:32.160 --> 28:35.120
But the reason that they're there is because I've found that different

28:35.120 --> 28:40.000
people get different performance, um, based on those different options.

28:40.000 --> 28:44.120
And what I really need to do is stop being lazy and build a, uh, like a

28:44.120 --> 28:47.560
benchmarking tool that just kind of goes through all the different permutations

28:47.560 --> 28:52.760
of those arguments and then runs, you know, very simple DD kind of benchmark

28:52.800 --> 28:58.000
and tries to find the best, um, the best permutation of values.

28:58.320 --> 29:01.280
You heard it here first folks, measure of first three is going to have a built

29:01.280 --> 29:02.720
in tool for testing performance.

29:03.040 --> 29:03.560
Nice.

29:04.800 --> 29:06.560
Um, so I had another question for you.

29:06.560 --> 29:10.160
Uh, how long has measure of first been around and what was your, your

29:10.160 --> 29:11.680
motivation really to create it?

29:11.720 --> 29:15.800
I know you touched on that at the beginning with the, uh, the lacking

29:15.800 --> 29:20.120
features in other products, but, uh, just curious, really sure how long it's been

29:20.120 --> 29:22.480
around, um, we'd have to check it hub.

29:22.640 --> 29:25.800
It's been longer than I, than I'd probably say.

29:25.880 --> 29:31.200
Um, cause if, for instance, this other tool I have BBF that I've been working

29:31.200 --> 29:36.000
on recently, which is a tool that is kind of bad blocks, plugs and

29:36.000 --> 29:42.520
blocks plus, uh, HD parm, um, plus some extra features, uh, that tool I thought

29:42.520 --> 29:45.160
I wrote like a year ago and it turned out I wrote it three years ago.

29:45.240 --> 29:47.600
So, um, my memory is not so good.

29:47.680 --> 29:47.880
Yeah.

29:47.880 --> 29:48.600
Time flies.

29:48.600 --> 29:49.320
It really does.

29:50.240 --> 29:57.720
But the, uh, I guess a little backstory is that I've always been a, uh, media

29:57.720 --> 29:58.280
collector.

29:58.320 --> 30:04.960
So in the eighties and nineties, I had all the CDs and all the movies.

30:04.960 --> 30:13.600
And once it became, uh, financially viable to store digital media, um, in

30:13.600 --> 30:22.000
bulk, I started doing so, and I had a iRiver H, uh, three 40 with all my

30:22.000 --> 30:29.520
AUG level six encoded or AUG Vorbis, um, level six encoded, um, music.

30:29.960 --> 30:33.920
And I had to immediately when I bought it, I bought an upgraded hard drive

30:33.920 --> 30:37.320
on the, you know, third, third party and took it apart and upgraded the hard

30:37.320 --> 30:42.400
drive to like 60 gigs, I think it was, um, just because I had a bunch of music

30:42.400 --> 30:44.200
and I wanted all my music in one spot.

30:44.200 --> 30:50.360
And, uh, as my music collection grew and hard drive prices declined, I

30:50.360 --> 30:56.440
started ripping my DVDs and, uh, I never liked the idea of transcoding it to

30:56.440 --> 31:01.040
some other format, especially because even if you're not someone who's ripping

31:01.040 --> 31:07.160
and then selling, which I don't, I just have tons, tons and tons of, uh, um,

31:07.720 --> 31:12.560
CD holders, DVD holders up in the closet, but just re ripping stuff as a pain.

31:13.040 --> 31:16.880
And so I ended up slowly building this hard drive collection.

31:17.720 --> 31:20.960
And it became, as soon as I got to a point where one hard drive couldn't

31:20.960 --> 31:24.640
hold the full collection of certain types of media, it got really annoying.

31:25.040 --> 31:29.720
And that's when I started looking for a solution, which led me to MHDFS.

31:29.720 --> 31:32.080
And cause that was so simple to do.

31:32.080 --> 31:36.320
And I, you know, I wasn't gonna spend the money for a raid set up or whatever.

31:36.480 --> 31:38.960
I think we both use MHDFS for a while.

31:39.360 --> 31:46.400
Cause before I use mergerFS, uh, I used MHDFS and AUFS and Unraid for a bit too.

31:46.400 --> 31:50.920
And I even tried FreeNAS, uh, and then your tool literally came out of nowhere.

31:50.920 --> 31:53.160
I've just found the first commit was March, 2014.

31:53.200 --> 31:53.480
Yeah.

31:53.480 --> 32:00.440
And I, I had a coworker who was also using MHDFS and, you know, it was a data hoarder.

32:01.040 --> 32:02.880
And, oh yeah, I love that subreddit.

32:02.880 --> 32:03.200
Do you?

32:04.040 --> 32:08.840
Oh, I'm on there and you'll find me, uh, anytime there's a, I have a, uh, if

32:08.840 --> 32:12.080
then, then that, uh, search for mergerFS.

32:12.080 --> 32:15.960
So people apparently know that anytime something comes up, I

32:15.960 --> 32:17.520
come in and snipe a response.

32:17.800 --> 32:18.480
You do.

32:18.560 --> 32:22.160
I've noticed that every time I post something, there you are, like a bad smell.

32:22.160 --> 32:24.680
So what does your home server look like?

32:24.720 --> 32:26.280
How many terabytes are we talking?

32:27.080 --> 32:34.560
I have, um, so, uh, I do have a Wiki page on the mergerFS GitHub, uh,

32:34.600 --> 32:40.920
account or whatever repo, and I invite people to go there and add their own home

32:40.920 --> 32:42.440
setup, um, mine's on there.

32:42.680 --> 32:48.680
So I have currently, uh, so I just redid my system.

32:48.680 --> 32:52.640
So I actually don't have that fancy of a setup, um, though I do have a lot of

32:52.640 --> 33:00.320
storage, so I used to just have a four port, uh, eSATA, AS media based multiple,

33:00.360 --> 33:03.360
um, can, uh, eSATA controller card.

33:03.760 --> 33:09.840
And then I had four IC dock port multiplier, four port, uh, enclosures.

33:09.960 --> 33:11.760
They fit well in my entertainment center.

33:12.320 --> 33:14.120
You know, I don't need a lot of throughput.

33:14.240 --> 33:14.960
It worked.

33:14.960 --> 33:18.920
Uh, but when it doesn't work, it really doesn't work because unfortunately port

33:18.920 --> 33:22.160
multipliers, uh, don't handle failing drives particularly well.

33:22.760 --> 33:28.600
And so just about a month ago, I decided I was fed up and I bought a old, uh, uh,

33:28.640 --> 33:41.000
LSI 90 201, uh, four port 16 or 16 port, um, uh, HPA, and I bought two generic, uh,

33:41.000 --> 33:45.120
eight bay, three and a half inch drive enclosures.

33:45.440 --> 33:48.520
I assume you flashed that LSI thing to it mode.

33:49.160 --> 33:50.000
Yes, yes.

33:50.200 --> 33:54.160
And, uh, these enclosures I linked to them on the Wiki page.

33:54.320 --> 33:55.680
I'd never come across them before.

33:55.680 --> 33:59.440
They're basically the, if you've ever seen an eight port SANS digital, it's

33:59.440 --> 34:03.080
that enclosure, but without anything, without a backplane or anything.

34:03.480 --> 34:08.440
Um, and so it's pretty bare bones, but at $70 shipped, it's pretty good.

34:08.440 --> 34:11.120
Nice if you're just kind of putting your own thing together and you don't have

34:11.120 --> 34:15.000
rack space, which I don't, I'm in a Manhattan apartment, uh, downtown.

34:15.000 --> 34:24.920
So I bought two of those and I bought four, um, For, uh, mini SAS to four port

34:24.960 --> 34:31.120
SEDA connector cables and just fish the wires through and hooked it all up.

34:31.120 --> 34:32.760
And, uh, that's what I'm on now.

34:32.760 --> 34:33.160
It works.

34:33.240 --> 34:34.360
It's way more stable.

34:34.360 --> 34:39.560
So I have, uh, let's see, I have 12, eight terabyte drives.

34:40.280 --> 34:45.920
I have a few thrown in there that were like dying drives, like a one, a two, one

34:45.920 --> 34:51.000
and a half, something like that, that I'm using as guinea pigs for my, uh, hard

34:51.000 --> 34:51.680
drive tool.

34:52.240 --> 34:57.800
And then I have a, uh, the audio isn't bad.

34:57.840 --> 35:00.760
Um, or, you know, how loud the drives are, aren't that bad.

35:00.760 --> 35:01.800
They're all sitting in the car.

35:01.800 --> 35:04.520
You know, how loud the drives are, aren't that bad, they're all sitting in my

35:04.520 --> 35:05.040
living room.

35:05.720 --> 35:07.560
Um, and I I'm in Manhattan.

35:07.560 --> 35:10.560
There's a lot of outside noise, so kind of all drones together.

35:10.560 --> 35:16.760
It's not the biggest of deals, but in terms of heat and the noise, especially

35:16.760 --> 35:20.360
from the fans gets annoying at times, um, I have updated the fans, so they're,

35:20.440 --> 35:25.160
they're less awful, but in any case, I've kind of started moving to two and a half

35:25.160 --> 35:26.480
inch, five terabyte drives.

35:26.480 --> 35:31.640
The, um, Seagate, oh, I forget what the serial numbers are, but

35:31.640 --> 35:34.600
in any case, they're the only five terabyte, two and a half drives out there.

35:34.880 --> 35:38.320
They're reasonably priced per terabyte and I've started using those.

35:38.320 --> 35:40.480
So I have, uh, three of those now.

35:40.920 --> 35:44.600
And so my total capacity is around a hundred terabytes.

35:45.000 --> 35:47.240
And unfortunately it's almost full.

35:47.520 --> 35:48.360
So that's really impressive.

35:48.360 --> 35:49.840
Do you have any kind of parity on there?

35:50.120 --> 35:51.000
Uh, I don't.

35:51.240 --> 35:58.320
So I used to use snap raid, but I kind of, I was using crash plan.

35:58.320 --> 36:05.440
So if you look in the trap exit, get hub, um, account, you'll find a repo.

36:05.440 --> 36:09.560
That's just, uh, examples on how to set up different environments, different

36:09.560 --> 36:13.360
storage environments and how to use different tools for maintenance and

36:13.360 --> 36:14.720
formatting and et cetera.

36:15.200 --> 36:19.200
And you'll notice that there's numerous mentions to crash plan, which I still

36:19.200 --> 36:22.960
technically have a crash plan plan, but I haven't been using it much lately.

36:22.960 --> 36:28.120
Uh, and for those who don't know, they were kind of the last, maybe

36:28.120 --> 36:35.720
kind of still are besides, um, well, I guess the only cross-platform fully,

36:35.800 --> 36:42.280
um, unlimited backup service, like true backup service, not a cloud, um, kind

36:42.280 --> 36:44.840
of data source like G drive or Dropbox.

36:45.360 --> 36:48.440
And so a lot of, I know a lot of data hoarders, including myself use that for

36:48.440 --> 36:52.280
years, but the company I don't think is doing that great or whatever.

36:52.280 --> 36:56.640
So that was kind of my backup plan because it, once I successfully backed

36:56.640 --> 36:59.720
up all my data, that was fine, right?

36:59.720 --> 37:04.440
Cause I have, I also have another tool called scorch, which is used to find bit

37:04.440 --> 37:04.800
rot.

37:05.000 --> 37:09.760
So if I ever lost a file or drive, if I ever found bit rot, I could just

37:09.760 --> 37:11.320
restore it from crash plan.

37:11.320 --> 37:14.960
And because it was true backup in the same way that maybe time machine or

37:15.200 --> 37:20.200
snapshots and in ZFS or butter FSR, then it really wasn't a big deal.

37:20.200 --> 37:25.800
I really didn't need, um, parody, um, or any sort of, uh, backup like snap rate.

37:26.600 --> 37:31.560
Uh, when that started giving me a problem, mostly because they store all their data

37:31.560 --> 37:33.440
when they're scanning your drive in memory.

37:33.440 --> 37:36.480
And so you need a ton of Ram to, uh, scan.

37:36.480 --> 37:39.680
And the more data you have, the more Ram you need and it just doesn't scale well.

37:40.400 --> 37:47.960
Uh, I did move to, uh, to snap rate for a while, but I realized that if I had a

37:47.960 --> 37:52.040
different backup service, then I could get rid of that again and get that storage

37:52.040 --> 37:54.800
back, which at the time I really needed similar to now, I guess.

37:55.400 --> 37:59.840
And so I kind of got off of snap rate at that point when I found a better backup

37:59.840 --> 38:04.880
service, which is I use G-suite now and our clone and that works quite well for me.

38:05.200 --> 38:05.760
Very well.

38:05.800 --> 38:06.080
Yes.

38:06.080 --> 38:07.160
I use that myself as well.

38:07.560 --> 38:13.320
Um, so you got, you mentioned snapshots just then, um, is there any technical

38:13.320 --> 38:17.840
reason why snapshots couldn't ever make it into merger FS one day?

38:17.840 --> 38:20.000
Is that something you might consider adding at some point?

38:20.560 --> 38:25.760
It would have to be based on top of existing technologies.

38:25.800 --> 38:30.880
Like the underlying file system would have to support it in some way, uh, because

38:30.880 --> 38:33.600
there's just too much going on there for me to replicate.

38:34.160 --> 38:39.840
I do have a number of optimizations that I really don't talk about because you

38:39.840 --> 38:44.880
wouldn't notice, but if you've ever used the CP, the GNU CP tool, right?

38:44.880 --> 38:47.160
Like CP, you're copying a file around.

38:47.560 --> 38:50.480
If you look in the man page, there's the ref link argument.

38:50.920 --> 38:56.320
And so certain file systems like ZFS, butter FS, and new versions of XFS have

38:56.320 --> 38:57.240
this ref link thing.

38:57.240 --> 39:02.240
So this copy on right, the ability to say that this file and this file use the same

39:02.240 --> 39:02.920
blocks.

39:03.000 --> 39:08.640
And so if you're going to copy a 10 terabyte file, it can happen in, in a, you

39:08.640 --> 39:13.960
know, in a second because it's really just making references to preexisting data,

39:13.960 --> 39:15.880
which is how snapshots work.

39:16.360 --> 39:22.720
And so in merger FS, when it is needing to copy a file from one location to another,

39:22.720 --> 39:27.920
and there's a few situations where that may occur, uh, it'll try to use those

39:27.960 --> 39:30.720
optimized, um, behaviors.

39:30.720 --> 39:35.200
And if they fail, it falls back to just traditional, you know, read from one file,

39:35.280 --> 39:36.840
copy and write to the other file.

39:36.840 --> 39:40.440
And so I could in effect extend on that.

39:40.560 --> 39:46.600
And if certain commands were made to, uh, merger FS, I could forward them

39:46.600 --> 39:51.440
appropriately to the underlying file system, but because merger FS is really

39:51.440 --> 39:56.560
just a proxy and I'm not messing with the data itself, there isn't a real way for

39:56.560 --> 40:01.040
me without in effect rewriting the whole product to offer snapshots.

40:01.160 --> 40:02.360
Yeah, that makes a lot of sense.

40:02.360 --> 40:08.720
You mentioned very briefly earlier how, um, if merger FS is removed from the

40:08.720 --> 40:12.400
system, uh, then the drives are still, the data is still available on the drives.

40:12.440 --> 40:16.280
Um, that is one thing that has kept me from exploring various options.

40:16.280 --> 40:20.720
I'm, I'm looking to build my own sort of storage system, uh, in my place.

40:20.720 --> 40:24.440
And I've been sort of looking at a bunch of tools, but haven't jumped in.

40:24.760 --> 40:28.240
Although Alex has convinced me that merger FS is probably the way to go.

40:28.280 --> 40:28.800
It is.

40:28.840 --> 40:30.480
Um, and so you can stop looking.

40:30.480 --> 40:36.600
One big question that I have is, and which I have applied to all of my systems, uh,

40:36.640 --> 40:41.760
in various ways is that I want to be able to understand the data if something goes

40:41.760 --> 40:42.080
wrong.

40:42.080 --> 40:46.080
So can you give me sort of a run through of what the data might look like if

40:46.080 --> 40:46.800
something goes wrong?

40:47.200 --> 40:49.720
Exactly as you would expect if it was a single drive.

40:50.040 --> 40:54.240
Well, I guess, uh, to dive into that a little bit, like I know the, let's say I

40:54.240 --> 41:01.200
write a file, it gets sent to the, um, free most drive.

41:01.440 --> 41:06.720
And so if I'm looking at several drives, um, that have a similar folder, for

41:06.720 --> 41:08.280
instance, uh, right.

41:08.320 --> 41:12.880
So, uh, I mean, in, in that case, what you're going to have is a federation of

41:12.880 --> 41:16.880
that data across those drives, um, but at a file level, right?

41:16.880 --> 41:23.160
So you're not going to be, let's say you've got an album and, uh, you've got

41:23.160 --> 41:28.640
most free space enabled, um, that album, you know, say of three drives and

41:28.640 --> 41:33.560
there's 10, 10 songs and they all happen to be around the same size, you might

41:33.560 --> 41:37.600
end up with three files, three files and four files, uh, across those three

41:37.600 --> 41:41.560
drives, uh, that's really not predictable at any one time.

41:41.560 --> 41:45.360
I mean, obviously at the specific time of creation, it's predictable.

41:45.360 --> 41:46.800
That's how merger FS is working.

41:46.800 --> 41:52.960
Um, though there is a random option, uh, policy, but, uh, you're not going to

41:52.960 --> 41:56.160
know ahead of time, obviously where that data exists.

41:56.200 --> 42:00.560
However, it will be in the same relative path.

42:00.720 --> 42:05.800
You'll be able to very easily see that this specific data, I mean, you'll know

42:05.800 --> 42:09.560
that that data is missing, but you're not going to be missing like half a file.

42:09.920 --> 42:15.360
You'll be missing maybe half an album, but, uh, you'll know every, every drive

42:15.360 --> 42:18.520
will have the same, largely the same layout.

42:18.840 --> 42:22.680
They'll all have the same directories or at least, uh, the directories that

42:22.680 --> 42:25.080
matter to be on multiple drives.

42:25.120 --> 42:30.400
You'll see this, the same relative layout, but so defined like a specific

42:30.400 --> 42:37.000
file, we would then use find or locate or whatever search tool against the drive

42:37.000 --> 42:40.440
Mount paths rather than the merger FS pool, correct?

42:40.520 --> 42:40.960
Right.

42:40.960 --> 42:46.160
And, uh, and my tool scorch, uh, to plug that a little bit, I actually use

42:46.160 --> 42:48.400
that, uh, for this kind of use case.

42:48.720 --> 42:52.960
I need to enhance the tool to make it so right now it hashes everything.

42:53.040 --> 42:57.080
I want to make the hashing algorithm configurable, and that includes no hash.

42:57.400 --> 43:01.240
So that would make the tool more of an indexing tool, but when you're merging

43:01.240 --> 43:05.920
together all these different drives and you lose a random drive, it's incredibly.

43:06.360 --> 43:08.520
Frustrating to not know what data you're missing.

43:08.520 --> 43:12.320
Like it's convenient that I still have my other drives live.

43:12.480 --> 43:15.400
Um, and I'm missing only this one drive, but then you have to go through

43:15.400 --> 43:17.160
the process of fixing that situation.

43:17.640 --> 43:23.040
And I started writing scorch in part because of that, where I wanted

43:23.040 --> 43:27.360
to know very simply, I wanted to have a database in effect that I can say, okay,

43:27.480 --> 43:33.120
show me every file in this system that I'm missing, uh, since my last, uh, you

43:33.120 --> 43:34.720
know, update of the database.

43:34.720 --> 43:41.560
And if you pointed that to the mounts rather than say the union, then you could

43:41.560 --> 43:44.440
find exactly what files you are missing.

43:44.680 --> 43:50.320
Now I'm going to probably release these random scripts that I have, um, where it

43:50.320 --> 43:54.280
kind of creates an audit of every drive that's in my pool every day and then

43:54.280 --> 43:59.360
creates a diff and so, and I use like a log rotate to keep those for 90 days or

43:59.360 --> 43:59.720
something.

43:59.720 --> 44:04.320
So if I miss a drive or accidentally delete something or, um, you know,

44:04.320 --> 44:10.160
whatever, I can just kind of unzip the file, check, do a quick grep and find

44:10.160 --> 44:13.920
where the file was at any particular time in the past 90 days.

44:14.480 --> 44:20.440
Um, and then that combined with scorch, which gives, I use to index all the data

44:20.440 --> 44:23.600
in my main union, uh, mount point.

44:24.120 --> 44:28.960
I can very easily find the data that I need to help me reconstruct the situation.

44:29.160 --> 44:30.760
I need to start using this scorch tool.

44:30.760 --> 44:33.600
It sounds wonderful, but just to expand a little bit on what's just been

44:33.600 --> 44:38.440
said, um, the, the key thing that merger FS lets you do, and to be honest, it's

44:38.440 --> 44:42.280
the same with, um, unraid and many of these other kinds of union file systems

44:42.800 --> 44:46.600
is you still have a, an individually readable file system on each disc and how

44:46.600 --> 44:52.920
that differs from traditional raid or, uh, ZFS or something is that rather than

44:52.920 --> 44:56.040
the data being striped, uh, across multiple drives.

44:56.040 --> 45:00.320
So if you lose disc five, you better hope that the stuff that was on disc five is

45:00.320 --> 45:05.280
striped somewhere on disc one through four, whereas with merger FS, like, um,

45:05.600 --> 45:10.840
Antonio said, it writes entire files to a disc rather than partial files or

45:10.840 --> 45:11.480
anything like that.

45:12.120 --> 45:17.360
Um, and so if you pull any of these drives from underneath merger FS and stick it in

45:17.360 --> 45:22.080
another Linux system, you'll be able to read that ext four or that XFS or that

45:22.080 --> 45:26.520
ZFS drive, but you know, that, that for me is the real win of these kinds of

45:26.520 --> 45:30.120
systems rather than striping dates or it's individually readable on each drive.

45:30.120 --> 45:30.720
So, yeah.

45:30.720 --> 45:35.080
And there are certainly, I am in, and I try to make this very clear in the FAQ

45:35.080 --> 45:39.840
section for merger FS, uh, that this is not a substitute for certain

45:39.840 --> 45:41.400
workflows and workloads.

45:41.720 --> 45:47.080
Um, you know, raid has its place, uh, granted with very large hard drives,

45:47.080 --> 45:54.040
raid is becoming precarious, but that aside, um, merger FS is really meant for

45:54.040 --> 45:59.960
the casual user who has numerous random drives and they just want to have some

46:00.040 --> 46:08.480
kind of single view into their data or prosumer who knows the risk or is using

46:08.480 --> 46:13.840
some other tool like snap raid or has full backups or whatever, and is willing

46:13.840 --> 46:18.480
to take these trans, uh, these trade-offs between, okay, I can lose one drive and

46:18.480 --> 46:20.640
the rest of my system will continue to work just fine.

46:20.640 --> 46:22.640
And I don't have to worry about, you know, okay.

46:22.640 --> 46:24.840
If two drives go, oh, that sucks.

46:24.840 --> 46:28.200
I'm going to have to restore a bunch of data, or maybe I've used a path

46:28.200 --> 46:29.960
preservation policy.

46:29.960 --> 46:33.480
So I put all the data that I really don't care about on these two drives.

46:33.480 --> 46:35.320
So if they die, you know, so what?

46:35.320 --> 46:40.280
And I back up the other number of five drives, whereas on raid, you may, if you

46:40.280 --> 46:42.760
lose two drives, you know, all your data is gone.

46:43.240 --> 46:46.360
So it's, it's really just about offering another tool.

46:46.520 --> 46:52.440
And I try to be very clear about where merger FS falls flat and where it is,

46:52.440 --> 46:55.000
uh, more beneficial than some of these other technologies.

46:55.520 --> 47:02.440
And because it's so much more simplistic to use, um, you know, certain people try

47:02.440 --> 47:06.280
to make raid systems like, you know, uh, free NAS and un-raid, they try to make

47:06.280 --> 47:08.160
things extremely simple.

47:08.320 --> 47:10.080
I don't think it's safe for un-raid.

47:10.080 --> 47:12.440
I don't think it's the cost that is a barrier to people.

47:12.720 --> 47:17.560
I think it's, you know, it's closed source or it's just scarier, you know,

47:17.560 --> 47:19.160
there's just a lot more knobs to tweak.

47:19.200 --> 47:22.320
I mean, granted, there's a lot of knobs to tweak and merger FS, but out of

47:22.320 --> 47:23.800
the box, it just kind of works.

47:24.320 --> 47:25.920
And, um, yeah.

47:25.960 --> 47:30.560
So most people, I think understand that trade-off and, uh, it, for a lot of us,

47:30.560 --> 47:31.080
it's worth it.

47:31.600 --> 47:31.880
Yeah.

47:31.880 --> 47:35.840
But un-raid, uh, has a GUI and merger FS doesn't, so it must be better.

47:36.320 --> 47:37.720
Uh, you know, I'm trying to fix that.

47:37.800 --> 47:43.040
Uh, the problem is I'm, I'm very much a low level developer and web development

47:43.040 --> 47:50.320
makes me want to rip my eyes out and, uh, uh, no offense to the web people, but.

47:50.320 --> 47:52.480
The web people, the web people.

47:52.520 --> 47:52.960
I love it.

47:52.960 --> 47:53.680
The web people.

47:53.840 --> 48:01.800
Um, anyway, uh, so I, uh, I've looked into doing it myself and, uh, that's

48:01.800 --> 48:03.080
probably just not going to happen.

48:03.080 --> 48:07.440
I ideally in the same way that I try and make merger FS as simple as possible.

48:07.680 --> 48:11.960
I know the guys over at, uh, open media vault, they have, you know, their front

48:11.960 --> 48:16.160
end and they have a merger FS exclusively as their union file system

48:16.160 --> 48:18.280
option, which thank you for them.

48:18.280 --> 48:23.480
Uh, but you know, it, their UI is very simplistic and obviously you have to have

48:23.480 --> 48:26.320
the whole package and not everyone wants to use open media vault.

48:27.240 --> 48:32.000
Um, what I would like to be able to do is have a simple binary or.

48:32.560 --> 48:35.760
You know, script that someone can download that it's its own web server

48:36.080 --> 48:41.720
that you run as root, um, or give proper permissions to access the merger FS,

48:41.800 --> 48:46.440
um, control runtime API that would allow you to kind of just start up a web

48:46.440 --> 48:51.760
browser, go to, um, whatever machines IP address that you've launched it on and

48:51.760 --> 48:56.280
just kind of drag and drop drives that you have, you know, it'll query my, the

48:56.280 --> 49:00.000
way I imagine is with query all your drives, look at the ones that are probably

49:00.000 --> 49:04.440
your boot drive, separate them out from your data drives, and then give you click

49:04.440 --> 49:08.120
boxes or dropdowns for all the different options and for merger FS.

49:08.120 --> 49:12.720
And then maybe I think it would be nice after if I could get that done to add

49:12.720 --> 49:17.160
maybe some snap raid stuff since that's a very common tool to use with merger FS

49:17.160 --> 49:18.960
and anything else in that space.

49:19.360 --> 49:25.320
But, um, I do have a number of web, uh, developer friends, but none of

49:25.320 --> 49:27.160
who are in this space.

49:27.520 --> 49:33.720
So I've been meaning to go on like data hoarder and, and home lab or any of those

49:33.720 --> 49:38.600
subreddits and just throw out a, um, requests to see if anyone's interested.

49:39.240 --> 49:41.040
I wanted to say thank you very much for joining us.

49:41.040 --> 49:44.920
It's been a pleasure speaking with you and, uh, from the community as well.

49:44.920 --> 49:47.920
I know that many, many people are very grateful for all the work you've

49:47.920 --> 49:50.160
done on this, on this amazing tool.

49:50.720 --> 49:54.320
Um, do you have any way you'd like to send people to follow you like Twitter

49:54.320 --> 49:55.840
maybe, or plug your GitHub?

49:56.080 --> 49:59.640
Yeah, just, uh, you can find me.

49:59.640 --> 50:03.400
I mean, trap exit is, uh, so that username comes from Erlang.

50:03.400 --> 50:09.760
I was a big Erlang or am a big Erlang fan and, uh, kind of, uh, use that as my handle.

50:09.760 --> 50:14.200
It's you'll almost always find any, any service.

50:14.200 --> 50:16.040
If you look up trap exit, it's probably me.

50:16.040 --> 50:18.480
There's a few that aren't, but that's pretty rare.

50:18.720 --> 50:22.240
If you just go to github.com slash trap exit, you'll find all my stuff.

50:22.280 --> 50:25.000
And, uh, I work on all these tools regularly.

50:25.000 --> 50:29.440
You can find me on Reddit or email, whatever, like contact me

50:29.440 --> 50:30.760
any way you will, you wish.

50:30.800 --> 50:34.040
I have a discord as well for those who want to chat about stuff.

50:34.520 --> 50:36.960
Um, yeah, I'm very open.

50:37.240 --> 50:38.400
Feel free to contact me.

50:38.400 --> 50:40.080
Contact me any which way.

50:40.600 --> 50:41.400
Well, thank you very much.

50:41.400 --> 51:08.480
Thank you for having me.

