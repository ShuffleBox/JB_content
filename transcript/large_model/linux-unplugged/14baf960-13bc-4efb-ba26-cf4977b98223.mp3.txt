An announcement from Microsoft's Office 365 team today, Wes, for Linux.
Microsoft Teams, their Slack competitor is officially here.
And I'm gonna go so far as to say, early prediction, so
I'm claiming this one, it's already on my list.
What?
This is a beachhead for Office on Linux.
You think so, the rest is gonna follow suit.
I do.
It's been interesting, there was an open support issue since Teams launched for
Linux and for a long time it didn't look like it was gonna happen,
there just wasn't interest.
Now it's finally here.
Although, you'll have to go like three directory levels deep to find it.
If you go to teams.microsoft.com slash download,
they do have a more friendly version.
But if you get where Wes got linked to,
he's like browsing through their web directory.
That was from one of their pages.
I know, I know, I know.
Nobody else in the world except for
existing Office 365 users was looking for this.
But if you want something besides Slack,
there's some really great open source alternatives.
However, a lot of Office 365 users.
Right, and Teams gets bundled in there, so of course you probably have to use it.
And now there's another Electron app you can use on your Linux desktop.
Yes, man.
Boy, wasn't that on my wish list.
So while you were spelunking in web directories,
I was just installing it from the AUR because, God bless the AUR,
it just showed up immediately.
Nice, all right, I'm not going to use Teams though.
Come on, come on, I'm not using Teams.
If anything, I'm going to Mattermost next.
Yeah, but the people who use Teams don't get to choose.
Hello, friends, and welcome into the Unplugged program.
My name is Chris.
My name is Wes.
Hello, Wes.
I've got a smile today because this is one of those episodes where
Wes and Chris are going to do something you shouldn't try at home.
Don't follow along.
Don't do what we do this week.
We're busting a stereotype.
Fedora worked so well.
It blew my mind how great it was as a server OS,
and all the stereotypes say don't run Fedora as a server.
So we thought, well, let's take that a little bit further
and let's replace our highly critical, perfectly functional Fedora server
with an Arch box and report back what the process is like to migrate
a live in production server from Fedora to Arch, lessons we learned,
and then truly what we think of it as a server platform,
and some of the things we're doing to safeguard the fact
that it is a rolling distribution.
I mean, what could go wrong?
We're doing this so you don't have to.
You're welcome.
I'm really not kidding.
This box is critically important to us.
But we really wanted to test the stereotype.
After having really good success with Fedora,
we thought we've got to replace this Fedora 30
install either with Fedora 31 or CentOS.
So let's go to Arch.
We've come a long way from FreakNAS.
Yeah.
Yeah, we really have.
So we'll tell you about that in a little bit.
But before we go there, we do have some community news
to get into.
Like, some of this is down the road,
maybe going to happen prediction stuff.
Like, we don't really know.
So let's start with some of those far out predictions.
NVIDIA looks to have some sort of open source driver announcement
just around the corner.
Michael Arboles hot to the trot with this one over at Ferronics.
He says, start looking forward to March,
when NVIDIA looks to have some sort of open source driver
initiative to announce, which is likely contributing more
to Nivu.
He was tipped off by a Ferronics reader
about a GTC session that's happening at GTC 2020
by NVIDIA.
Now, the engineer from NVIDIA, John Hubbard,
is running a talk titled Open Source Linux Kernel and NVIDIA.
GTC for the unaware is the GPU Technology Conference.
And boy, does that talk have an interesting abstract.
Here it is.
We'll report up to the minute developments
on NVIDIA's status and activities,
and possibly, depending on last minute developments,
a few future plans and directions regarding
our contributions to the Linux Kernel, supporting Nivu,
including signed firmware behavior, documentation,
and patches, and NVIDIA Kernel drivers.
Whoa.
OK.
There's a lot in there.
Signed firmware behavior is huge.
Documentation would be ginormous.
But patches to the actual upstream projects
is mind-blowing.
That is mind-blowing.
Did something change inside NVIDIA?
Perhaps they've noticed that AMD is kicking some ass these days.
So you're going to have to wait till towards the end of March
because GTC 2020 runs from the 23rd to the 26th of March
in San Jose.
You going to go, Wes?
I know you're a big gamer.
I don't think I'll make it, but I will
be equally awaiting this news.
Hopefully, Michael Larbo will make it down there,
and he'll give us a report back.
All right, and then one other kind of far out
in the future community news, but one
we are so dang excited about.
Your co-host and buddy Jim Salter over at Ars Technica
writes that the WireGuard VPN is one step closer
to mainstream adoption.
This is all coming from the Linux Network Stack maintainer
David Miller, who committed the WireGuard VPN project
to the Linux Kernel's NetNext source tree.
He maintains both the Net and the NetNext source trees,
which govern the current implementations of the Linux
Kernel's networking stack and, of course, the future one.
Yeah, NetNext gets pulled into the new Linux Kernel
during its two-week merge window, where it becomes Net.
With WireGuard already a part of NetNext,
this means that, barring unexpected issues,
and there's always time for those,
there should be a Linux Kernel 5.6 release
candidate with built-in WireGuard in early 2020.
Oh, just excuse me, Wes, just going
to do a little happy dance.
Something tells me we're going to be
trying those release candidates when the time comes.
Now, do take a little bit of salt with this one.
I mean, yeah, the Kernel community
does what they want.
The maintainers have their own priorities and schedules,
and we'll see what happens.
We've heard this song before, but we did just recently
see the required encryption bits land.
We covered that recently.
So this is sort of the next required piece.
Now, there's kind of an unfortunate possibility
on the timing here, because if I'm doing my time math right,
and Jim points this out in the article,
this is probably going to land after Ubuntu 20.04,
the next big LTS.
Yeah, that's unfortunate.
I imagine there'll be long timelines
to see it get in other sort of LTS releases like RHEL.
But WireGuard founder and main developer Jason Doenfeld
offered to do a bunch of the work backporting WireGuard
to earlier Ubuntu Kernels directly.
That's great.
That is.
We've had very brief exchanges with Jason in the past,
and he seems very passionate about this.
Oh, yeah, obviously cares a lot, and has
put in a ton of work promoting WireGuard to get it
where it is.
I hope they take him up on that.
He also teased that a WireGuard 1.0 is on the horizon.
Well, what?
I know.
Wow.
How can it get better than it is?
That's a good thing to say, too.
If you could have that land right before 5.6,
then you're including 1.0.
We've got here a nice, stable, maintained VPN.
Mr. Brent, you just recently had adventures in WireGuard land.
Yeah, it's been on my docket for about six months.
Wes, I've been asking you for like, hey, Wes,
when you're ready, if I go ask you, would you help me?
And it turns out you didn't even need it.
Well, it turns out I am more capable of things
than I think I am.
So this morning, I thought, I got a pretty wide open docket,
and WireGuard seems like the best thing to do in the morning.
So dove right in, and it was super smooth.
What are you using it for?
I mean, are you replacing an existing VPN?
Yeah, my idea was I'm currently using private internet access
just to give me, as some people may have picked up,
I travel quite a bit.
So it gives me some extra protection, basically,
everywhere.
And so that's been playing fairly nice with my phones
and computers, but they just got sold recently, I believe.
And if the rumors are true.
Yeah, they are.
And also, the apps kind of, they're buggy,
and they're not native.
I don't like them.
So I thought, OK, WireGuard really seems to make sense.
You guys have been telling me it's super stable.
So I jumped in, had to learn a few things,
but there's some great tutorials.
And yeah, just spun up a VPS on our trusty DigitalOcean
and got it configured pretty easily, actually.
And it's working pretty good.
How proud are you of Brent right now?
Oh, yeah.
Amazing.
Super proud of you, Brent.
Good for you.
Oh, thanks.
And did you find any particular documentation useful,
or any kind of tips or tricks you could pass along
to people that are also kind of starting from zero?
Yeah, I think my setup is wonderfully simple.
All I'm trying to do is have a kind of a dumb server
out there that's just waiting for me to connect to it.
So it's not doing much fancy stuff,
but maybe that's a good place to start.
Absolutely.
I will share in the IRC here the tutorial that I used.
And it just kind of worked.
It was super simple, very straightforward, great,
written in a nice way that allows
you to learn along the way, which is the whole idea.
Excellent.
Will you grab that for the show notes, Mr. Payne?
Sure will.
And it's great that it's already pretty easy.
And once it's in the kernel, it'll
be even easier than that.
Yeah, that's the thing is part of that setup
is just getting it installed right now and figuring out
how you're going to do that.
We just went through the installation process last night
ourselves, which we'll talk about more in a little bit.
Really hope that they do take Jason up on that offer
to backport it to the LTS, because like you kind of
implied there, Wes, there's also the question
of the RHEL release cycle.
RHEL is currently using, the current version of RHEL,
is using the 4.18 kernel, which is already nine months old.
And they tend to stick with that for quite a while.
Yes.
Even that said, it's pretty straightforward to get going,
even on a system that doesn't have it baked in.
Right, it's not a big module to load, and it's very easy
to build.
And it's one of the better behaved DKMS modules
I've ever used.
So this next story kind of made me smile this morning,
because it felt like the good old days of using a Linux
distro.
I knew that Manjaro, which I've been running on a couple
of my workstations, had an update coming.
They've been kind of teasing it on Twitter.
But I didn't know when it was going to land.
And so as I always like to do, because I
think it's the way to keep an art system running great,
is I decided I'd check my packages this morning.
Just do a synchronize my mirrors.
This is show day where you need your workstation to work,
so you better update it.
You're right.
I really am a dummy, aren't I?
I really am dumb.
I'm so dumb.
I think you're just excited.
You know, you have this great little workstation going,
and you're like, oh, there's probably new packages.
You know what it is, is I just love the way
it all looks on the cool retro CRT terminal.
And Peckman's so pretty.
Yeah.
So anyways, I'm just synchronizing my mirrors,
and I notice I'm getting like 30k a second from the mirror.
It's just going really slow.
And I'm like, oh, oh, must be a big release day,
because that's how this works.
And sure enough, I check on Twitter,
and a brand new stable release of Manjaro is out.
525 packages needed updating on my system.
Oh, boy.
It includes the new 5.4 LTS kernel and a new version of,
remember how I was trying to tell you Manjaro has this package
manager that's like unique to them?
PAMAC, yeah.
New version of that, but as well as updated desktops.
And just loving, loving it so far.
I wasn't sure how the first update experience would be like,
but this was great.
It was really kind of fun.
Oh, my mirrors are running slow, and my first thought
was, let's go check Twitter and say, hey, look, a new release.
I mean, it's a little bit of difference
there if you're used to Arch, because it's not quite rolling.
Right, right, right.
Yeah, it's just sort of fun to have moments where
there's a bunch of things, but it's not a complete huge thing.
It's a mini new Fedora release.
It's mini, yeah, all the time.
It's a good way to put it.
It's a little mini update, and I really
think they did a pretty good job of this one.
Haven't actually rebooted yet, though, so I'll reserve my time.
Oh, come on, that's the real test.
I know.
Got to kick the tires on the new kernel.
Well, now that we have the show at the newer time,
I was like, I got to run down there and do the show.
I didn't have a half hour.
So you should have rebooted it, and then
would check at the end of the show.
Yeah, I could run up there.
Should I try it?
Should I try running up there during one of our clips
or something and see?
I don't know.
I hope it works, because I'm really falling
in love with that workstation.
Well, actually, we've got a tip later on in the show
that maybe you should set up on that workstation,
and then you wouldn't be so worried.
You're right.
I totally could set it up on that workstation.
Oh, Wes Payne, you are clever.
We will definitely be talking about that.
But before we get into all of that,
I want to mention, speaking of how much I love my dang plasma
desktop, we had ourselves the Making Plasma Brilliant
live stream on Friday.
And I'm pretty happy to say I think it went decent.
We had a good attendance.
The Mumble Room was popping.
The video after the fact has already been posted
and got a lot of views, relatively to what I expected.
I think the sign it went well is there was just more stuff
than we could possibly talk about.
And we weren't hunting for content,
because there's so much to do in plasma.
Nailed it right at the hour mark.
Still got the live stream.
And there's like six or seven topics
we just couldn't go into.
I got that internal clock, pow, right there.
So it's up now.
If you are interested in what I do to beautify and make
my plasma desktop, like I say in the video,
from basic to brilliant, that link
will be in the show notes at linuxunplug.com slash 331.
And it's up on the YouTube channel at youtube.com slash
Jupiter broadcasting.
And you could just jump in if you want.
At the very beginning, I kind of tell you what to expect.
If you just want to know how to tweak your fonts or tweak
console, you can just jump to that.
Also, Brent just keeps hitting it out of the park.
Brunch with Brent and Alan Pope.
Mr. Popey sits down with Brent for a fantastic brunch.
Is there anything you want to tease?
I haven't had a chance to listen yet,
because it just came out this morning.
Oh, there's a lot of stuff in there.
Both of us, at the end of the conversation, went, jeez,
I went in a bunch of directions we never expected.
So.
That's usually the sign of a great brunch with Brent.
I didn't know this about Popey, but he's
a very well-practiced fuzzy tester.
And he tells a little bit about that, his adventures there.
So that's a pretty good one.
You get some good Popey flavor.
I can tell just by looking at the links.
There's some good Popey flavor that comes through on this one.
And you know, we were just sitting around here
and said, you know, if there's just one thing we need
is Popey on more podcasts.
So he's on the Ubuntu podcast.
He's on the UserAir podcast.
We need him on more podcasts.
So Brunch with Brent, extras.show slash 38.
Maybe someday he'll come back to love.
Yeah.
Oh, that's right.
What the heck?
I forgot him.
I remember.
Remember that, Wes?
Wes remembers.
Wes remembers.
Oh, yeah, but it was really great.
So if you've been missing yourself some Popey.
Also, there was recently one with Wimpy, too.
I see.
I see how it is.
Now they got time for brunch.
Sorry, guys.
I'm taking over at least all of your appearance.
No, actually, I've known these guys for a long time.
And I know that just by listening to Wimpy's,
I knew I learned stuff about Wimpy.
So I know I learned stuff about Popey, too.
So it's really great.
Also, I am so happy to say that our Telegram channel has really
leveled up recently.
Lots of great conversations going all the time.
And thanks to the work of Cheese Bacon and others,
we've got some good spam prevention in there now.
So it's a really nice, good, clean chat at jupiterbroadcasting.com
slash telegram.
Yeah, there's really a fun conversation going on there
almost all of the time.
Pretty much all the time, because there's
folks from all different time zones in there.
But you also see the host popping in there
throughout the day, as well as network announcements.
So if you felt like you want to take the conversation beyond just
the download, jupiterbroadcasting.com
slash telegram.
Even sometimes that Wes Payne's in there.
Oh, yeah.
Happens.
All right, Wes.
So here we were with a perfectly functional Fedora 30 workstation
server, which is even funny to say.
Actually, no, we might have used the net install image.
So it might have been the server at the end of the day.
All right, fine.
Fair enough.
Because there wasn't a GUI installed.
But it was still Fedora.
I think we had a workstation USB drive going, though,
to troubleshoot.
Did you think I was crazy when we picked Fedora coming
from FreeNAS?
So the background there is we were running FreeNAS.
We had a whole ZFS array that Alan Jude set up for us.
But we found FreeNAS to be limiting.
So we went to Fedora.
I mean, I'd say limiting is just that it didn't work for our use case.
We needed less of appliance because we could both.
We wanted to manage the server a little more interactively.
And we just weren't that familiar with FreeNAS or FreeBSD.
Yeah, we also wanted to take advantage
of being able to run things from the command
lines for testing for the show and setting up and spinning up
things that are a little more cutting edge,
that maybe there wouldn't be a pre-cut something for FreeNAS.
I mean, we're Linux nerds.
That's what we do.
And then, honestly, we just wanted a Linux system.
I would have gone Ubuntu LTS, probably, myself.
But it didn't seem crazy.
One of the first servers I ever set up
when I was starting to make the transition from playing
with Linux on my little laptop to like, oh, OK,
I'm going to try this on the server.
It was like Fedora, I don't know, it was like 14, 15,
somewhere in that era.
And it was a fantastic server.
So I knew it could work.
For a limited time, though, that was usually the concern.
Yeah, I mean, I think I had that thing for a year or two at most.
And I wasn't current on updates.
And it's a lot of updates.
It is a lot of updates.
And that is something we had to deal with,
which Arch will be the same way, is there
was sometimes updates that would then break things
like our ZFS support, momentarily.
We are running a lot of out-of-tree modules
relative to most servers.
WireGuard, ZFS, to just name a couple of really critical ones.
And that was a bit of a struggle,
although not insurmountable.
But it did cause probably one outage in total.
Well, you know, one something.
And I think we sort of found that while we liked Fedora,
there was a lot going on that we appreciated.
It was almost too complicated, too ready for the enterprise,
if I'm allowed to use that phrase,
because there were just a lot of systems in place for good reason
that you would want and that were well-configured,
but that we just didn't need in our tiny server use
case here in the studio.
But end result was we were very, very happy with Fedora.
And we're very much considering, because of that,
going with CentOS 8 or CentOS 8 Stream
and then just loading ZFS support into that.
However, we got talking about this
from kind of like a philosophical standpoint.
And we realized this is something
that we have an opportunity to kind of try and maybe bust
a myth here on the show.
We are the Linux Unplugged Mythbusters
because we have a theory.
And that theory goes that if you were
to build a minimum viable Linux server,
or another way to put it is just enough Linux,
so the system boots and launches containered applications
and really does almost nothing else.
Right.
I mean, it might make me think of some
of those container-specific distributions.
And we're not going that far, because we still kind of want
all of our usual tools.
Yeah.
We go a little bit further, because we
install things like NetData and Samba on the host system.
But I think our base install is still well under a gig.
It's a very minimal Linux install, very few things
are running.
And we wanted something that was everything was off by default.
And what we turn on incrementally
is all that's running with all the other functionality
provided by applications and containers that
are divorced from the host operating system.
That was our theory.
And we thought, well, in these conditions,
if we could come up with a belt and suspenders approach
to running Arch, it would probably
be a viable server platform.
I mean, we are both familiar with Arch,
have run it many times.
And I think we nailed it.
So I'll tell you what our belt and suspender was as we go here,
because I think you're going to like this.
But I first want to start, I want to set the scene.
I didn't know exactly how Wes planned to pull this off,
because here we are.
We have a Fedora 30 box.
It's in production.
We've set an evening aside with the team.
Hey, this thing's going to be offline for a couple of hours.
We weren't clear on that one.
More like six hours, but we'll get to that.
But I didn't exactly know how Wes
was going to accomplish this.
So I was delighted to learn exactly how
we were going to install Arch on top of this existing Fedora
instance.
Wes Payne has decided the best way
to load Arch Linux on our server is
to boot with an Ubuntu thumb drive.
Yeah, that's right.
I mean, Ubuntu is just a reliable operating system.
Why not use it to install Arch?
You get a GUI and everything.
And I'd just like to point out that the Arch install scripts
are packaged in the Ubuntu repository,
so it couldn't be easier.
Oh, I didn't realize that.
That's really cool.
OK.
So we've got the thumb drive.
We've done a lot of the preliminary.
So go ahead and fire it up.
We've done some backups.
We've exported the ZFS pool.
We shut down the Docker containers.
We're going to do one more export just
to make sure everything's nice and clean of the ZFS pool.
But job one now will be to boot off of this Ubuntu thumb
drive and create an Arch Chirrut environment.
So these Arch install scripts, they're
in the Ubuntu universe repo.
So you've got to turn on the universe repo.
But what is this?
Is this some sort of backdoor way
to get Arch on an Ubuntu system?
Yeah, oh, yeah.
Well, I mean, really, it's just the minimal tools
that the Arch team has written to help
aid you in the install, things like Arch Chirrut or Pack
Strap, just a few utilities that can get things up and running,
or like GenFSTab, the handy tool to make you
an FSTab entry.
Yes, that was really nice.
And most of them, I mean, they just need the core Unix tools,
all the stuff you would get in core utils.
So that was fun, because I was really surprised to see that.
And it was great, since it supported all of our hardware,
including the ZFS disk.
And we, of course, had to struggle a little bit,
because it's a server.
It's a super micro system, so it doesn't
have a very fancy graphics card.
But we managed to get that figured out by just going
into safe graphics mode.
And then it was off to the races and time
to destroy some data.
We're up and running in Ubuntu safe graphics mode.
And Wes was pretty clever in choosing 1910,
because it supports ZFS automatically.
So that was really nice.
And we've already made the drastic step
of wiping out the partitions.
And Wes is currently in the process of creating new ones.
We're doing a really simple layout.
The host drive will be ButterFS.
The storage array is ZFS.
So we are doing a Butter ZFS hybrid arch install.
We'll explain more about that.
There we go.
That's our three simple partitions.
Have you written it to the disk yet?
No, not yet.
Are we ready?
Let's do it.
There's no turning back now.
OK.
I didn't give you much notice.
There was no time for you to say wait.
You were just like, well, you were just ready to go.
I had my finger hovering over the keyboard.
You know, it's cold out there.
Because this is Pacific Northwest.
I didn't have the most dexterity.
No, and it's a cold garage.
I mean data center.
So I thought this would be kind of fun
to just talk about on the show for a moment.
How about a hybrid arch file server
that's ButterFS on the OS disk and ZFS on the data disk?
Radical.
The further we got into this setup, the more I love it.
And I think this is how I'm doing my workstation setups
from now on too.
And it really kind of comes down to how you set up the sub
volumes for that belt and suspenders approach
I was talking about.
Alex, right?
OK, so we've done a simple disk layout.
But inside that simple disk layout,
we've created a series of ButterFS sub volumes.
Can you give us a quick rundown?
Well, we'd like the ability to take snapshots considering
we're installing arch here.
Down the road, some packages may go wrong.
And therefore, we've got a root sub volume.
And we're also going to have some data stored
under the home partition.
Maybe some of our Docker setup or other configuration.
And we'd like that at probably a different cadence.
We might integrate snapshots of the root file system
with the package manager, or perhaps on a daily cadence
and want something different for home.
So we've got those separate.
And then we've actually got a totally separate boot partition
that we can take snapshots of as well.
I'm really looking forward to experimenting
with integrating snapshots into Pac-Man.
So before and after Pac-Man actions,
we do a pre and post snapshot.
We'll talk more about that in a little bit.
So we just got done setting up those sub volumes that
will enable that flexibility once the system's up
and running.
Now we've got to get them mounted and chroot inside.
Now, we should probably mention right here,
so that way we avoid confusion.
Later we decided it would probably
be better to actually have slash boot as part of root.
Right, once we invested a little more in some of the tooling,
we'll talk about we realized it just made more sense
to do it that way.
But it's a flexible setup, and it wasn't much work to change.
Yeah, and this is great because, and we'll
have a link to the wrappers that let you do this,
but this is great because it lets
you have an automated system that
will take a complete snapshot before a package action
and after, much like Sousa does, but for Pac-Man.
That's right, we didn't have to switch distributions.
And there is a way to extend that into Grub
so it also creates completely bootable snapshot
environments.
So we can just, from Grub, choose a previous environment
and boot completely into it and revert all of the changes that
happened on the system.
And that's really what we want here, right?
I mean, if an update goes wrong, we're
trying to stay on top of our updates for security
and features.
We want the ability to easily roll back
if we don't have time to deal with any issues right now.
Yeah, and to kind of get that boot environment
thing working and the Pac-Man integration,
that's where it was sort of necessary,
just because of some of the assumptions the tools make,
that's where it was sort of necessary to have boot
on the root file system.
And it kind of makes sense, too, to have
snapshots sort of integrated so we have a full snapshot of,
basically, everything we need for the system,
especially the way, by default, how Arch does kernels
as compared to, say, Ubuntu with a whole bunch of versions
laying around.
Yeah, and that's why Home is its own subvolume,
so we can snapshot that independently.
So that's kind of nice.
And then all of the data and the container data and all of that
is living on ZFS.
So you could completely just unplug the OS drive
and plug in a new OS and then just re-import the ZFS pool,
which is kind of essentially what we did.
But we hadn't actually truited and booted into it yet.
So once we got all the subvolumes created,
Wes used our setup scripts and set up an environment.
And we went through and configured FSTab
and generated that.
Arch has also got a handy little bootstrap tarball image
you can download that has basically everything you need.
It really was great.
It was really nice to go through this process again
and really just understand how clean the setup is
on the server.
I've never installed Arch with someone else before,
so that was kind of fun, too.
It was fun.
I haven't either.
It actually was a lot of fun.
We installed it together.
And our favorite part, I think, was doing FDIS,
because that's old school and setting all those up
and deciding what the volumes are going to be
and how to lay that out.
That's fun to do with somebody else.
But then there's that moment where
you got a reboot from the host Ubuntu system,
and you have to boot into your handcrafted Arch environment.
Did we get it right?
Did we get it right?
Will it actually boot?
We've done everything on the checklist, so far as I can tell.
But I mean, can you ever really know until you push the button?
No, we just got to find out.
So here we go.
This will be a nice, lean, mean, just enough Linux installation
if all goes as planned.
But I don't know.
Something about this first boot, it's
always like the most special.
I made some sous vide pork shoulder this weekend, you know?
I thought you were going to say something about this.
You did, huh?
That's funny.
We just picked up some pork shoulder,
and we are planning to sous vide it.
How'd it go?
Amazing.
Amazing.
Sous vide tips with Wes Payne.
All right.
Selecting the built-in boot disk.
Welcome to grub.
We got grub.
Monitor power save mode at the worst possible moment.
Arch Linux grub option comes up.
Hit it, Wes.
Boom.
Loading Linux Linux.
Ha, ha, ha, ha, ha, ha, ha, ha, ha, ha, ha, ha, ha, ha, ha, ha.
Those little, large details.
Yeah.
Aka the default configuration for all software.
Yeah.
OK.
System D is starting up.
Version 2.44.
This is getting pretty far.
I'm feeling pretty good about our potential.
Oh, yeah, our ZFS array is lighting up.
Look at that.
And we're at the boot, just like that.
We're done.
Any thoughts, guesses?
Do you think ZFS is going to load?
I do.
Because those disks lit up, I think it's going to work.
Go ahead and hit it.
Let's see.
A little mod probe ZFS.
No problem.
There it goes.
So that'll just scan it.
Now we've seen it, and we can actually import it fully
online.
I think the next step is to set up SSH on this thing,
and we get ourselves out of this cold garage.
I mean, server room.
And we set this up from the comfort of the studio.
That's a great idea.
I think in our rush to get out of the cold garage,
we may have made a slight mistake.
We did not unplug the thumb drive
from the back of the server.
No.
You know, Ubuntu, in fairness, which I always complain about,
but it hopefully prompts you that you should do that.
Yeah.
We didn't listen.
No, we just want to get the heck out of there.
And then we also, during our setup,
because we were really just enjoying the simplicity of things,
we made a decision for simplicity that ended up
costing us several hours, maybe two and a half hours,
if I'm being generous, of time.
Once we went back in the living room,
we were troubleshooting this issue
where we would fire up all of the containers,
and everything would start.
They would know nothing in the logs was weird,
but none of them could communicate with the network.
But at first, it wasn't even none.
You know, there were a couple that wouldn't work.
Oh, yeah, right.
So it wasn't like it was a total failure.
It wasn't obvious, because a couple did work.
The containers were up, the ports were forwarded,
but we couldn't get to all the surfaces.
No.
And it was simply because of a decision
we had made hours earlier.
But in the order of process, we hadn't
realized we were making a critical decision that
would affect us later on.
Well, a couple of hours now in the living room of the studio,
working from the couch, much more comfortable
than the garage.
But we ran into a couple of snags
that we didn't quite expect, one which
we'll tell you about in just a moment.
But the one that we should probably warn you about,
first of all, is some of the troubles
we ran into with systemd networkd and how it interacts
with Docker, which it's documented.
We kind of probably could have, should have known about it,
but just because of the kind of the way
we built this thing up from the ground up,
didn't really hit it until it was already an issue.
Yeah, we were attracted to systemd networkd
because it's so simple.
You know, you make one file with like three lines,
we have DHCP, there's one interface that we care about.
It was easy.
But then you have to start jumping through hoops
to get it to ignore Docker.
It tries to basically manage any interface it can
get its hands on by default. We've just disabled it
and are moving back to DHCP CD.
Yeah, that'll work for now.
And it means that our applications and the containers
are spinning up successfully.
And they're pretty much none the wiser.
They have no idea that we just transitioned from Fedora
to Arch and just turned them back on again.
And the ZFS stuff seems to be working pretty well.
Oh, except for that one thing.
Yeah.
So we had this kind of moment where
we thought we gone done mess with stuff
and we broke a couple of hard drives.
We were expecting this to be easy, right?
I mean, that was the whole point of the way
we'd set the system up is just lift and shift.
Right.
But a couple of decisions we had made for simplicity
had actually screwed us.
Number one, the fact that we use DHCP for our server.
However, we, and I have done this for, oh, wow, ever.
I mean, a very long time.
I set my servers via DHCP, but I do it with a reserved address.
So everything's mapped to the Mac address.
Right.
We could just statically assign it and it would work just fine.
But I've always preferred to do it that way.
So that way, even remotely, I can change the IPs of systems.
It centralizes your admin right there.
Yeah.
And we chose to use systemd networkd
instead of just going with a straightforward DHCP client.
And I mean, I'll take the fall for this one
because I proposed it.
I really like systemd networkd, at least in the other use
cases I've used it for.
I mean, the Linux-based router I have at my house,
that's all powered basically by systemd networkd,
but it also uses Nspon.
So I've used systemd network with Nspon and Podman
Now, when I think about it, not a ton with Docker.
So I really hadn't encountered this issue before.
It shows up as a new interface to systemd networkd.
And it's like, oh, well, let me take care of that for you.
And we have a lot of Docker interfaces.
And one of those Docker containers
is also managing WireGuard, which
needs a whole bunch of more complications involved.
So there are definitely ways you can tell systemd networkd,
hey, don't manage this link.
Make sure that forwarding works as we expect.
But for our case, because we weren't
using any of the features really of systemd networkd
besides its DHCP support, DHCP CD works very well.
And then there's that disk issue that I was implying there
at the end of that clip.
When we loaded up, it said, hey, the three disks are offline.
And the system isn't that old, but it
did get really hot in there over the summer
despite our best efforts.
And we looked at each other and we
thought maybe powering this thing on and off again
a few times popped some disks.
It wasn't nearly that bad, though, was it?
No.
I mean, they're probably still old disks.
And we should consider upgrading anyway.
Yeah.
But the pool's fine.
The pool is fine.
It is fine.
All the disks are online.
All 13 drives are actually operating.
But I think in part was it because it thought, um.
Well, so we were just going through this.
It's not like we had a major plan
or a giant sort of sophisticated write-up.
Who us?
Yeah.
I mean, we may have done this as a sort of.
We're the kind of guys that put Arch on the server, Wes.
I think that's enough said.
Right.
And so normally when you're setting up ZFS,
I'm normally sort of mapped out ahead,
like how I'm going to shape the pool.
But because we were just sort of importing a pool that already
existed.
Yeah, we inherited that.
Right.
And so we just popped over to the ZFS page on the Arch wiki
and entered the first command that you saw.
And it's like, oh, yeah, import the pool.
That looks great.
We did not remember.
And I do usually like to do this is have ZFS use the disk IDs
and not whatever SDX it happens to be assigned.
And because we still had that USB drive plugged in,
all of our drive names were shifted and messed up.
Well, not all of them.
The few that happened to be assigned after that device.
So that's why it couldn't find the disks.
Linux could see them, but they had new names to ZFS.
Yeah.
I was convinced.
I was like, oh, man, we killed them.
Like just turning them on and off.
Because one of the things we thought we'd do
is we'll do a full shutdown.
We'll test this thing.
We'll do a full power off and power on.
And that's when we thought, let's then check the disk.
And that's when we saw that, well, three of them
are not responding.
But thankfully, it was all resolved.
Right.
I mean, you just had to export and then re-import,
telling it, like, hey, label these this way.
ZFS is smart.
And on the upside, it, of course, goes and re-checks,
scrubs the data to make sure, like, oh,
does this match up with what I think
I should have for this disk?
And add it back into the pool.
It did find some checksum issues on the drive.
So we do have some homework.
Right.
Yeah, it does turn out that perhaps it's a good thing
we're using ZFS and that bit rot protection,
because we do have some disks that are kicking airs.
Oops.
I mean, that's why you use something like ZFS
for your important data, right?
So now I'm feeling good about that decision.
So going back to those belt and suspenders,
we did decide to use Snapper from OpenSUSE, which
is integrated in both as a snapshot that does a snapshot
every single week, but also with a Pac-Man wrapper.
And this is our kind of insurance.
Even though it's a very, very simple base arch install
that we'll just update once a week,
this kind of integration with snapshots with ButterFS,
which is baked into the Linux kernel, unlike ZFS,
is our insurance policy.
Right.
So we should be clear, right?
The ZFS pool is the existing pool
we've inherited all the way from when the system originally
ran FreeNAS.
And that's great.
It keeps our data safe, as we were just talking about.
ButterFS we're just using on the one disk
that we have that's actually sitting outside of the server
case that we're running the OS on.
And that's kind of played in this role, right?
Basically, all of our data is on the pool,
and we can mix and match OSes as we see fit.
Yeah.
The one thing we need to change is
we need to get the Docker compose files off of the OS
disk.
Not a big deal.
We have a backup of it.
You know, I think we might just want to move the home partition
over to the pool.
Could.
Totally could.
Actually, that's a really interesting idea.
That would be a good way to do that.
I like having the separation of OS and data,
though, because now we have, I think, really proven it
three times over.
We have gone, initially, this pool initially
started inside of a FreeNAS mini.
Right.
An actual enclosure with four disks.
And then Alan came along, and we said,
let's turn this thing up to 11.
And we got a super micro enclosure.
Great recommendation.
I mean, it's been a great server.
Super solid.
Really been nice.
And took those disks in there, plus added all the way up
to 13 disks total, and took that to a FreeNAS
install on a super microchassis.
And then we said, hey, you know what?
Let's go crazy.
What's the most ridiculous thing we could do?
Let's put Fedora on here.
That's when we put it on Fedora.
And then we got to the end of that.
We said, well, what's even crazier
than Fedora in production?
Let's put Arch on here.
And this ZFS pool has moved every single time.
I think we learned some stuff, too, about how we wanted to use
the box along the way.
Because when we switched over to Fedora,
we talked about how we discovered that it
was more useful to us.
And I think that helped shape why pick Arch.
Because it turns out we're doing everything in containers,
besides cockpit and admin stuff.
So we don't need a lot in the OS.
Yeah.
As a FreeNAS storage box, it was a storage appliance
that we dumped files on.
When we moved to Fedora, we expanded the applications
that we could run on it.
We really kind of started enjoying using the system.
We realized, this thing's got 24 cores.
This is insane.
We can use this for encoding.
It doesn't have to just be a file server.
And it changed the way we use the hardware fundamentally.
And so this go around, we had all
of that kind of experience of, now, well, this is really,
like you're saying, this is how we use the machine now.
This is how we use it.
And so this super simple core approach, I think,
is the key to a long-term, sustainable Arch install.
So we're going to set ourselves a reminder
to check in on this box and let you know how it's doing.
Because obviously, the real, like,
I don't want to say proof in the pudding,
because you know what I'm saying.
You know what I'm saying.
We suspect that you'll be curious to see how this is
working out for us in six months.
Well, yeah, because I can say it's been great for 24 hours.
But the real proof will be in the long-term, I don't know,
like six months from now.
We'll talk about it after the show
and figure out when's a good check-in date.
Probably sooner than six months, because it
could go all wrong in 90 days, for all I know.
Right, we'll let you know.
Yeah, it's a lot of updates.
I mean, it's a lot.
I've been very pleased with the tooling we've adopted.
And that was part of the reason to choose ButterFS, right,
is that, I mean, Canonical's working on things like ZSYS.
But right now, OpenSUSE and the integration with Snapper
and Yass, that's kind of the state of the art
on the Linux side of things.
I know the VSDs have very neat solutions here, too.
We wanted that goodness.
And using ButterFS, at least on our just single no raid
or anything, right on the root drive, it's been so easy.
I think I'll probably replicate that on my laptops.
Any time we make a change, you run Pac-Man,
and you get two snapshots before and after.
I've been hard on ButterFS, Wes, but I'm not
expecting a lot here.
I'm not trying to do a RAID 6, or I'm not
trying to do anything fancy.
It's just a super simple one-disc install.
We also don't care about the data.
I mean, we care a little bit.
Wes, I was trying to make that nice thing.
Oh.
I was trying to make that Chris says
something nice about ButterFS, and you just are like, yeah.
But it's also because you don't care
about the data, which just sort of undercuts
the whole sincerity of it.
I want to talk about something that I think that
was kind of our key to success, because this did end up
going longer, but we kind of also
knew when to call it a night.
Because before we started, and credit goes to you, Wes,
you said, all right, before we start,
what's our benchmark of success?
When have we successfully converted this thing
to a production server again?
And so we took a hot couple of minutes
before we started deleting partitions,
and we said to ourselves, all right,
when it is capable of running all of the applications
the current production server runs,
when it is capable of supporting WireGuard connections
with our existing keys.
Yeah, that's pretty important.
Because we didn't want to have to force all of the team to.
We just got that working nicely.
Yeah, we just got all their keys out there, and everybody.
And also, we wanted to end with something
that this system could do that the previous system couldn't
do.
And that was where we integrated Snapper with Grub, with Pac-Man,
and on a weekly basis.
And that was something I asked you,
are we going to blow out all of our disk space
by using snapshots?
But I guess Snapper actually shipped with some pretty
sensible defaults.
Right, I mean, you can go adjust settings and tunings
to say, how often should you take snapshots,
and then how long should you let them hang around?
And there's just some system D services
that you can enable to set up the automatic prune job.
Not so bad.
And we thought, really, for something
we're not using that often from at a base OS level,
once a week is enough.
Right, I mean, we've got.
Plus every package transaction.
That's it.
That's the majority of system changes we'll be making.
And then some snapshots around to capture anything else
we might mess with.
Configuration in Etsy, for example.
Yeah, because it's, I mean, literally, it's NetData, Samba,
and Cockpit.
Those are the three applications we have installed.
But it's just Admin stuff.
I really consider SSH almost like part of the system, right?
I really do.
As far as we went out and installed something, Cockpit
and NetData.
Well, we have to install the SSH package.
Yes, OK, fine, fine, all right.
Yeah, OK, well, I'll count it.
Four.
We've installed four applications.
But really, that's it.
It's just Admin stuff we need on the box for ease of life stuff
to monitor what's going on.
It's truly a fundamentally simple system.
There's nothing really all that fancy,
other than the only kind of edge casey thing
is the ZFS, DKMS stuff.
And that's why I thought it was pretty important we
didn't use ZFS on route.
We went ButterFS, so that way, the system
would at least boot if something went sideways.
Yes, it's just simpler, right?
Worst case, I mean, we still have some outage issues,
because the applications don't come online.
But we've got our environment set up
to actually deal with that in an easy way.
We can at least log into the machine
and begin to troubleshoot and rebuild those DKMS modules
and get the system back online.
Something I'll need to find for the show notes,
and this is a little reminder to myself here,
there's a handy little script.
Because we have Docker sitting on top of ZFS,
which also made this so easy.
Docker has a ZFS driver, and it meant
that all the Docker stuff from our previous install
was already on the pool.
So once we loaded Docker back up on the system,
it just found all of its old containers
exactly like it needed.
Yeah, it was just pick up and run.
But as a result, you need your ZFS pool online
before you can have your Docker game and start.
So there is a handy little system D service
that just sort of acts as a shim
and doesn't start Docker until your ZFS pool
is actually online.
Oh, okay.
And that's one thing we did not have on our Fedora setup.
Yeah, no, that did cause problems.
That did cause problems.
So that's, I feel like, again,
lessons learned on this build.
And then taking what you and I know
about managing Arch boxes and applying it to a server,
I think we're pretty good.
I will say when I got in this morning,
I did run updates on the server, so.
That's gonna be it.
Like we talked about, do we wanna automate the updates?
And we both decided, even with snapshots,
probably not, both from like a storage use standpoint,
but also just to be careful for a while
with maybe something we'll try.
Maybe if we get a few months into this
and everything's fine, maybe we'll try it.
There is the added benefit of if we have to do updates,
we'll have to at least log into the system.
And that's sort of an incentive to check around on things.
I would like to know,
I'm sure there must be other crazy pants people
out in the audience that are running an Arch on a server.
Have you automated the updates?
Have you found a way,
like can you have it like ping a Slack channel?
What are you doing?
How are you doing that part of it?
I'd be curious if anyone's building custom Arch images
out there too, cause that would be my approach probably
is to set up a job to bake a new image
and then just have the server reboot into it.
Yeah, oh.
So how would you, where would you,
you would build that somewhere safe,
test it and then deploy it to the box
and then just say, next time you reboot,
tell grub next time you boot,
boot into this instance,
like a partition that just the image gets expanded onto
sort of using like your own home baked OS tree approach.
Exactly.
Huh, that's a lot of work to maintain one Arch box,
but I could totally see the value
if you had like a whole rack of them.
Right, then you could get the testing would matter more
and you'd really wanna be sure before you push down.
Yeah, I feel like really almost something,
just imagine, picture it West, Sicily, 1987.
The Arch Day.
Everybody knows, a Slack message comes in
that says in 24 hours, I'm auto installing these packages.
But of course by then there'd be more packages.
There are tools that'll help scrape the Arch website,
check for things.
You can probably tie those things together.
If there's not been a blog post since the last update
on the Arch page, just install them,
otherwise prompt for my approval.
Somebody must have solved this, somebody must have.
And so I thought I'd put the question out to the audience,
linuxunplug.com slash contact,
if this is something you've got an idea on how to solve.
Just tell us the weird ways you're abusing Arch,
that's what we wanna know.
It's been a lot of fun, really enjoyed it.
And I think if we had thought about the DHCP system,
D network, D thing ahead of time.
Yeah, it was just an instinct of like,
the last time I used this tool, I liked it a lot,
so why don't we use it?
I think we would have had this thing
in like two, three hours.
Once that was done, really, the dream of just being able
to move the containers over, it worked great.
Yeah, it really was pretty awesome.
I will say also, it was just,
installing Arch is just a dream,
and I love understanding all the pieces of the system.
It does constantly give you moments of like,
oh right, yep, gotta get that and that.
But that's a good reminder
of all the implicit dependencies you have
on pieces of the other operating system.
I think for you, ad user was a good example of that.
I thought that was a,
I didn't realize that was a Debian thing.
I went to go ad user,
and I'm like, what do you mean you don't have ad user?
What is this?
And you're like, it's in the AUR.
I'm not installing it, it's fine.
Also, nano?
I mean, this is 2019.
I think we're all civilized here.
Let's install nano in the base image.
I thought we agreed to just ship the S code instead.
Or at least Vim, for heaven's sake.
Please.
Watching Wes try to use a system without Vim is painful.
It's very painful.
It's just, it's ingrained in my fingers.
It's literally every single time he forgets.
He never remembers that Vim is not installed.
That's how I edit.
All right, well, Richard wrote in on some Arch update tips.
He says, you can slowly apply backpatches to Arch Linux
and not apply months worth at a single time.
If you edit a repo to a specific date, say two weeks out,
then you can run the update against that.
And he gives us an example,
which we'll have linked in the show notes.
After you set that, you save an exit
and you update the system via Pac-Man.
And it will only go back to that date range.
This is clever.
I never knew about this.
No, and it makes sense though.
You can, I mean, you'll still have breakage.
You have to progress with the updates,
but that should make it easier to deal
with sort of one thing at a time.
So you could go back and if you're six months back,
you go all six, just one month at a time.
Wow, he says, once you're caught up,
restore your original pacman.com file to normal
without the date ranges in there, and you're good to go.
You just keep updating from there, from there forward.
Also a note for the nervous updater,
the program informant, which is in the AUR,
prevents you from upgrading
if there is a fresh Arch news item
that you have not yet read
since the last time you ran updates.
You might want that for your Minjora box.
His base Arch image is still
from the Antigros install he did back during our
Arch challenge, which was March 29th, 2016.
Wow.
And he's kept it running ever since.
Let's hope that's the future of our server.
No kidding.
I mean, you and I were kind of joking.
We were like, this took us about five hours or so,
six, I don't know, whatever, to set up.
But if we were the types to leave something
and not fix something that isn't broken.
Which, I mean, fingers crossed,
maybe someday we'll become those people.
This could be our forever install.
It's our forever server, Wes.
It could, I mean, I like it right now.
Wes, we built a little forever server.
And it's working, which there were moments when it wasn't.
When we first started the containers
and the networking wasn't working,
I thought we had leaped too far,
and I was legitimately concerned
that I would never see my pillow again.
But thankfully, we figured it out pretty quick
while Wes figured it out, really.
I read forum posts and suggested different things
while Wes ignored me and fixed it.
So that worked out pretty well.
What matters is that we fixed it.
Yeah, really.
Anybody in the virtual lug there running Arch
on their production system?
Yep, been running it about since April now.
Oh, really?
On a server or on a workstation?
It's on my XPS 13 and it's also running BusterFS, actually.
I did it as a bit of an experiment
and it's gone quite well.
Oh.
Yeah, I mean, if somebody installs a system on Arch,
they've got to tell you about it.
So that's why this podcast exists today.
We are completing the circle of meme life right there.
I acknowledge that.
Brent, I know you're on Arch as your daily production system.
What did you use for your VPS for WireGuard?
That's something I have yet to solve.
So I didn't actually get WireGuard working on my laptop
because I made a subtle mistake when I installed it
during the Antergos challenge that you gave a long time ago,
which was to put too small of a system partition.
So I'm having a hard time upgrading anything
because I'm out of space there and it's a real disaster.
So it didn't quite work for me.
Oh, we could help with that.
Because in this process, we realized
that we had made boot its own dedicated two gigabyte
partition and then realized, well, crap, we don't need that.
It needs to be part of our root partition.
And then like, what are we going to do with those two gigs?
There are a lot of options.
There's some fancy dancing you can do.
As long as you're willing to wait for Gparted
to drag bytes around.
Yeah.
Yeah.
Well, I admit that my partition is also encrypted.
So that should add an extra level of fun.
Attaboy.
Attaboy.
Got to learn sometimes.
Tell you what, we need to have a laptop support week where
the three of us get together.
I reload to Arch, you reload your box, Wes,
and we help Brent out with his partitioning scheme.
Alex really tried to help me do a fresh Arch install,
and I didn't finish it.
So sorry, Alex, but I'll get there.
I'll get there.
The problem is there's work to be done,
and that system is functional.
It's just tight on space, right?
And hard to update, but it's still working.
I've been there.
Yeah, it sounds like you've been there.
I've been there.
I'm really hoping we don't put the server in that position.
I feel like this minimum viable Linux thing, just enough Linux,
is the key to success here.
What are we going to update on that box that's going to break?
There's not a lot.
I mean, I think the biggest thing will be kernel updates,
but we've got snapshots for that.
Yep, and we can just boot into the old environment,
or boot into the old kernel, or whatever,
because the host operating system will still
load regardless.
And so from there, it's just minutes to resolve an issue.
And I think both you and I felt a lot more comfortable managing
a Linux box versus a FreeNAS box,
simply because of this exact reason.
If we have to SSH into the host, we're good to go.
No problem, we're comfortable there.
And it just lowers the debug cycle.
It's not that we mind having to fix occasional problems,
but if it's a 10-minute thing versus an hour thing,
and it's a real server belt, it's not fast to boot up.
So every time you have to mess with trying to reboot and test
something, it's a long wait.
Everything's slow on booting a server.
But if you've already booted and you just need to test if you
can load a module, that's fast.
Yep, so we're going to do this so you don't have to.
So don't do this at home.
You should probably use Ubuntu LTS or CentOS or something.
I think I am going to rebuild my router on Arch again.
I know, I was thinking it'd be really nice to run those
Raspberry Pi 4s with Arch.
And A, you are.
I appreciate Flatpak snaps and app images, and RPMs, and devs,
and TARs, and all of that.
But one package manager to rule it all, one package manager to
install a package, to update my packages, just so nice.
Like watching you, legitimately, I'm laughing at you hunt
around on Microsoft's site trying to get the teams dab.
Meanwhile, I've just, I literally try and dash S teams
and just hit enter and it just goes out and installs it.
And what I love about it is it's so simple, right?
Like, I mean, you can have these complicated managers that
do a lot for you, but at the same time, I mean,
make packages, great.
The package config files, you can mess with it.
They even prompt you, do you want to make any changes to
this thing?
So it's so accessible because it's like radically simple.
I think I'm going to enjoy that a lot.
Anyways, we'll come back.
If this blows up on our face, we'll tell you about it.
Yeah, you'll hear about it.
But we'll check back in.
Obviously, we haven't determined the win yet, but
we'll pow wow after the show.
Maybe let us know.
Yeah, let us know what you'd like.
That's a great idea.
Look at you, Wes, jupitabroadcasting.com slash telegram
or linuxunplugged.com slash contact, like Richard did here
with his update trick on setting the repo dates back.
Brilliant.
Thank you, Richard.
Like I said, we'll have examples linked in the show
notes if you want to check that out.
Well, Mr. Payne, is there any other bits of business we need
to attend to on today's Unplugged program?
No, I mean, maybe I'll just update the server again real
quick, I don't know.
Yeah, it's like a fidget spinner.
When you feel bored, you just go do a quick update on the
old server.
We already got a snapper update this morning.
Did we really?
Wow, look at that.
Isn't that funny?
Something that's really kind of you think of as an
openSUSE thing.
And here we are all the way over here in
arch land using it.
I just love open source.
Hopefully that update didn't break our update protection.
Guess we'll find out.
All right, well, we're going to wrap it up there.
You know, it is getting towards the end of the year, so that
means our predictions episode.
We've got some special stuff planned.
So please join us live, because there's so much more
to participate in.
But also just sit back and enjoy probably a whole other
show's worth over at jblive.tv.
We do it on a Tuesday at noon Pacific.
You get that converted over at
jupiterbroadcasting.com slash calendar.
I'm also going to give a personal plug to my buddies
over at User Air.
Latest episode was so great.
So funny.
User Air is so fun.
Check it out, air.show, really, really great.
And Brent, another fantastic branch with Mr. Popey over at
extras.show and Westpain over at techsnap.systems.
And this here humble podcast will be right back here next
Tuesday.
Whoa.
You know, Wes, no matter how much we disclaim that people should not do this at home, we're
going to get a lot of crap for installing Arch on a server.
I mean, I think our lawyers advised us to read a long legal spiel about liability.
The Linux Unplugged program does not endorse running your home server on a rolling distribution.
And will not be held liable for any package upgrades that fail.
That'd be really funny if we actually were in that situation, and also horrible.
We will fix your server, but it's $500 an hour.
Oh, yeah, a little side biz right there, a little side hustle.
Thank you, Mumble Room, for being here today.
I really do appreciate everybody showing up at the new time.
It's still kind of new.
Thank you for joining us.
It's still new to us.
Very new.
So, very much a big thank you for being here.
Appreciate you guys.
And appreciate you live streamers, too.
Man, you know what?
It's so much fun live.
While I'm thinking about it, Wes, you know who else I appreciate?
People who download the podcast, too.
Yeah.
You.
Appreciate you.
Not Joe Razington, though.
