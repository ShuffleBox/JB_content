WEBVTT

00:00.000 --> 00:09.680
Hello, and welcome to Linux Action News, Episode 234, recorded on March 30th, 2022.

00:09.680 --> 00:10.680
I'm Chris.

00:10.680 --> 00:11.680
And I'm Wes.

00:11.680 --> 00:12.680
Hello, Wes.

00:12.680 --> 00:14.360
Let's do the news.

00:14.360 --> 00:20.260
And we start with the story that was sent into the show by far the most this week.

00:20.260 --> 00:28.180
The recently refreshed Rolling Remix of the Ubuntu Desktop, codenamed Rolling Rhino.

00:28.180 --> 00:32.520
This new Remix builds on the work our friend Martin Wimperis kicked off, although that

00:32.520 --> 00:35.960
was mostly on a lark, if I recall, right, Chris?

00:35.960 --> 00:36.960
It seemed.

00:36.960 --> 00:41.520
So, you know, Wimpy was really on this a little while ago.

00:41.520 --> 00:42.520
Others have talked about this.

00:42.520 --> 00:47.520
I mean, I actually kind of feel like this idea of a rolling Ubuntu has been popping

00:47.520 --> 00:53.680
up in one form or another for years, maybe since 1804 shipped.

00:53.680 --> 00:57.160
I feel like it's been in the conversation, at least in the background.

00:57.160 --> 01:02.560
I think this idea just appeals to some hardcore Ubuntu users who love Ubuntu as a base, love

01:02.560 --> 01:07.320
that environment, but they just want the freshest package as possible.

01:07.320 --> 01:11.680
Maybe they're an enthusiast, maybe they're a developer, something like that.

01:11.680 --> 01:17.660
The Rolling Rhino website describes the project as the following, quote, Rolling Rhino Remix

01:17.660 --> 01:23.000
is an unofficial Ubuntu flavor which converts the Ubuntu operating system into a rolling

01:23.000 --> 01:26.040
Linux distribution that tracks the development series.

01:26.040 --> 01:31.160
The project also highlights the primary tool they've created to help smooth this whole

01:31.160 --> 01:38.000
new rolling process out, quote, the main tool we've created is Rhino Dash Update.

01:38.000 --> 01:43.120
It will ensure that system updates are ran smoothly and extends the capabilities of what

01:43.120 --> 01:45.640
apt is able to provide.

01:45.640 --> 01:48.480
Well, that sounds fascinating.

01:48.480 --> 01:51.560
And yeah, at this point, apt might need a little help if it's going to be doing some

01:51.560 --> 01:53.200
radical new package promises.

01:53.200 --> 01:54.200
Yeah, yeah.

01:54.200 --> 01:55.200
Huh.

01:55.200 --> 01:56.200
That's a good point.

01:56.200 --> 01:59.000
I wonder what's this is probably something I'm going to throw in a VM to be honest with

01:59.000 --> 02:00.000
you.

02:00.000 --> 02:01.880
I was debating it up until this point.

02:01.880 --> 02:06.240
And then when I saw that tool, I'm like, hmm, nope, gonna give this a go because it sounds

02:06.240 --> 02:07.640
really handy.

02:07.640 --> 02:11.520
It's also nice to see that this has the blessing of our buddy Wimpy.

02:11.520 --> 02:15.400
He's been helping with this renewed effort, it seems filing some bug reports, responding

02:15.400 --> 02:18.760
to team questions, obviously, giving the okay to use the project name.

02:18.760 --> 02:21.440
So you know what, I'm all for this.

02:21.440 --> 02:22.960
I think this is great to see.

02:22.960 --> 02:26.400
I'm really glad this has a couple of developers behind it.

02:26.400 --> 02:29.320
Now I hope they continue and I hope that team grows.

02:29.320 --> 02:34.280
And even though I don't suspect that I'm the target user, because I like probably a lot

02:34.280 --> 02:37.740
of Ubuntu users, I've just kind of adapted to that release model.

02:37.740 --> 02:41.920
So if I deploy Ubuntu on a system, that's probably a box, I don't expect is going to

02:41.920 --> 02:43.640
change very often.

02:43.640 --> 02:47.840
So the end user that I see this is probably going to be the most suitable for would be

02:47.840 --> 02:53.240
a developer, a developer who's likely trying to target the next iteration of the Ubuntu

02:53.240 --> 02:54.240
platform.

02:54.240 --> 02:59.320
And in that scenario, I could see this being a very handy release, and potentially one

02:59.320 --> 03:04.520
that's worth keeping around in a VM by us enthusiasts, just to see where Ubuntu is going.

03:04.520 --> 03:10.240
I use Rhino, by the way.

03:10.240 --> 03:16.880
Continuing off my remarks from the last chapter, and maybe I should be running it on AMD hardware,

03:16.880 --> 03:22.200
at least at this rate, Michael Larble over at Ferronix continues to be hot on the trail

03:22.200 --> 03:29.440
of new AMD Linux positions, writing this week that AMD is recruiting more engineers to better

03:29.440 --> 03:37.440
support the Linux platform, and also develop support for Compute Express Link under Linux.

03:37.440 --> 03:42.660
We've really been watching AMD kind of take a whole corporate level commitment, like the

03:42.660 --> 03:48.920
entire corporate stack, a commitment to Linux and Linux support over the last year and a

03:48.920 --> 03:50.200
bit.

03:50.200 --> 03:54.640
And this is good to see because AMD is part of the Compute Express Link organization,

03:54.640 --> 03:57.380
part of that group that is pushing that standard.

03:57.380 --> 04:01.880
So it's kind of important that some of the key members of that standard are putting their

04:01.880 --> 04:04.860
money where their mouth is and supporting Linux development.

04:04.860 --> 04:08.320
And that's exactly what we're seeing with these new AMD positions.

04:08.320 --> 04:12.800
And by the way, if you're not familiar, the Compute Express Link is an industry supported

04:12.800 --> 04:18.480
cache coherent interconnect for CPUs, memory expanders and accelerators, you know, basically

04:18.480 --> 04:21.720
all the plumbing you might need to make things go fast.

04:21.720 --> 04:31.460
And it's backed by the CXL Consortium, which calls itself an open industry standard group.

04:31.460 --> 04:35.080
Don't you just love it when a good thing keeps getting better?

04:35.080 --> 04:38.620
Wire Plumber 0.4.9 is now out.

04:38.620 --> 04:43.640
And it properly exposes surround sound support to those Linux games that support Dolby 5.1.

04:43.640 --> 04:48.440
Yeah, a little disappointed if you go and invest in that surround sound system and then

04:48.440 --> 04:50.400
Linux breaks it.

04:50.400 --> 04:55.680
It looks like actually the fix landed in Wire Plumber approximately a month or so ago.

04:55.680 --> 05:02.000
And it was the relaxation of format parsing within the SI audio adapter module that actually

05:02.000 --> 05:05.460
appears to be the thing that fixed the issue.

05:05.460 --> 05:08.120
This latest release has a whole lot more than that though.

05:08.120 --> 05:13.860
I mean, there's the sort of usual crash stability improvements, but it also addressed a kind

05:13.860 --> 05:18.500
of nasty race condition within the Zoom desktop app, which could have resulted in your audio

05:18.500 --> 05:21.720
sharing to fail, which that's never a great impression.

05:21.720 --> 05:26.840
It also adds Brave and Edge and Vivaldi and Telegram all to the Bluetooth auto switch

05:26.840 --> 05:30.980
application list, which should be a pretty smooth little experience.

05:30.980 --> 05:37.320
But then there's just a whole bunch of other fixes and improvements.

05:37.320 --> 05:38.880
Linode.com slash LAN.

05:38.880 --> 05:43.240
Go there to get $100 in 60 day credit on a new account and you go there to support the

05:43.240 --> 05:44.240
show.

05:44.240 --> 05:47.640
Linode is cloud computing made simple, affordable and accessible to all.

05:47.640 --> 05:51.120
It's what we use for everything we've built in the last two and a half years and it constantly

05:51.120 --> 05:52.120
gets better.

05:52.120 --> 05:56.440
I just did a free upgrade to their new MVME storage and it's blazing fast.

05:56.440 --> 06:01.520
So take advantage of our $100 offer and deploy Linode on your own and experience what it

06:01.520 --> 06:02.520
can do.

06:02.520 --> 06:04.960
Kick the tires and support the show.

06:04.960 --> 06:10.140
Linode.com slash LAN.

06:10.140 --> 06:11.140
And thanks to Ting.

06:11.140 --> 06:12.140
Linux.ting.com.

06:12.140 --> 06:18.720
I've been a Ting customer since 2013 because they offer great nationwide coverage on the

06:18.720 --> 06:25.000
big duopoly carrier networks, but at a price and with customer support, nobody can touch.

06:25.000 --> 06:29.320
Ting was recently named the number one carrier by Consumer Reports in 2021.

06:29.320 --> 06:33.960
There's a plan for everybody starting around $10 a month and up from there and every plan

06:33.960 --> 06:39.160
gets access to Ting's award-winning customer service and nationwide LTE and 5G coverage

06:39.160 --> 06:40.680
and no contracts ever.

06:40.680 --> 06:46.840
So go to linux.ting.com.

06:46.840 --> 06:51.640
And we wrap things up this week with a series of impressive updates from the Linux kernel

06:51.640 --> 06:52.640
team.

06:52.640 --> 06:57.920
Let's start with a problem that I'm sure impacts just about everyone listening.

06:57.920 --> 07:04.240
You have gosh darn so many NVMe disks that Linux takes too long to shut down.

07:04.240 --> 07:09.520
Well apparently some people do have that problem, including the folks at Google who wrote into

07:09.520 --> 07:11.740
the mailing list, quote,

07:11.740 --> 07:16.960
Some of our machines are configured with many NVMe devices and are validated for strict

07:16.960 --> 07:19.480
shutdown time requirements.

07:19.480 --> 07:24.840
Each NVMe device plugged into the system typically takes about 4.5 seconds to shut down.

07:24.840 --> 07:32.400
So a system with 16 such devices takes approximately 80 seconds to shut down and reboot.

07:32.400 --> 07:37.700
It's rough, you know, 16 NVMe disks, it's just rough.

07:37.700 --> 07:41.580
The register actually does give us some context on this problem they write.

07:41.580 --> 07:45.180
The problem is that the kernel's drive shutdown function is synchronous.

07:45.180 --> 07:50.440
For each drive it waits for the shutdown command to complete before carrying on to the next.

07:50.440 --> 07:55.100
The new kernel patch does exactly the same thing, but changes the way that the calls

07:55.100 --> 07:57.640
are issued to be asynchronous.

07:57.640 --> 08:02.680
It issues the call to the first drive, then immediately moves on to the next, and it works

08:02.680 --> 08:08.560
its way down the list when they all return the desired status, the job is done.

08:08.560 --> 08:12.800
Well while we save up to buy some more NVMe disks, let's take a look at the upcoming

08:12.800 --> 08:17.760
Linux 5.18 release, which is shaping up to be another barn burner.

08:17.760 --> 08:23.520
And one area that's getting a kick in the pants is 64-bit ARM support, adding a shadow

08:23.520 --> 08:26.880
call stack, thanks to the work of Alibaba.

08:26.880 --> 08:31.120
Yeah, the shadow call stack is an instrumentation pass, that's what they call it, and it's

08:31.120 --> 08:36.120
currently only implemented for ART64 systems, but it protects programs against the return

08:36.120 --> 08:40.600
address overwrites, which is essentially, I guess, a stack buffer overflow.

08:40.600 --> 08:45.200
Yeah, it works something like this, when the compiler sees code that's calling into

08:45.200 --> 08:51.480
another function, it first emits code that creates a separate, quote, shadow call stack,

08:51.480 --> 08:55.440
and then saves the function's return address to that shadow call stack before continuing

08:55.440 --> 08:57.160
on.

08:57.160 --> 09:01.280
Once whatever function that was being called is complete and has done its work, the compiler

09:01.280 --> 09:07.180
ignores whatever return address is on the real call stack, and instead uses that value

09:07.180 --> 09:13.120
it sneakily stored on the shadow stack to jump back to the calling original function.

09:13.120 --> 09:14.920
Why bother to go through all this work?

09:14.920 --> 09:19.960
Well, the idea is that if anyone malicious on the system tampered with the real call

09:19.960 --> 09:24.520
stack, which, you know, that's the first place you'd look, it doesn't actually interfere

09:24.520 --> 09:27.320
with the intended control flow.

09:27.320 --> 09:31.960
Sticking with 5.18 for just a bit longer, support for Intel's hardware feedback interface

09:31.960 --> 09:37.920
landed in the thermal subsystem, and the Linux 5.18 power management updates that have landed

09:37.920 --> 09:38.920
look noteworthy.

09:38.920 --> 09:45.080
Yeah, it seems that both AMD and Intel have received important piece-date improvements

09:45.080 --> 09:46.600
this time around.

09:46.600 --> 09:50.820
And in conjunction with some other bits of work, this should give the OS more governance

09:50.820 --> 09:55.720
over the power state the system is in, as well as the idle states of the CPU, all of

09:55.720 --> 10:01.080
which should work together to provide more efficient power usage, and hey, maybe better

10:01.080 --> 10:02.640
battery life to boot.

10:02.640 --> 10:05.120
Now, it's not all good news in 5.18.

10:05.120 --> 10:11.400
It seems our collective cries to save the once great Ryzer FS have not been heard.

10:11.400 --> 10:13.640
Or there were just no calls.

10:13.640 --> 10:17.840
Either way, it seems there is now consensus among the upstream kernel developers to deprecate

10:17.840 --> 10:20.080
Ryzer FS.

10:20.080 --> 10:25.280
Given that there are no notable users of it any longer, the code is barely maintained,

10:25.280 --> 10:29.320
and it's hard to come up with a single legitimate reason to keep it, especially when you have

10:29.320 --> 10:33.120
modern alternatives like ButterFS and XFS.

10:33.120 --> 10:37.360
So the plan is to treat it as deprecated and then formally remove it from the mainline

10:37.360 --> 10:40.800
kernel in 2025.

10:40.800 --> 10:44.280
So long Ryzer FS.

10:44.280 --> 10:46.760
Don't worry though, it's not all bad file system news.

10:46.760 --> 10:50.560
There's some good news this time around, at least for XFS users.

10:50.560 --> 10:51.560
We hope.

10:51.560 --> 10:54.320
The XFS file system changes.

10:54.320 --> 11:00.640
Now it's not actually in 5.18, those changes are mostly bug fixes, important as those are.

11:00.640 --> 11:06.560
But after the 5.18 cycle is complete, it seems that there are some major changes potentially

11:06.560 --> 11:07.560
planned.

11:07.560 --> 11:11.280
Yeah, this came to light after a comment by the XFS maintainer Derek Wong.

11:11.280 --> 11:17.600
He writes, Dave Chinner will be taking over as the XFS maintainer for one release cycle,

11:17.600 --> 11:25.160
starting from the day 5.18 RC1 drops until 5.19 RC1 is tagged, so that way Derek can

11:25.160 --> 11:31.760
focus on starting a massive design review for the online repair feature.

11:31.760 --> 11:37.040
That could be pretty big, so we'll have more details on that when it gets closer.

11:37.040 --> 11:41.840
Our final story today is one we've been following for a bit of time, with an excellent update

11:41.840 --> 11:44.520
this week from LWN.

11:44.520 --> 11:49.640
Back in mid-February, LWN first reported on the plan to unite the two kernel devices that

11:49.640 --> 11:55.860
provide random numbers, effectively making one just a pointer to the other.

11:55.860 --> 12:01.200
And that change made it as far as the mainline during Linux 5.18's merge window, but it was

12:01.200 --> 12:04.380
actually quickly reverted when problems were found.

12:04.380 --> 12:09.600
So it may be possible, LWN reports, that this unification work will continue someday, as

12:09.600 --> 12:15.440
they put it, but for now there's just production environments that need their random numbers

12:15.440 --> 12:20.280
early on in the boot process and can't wait for the entropy to be available or rely on

12:20.280 --> 12:22.860
that Linus jitter dance.

12:22.860 --> 12:25.160
What is the Linus jitter dance, you ask?

12:25.160 --> 12:26.160
Well, fair question.

12:26.160 --> 12:31.120
Well, for several years now, it's a procedure that's been used during boot time to take

12:31.120 --> 12:36.580
advantage of CPU execution time differences to initialize the pool of random numbers in

12:36.580 --> 12:38.120
less than a second.

12:38.120 --> 12:43.880
It basically uses difference in code execution speed of repetitive operations due to just

12:43.880 --> 12:49.680
the sort of inherent unpredictability in modern CPUs, you know, from things like caches, branch

12:49.680 --> 12:52.080
prediction, that kind of thing.

12:52.080 --> 12:57.320
That actually sounds really clever, but a day after the code was merged, a bug report

12:57.320 --> 13:03.360
came rolling in reporting, quote, a large number of QMU boot test failures for various

13:03.360 --> 13:09.240
architectures, and the submitter noted that the common denominator in those boot hangs

13:09.240 --> 13:14.280
was one error message, and it read, quote, saving random seed.

13:14.280 --> 13:18.560
During the user's troubleshooting, they tracked the issue down to that new unified random

13:18.560 --> 13:24.320
number device generator and noted that when they reverted that change, the problems went

13:24.320 --> 13:25.320
away.

13:25.320 --> 13:29.660
You know, it can be tricky on things in data centers or yeah, especially virtualized situations

13:29.660 --> 13:33.880
where you just don't have access to a lot of real physical devices with unpredictable

13:33.880 --> 13:35.880
behavior.

13:35.880 --> 13:41.260
So for now, the kernel team is keeping things the way they've always been, and the generator

13:41.260 --> 13:45.000
unification must wait for another day.

13:45.000 --> 13:49.420
But all that said, it does seem like there's more thought and work going into Linux is

13:49.420 --> 13:54.980
random number generation systems these days, and that's got to be a good thing.

13:54.980 --> 14:00.640
I am impressed with the amount of energy and focus all these different deep layers of the

14:00.640 --> 14:05.560
kernel are getting and we've been talking about device drivers rewritten in rust, critical

14:05.560 --> 14:11.040
power support for the different primary CPU platforms, this AMD stuff coming in the file

14:11.040 --> 14:12.880
system improvements.

14:12.880 --> 14:15.680
Every level in there seems like it's getting some attention right now.

14:15.680 --> 14:20.640
And that's just so impressive for a project the age and size of the Linux kernel.

14:20.640 --> 14:23.320
We'll keep an eye on all those developments and everything else happening in the world

14:23.320 --> 14:26.480
of Linux and open source news and keep you posted.

14:26.480 --> 14:31.380
So make sure you get every single episode by going to linuxactionnews.com slash subscribe

14:31.380 --> 14:33.840
for all the ways to get new episodes.

14:33.840 --> 14:38.940
And don't forget linuxactionnews.com slash contact for ways to get in touch.

14:38.940 --> 14:42.920
If you'd like to get Linux action news ad free become a member at Jupiter.party and

14:42.920 --> 14:47.240
get all of the network shows ad free with all of their extra special content.

14:47.240 --> 14:52.600
Either way, we'll be back next week with our take on the latest Linux and open source news.

14:52.600 --> 14:53.600
Thanks for joining us.

14:53.600 --> 15:23.200
And that's all the news for this week.

