Hello, and welcome to Linux Action News, episode 175 recorded on February 6th, 2021.
I'm Chris.
And I'm Wes.
Hello, Wes.
Let's do the news.
And let's start with some chatter you might have heard this week regarding Raspberry Pi
OS and a secret Microsoft repository.
It turned into a bit of a storm in a teacup, so we wanted to give you some context.
In a recent update to Raspberry Pi OS, previously known as Raspbian, a Microsoft hosted app
repository was installed on all machines currently running the OS without the administrator's
knowledge or explicit permission.
By all accounts, this change was made to make it easier for users to get access to VS code,
which is their recommended IDE for the Pi Pico.
But something that's curled a few eyebrows is you're even going to get this installed
if you use Raspberry Pi headless as a server.
And it also means that every time you do an apt update on your Pi, you're pinging a Microsoft
server.
Additionally, it means that a Microsoft GPG key used to sign the packages in the repository
was added and now trusted by the system.
As you can imagine, some Linux users were just not happy about that.
Though I think it's probably safe to assume that most users, especially the ones Raspberry
Pi OS targets, care more about easily getting access to VS code than Microsoft collecting
some metadata.
For comparison, how would you feel about this repo coming pre-installed on, say, a new Dell
developer laptop?
I don't think that would surprise me.
I don't know if I would love that, but ultimately I think I would end up installing VS code
myself, regardless.
And if I'm on an Ubuntu system, I probably would use their dev which adds the repo.
So I'd actually probably just do this to myself.
But honestly, if I could choose, my preference would be that, say, a company like Dell ship
it as a Snap or a Flatpak.
That I think would be even better.
But to your point, I would not be surprised.
And it's something that Windows OEMs have done to an extreme for years.
I think consumers have actually normalized it for so long.
And we have to remember, they're both the OEM, like, you know, a Dell, and they're also
the Microsoft in this comparison.
They're also the ones creating the OS.
They're fulfilling both roles.
Yeah, to that end, my other tempering thought on this, and not to say I don't understand
why people are upset.
This was a change that could be concerning for a lot of folks.
But the Pi is designed as an educational device.
And to this end, Linux is kind of just an implementation detail.
Windows also runs on the Pi, and it certainly also runs VS code.
We could have been living in a world where the Pi platform was built on Windows.
I'm glad that's not what's happening.
Yeah, okay.
Yeah, it could be worse, I guess is what you're saying.
I think, though, it's two things.
I think the community has higher expectations for systems that ship with Linux.
Just because Windows OEMs got away with something, it doesn't really mean we should repeat those
sins.
And I think there's also an issue around transparency about this change.
It didn't end up in the changelog until a couple of weeks after it was shipping.
When the community noticed the change, it was already done.
It was already final.
There was no upfront heads up about this.
That could have gone a long way.
And when the community did find out and respond, the conversation was shut down really hard.
Threads were closed.
There wasn't really much entertaining of the conversation at all.
And you're right, I get this is the vocal minority.
But I feel like that minority is also the most passionate fan base or the group that's
often pushing the products forward to be better.
They're the ones that are telling companies you need to do better and they're telling
foundations you have to do better.
And when they do that, they also end up being their biggest advocates.
So I think they're a demographic that, while might not be the key target, is probably still
worth listening to for market signals.
Yes.
Well, and just also perhaps in some of the technical advice, say.
Because like you mentioned before, you get this even if you're running on a headless
server.
Maybe there were some other options that if the community had been consulted, you could
have worked this out in some sort of first boot experience that lets you easily check
and add VS code if that was something you're interested in.
I don't know.
The Linux community is very creative and probably could have considered some better options
for this.
This story, though, there's a few things that are notable about it for me.
One of them being, I am not a big proponent of people actually using Raspberry Pi OS on
their Raspberry Pis if they are a general Linux user.
If you're brand new to the category, yeah, they do some stuff in there that make it easier.
If you've run Linux for a couple of years, I think the Raspberry Pi OS might not be the
best OS out there to look at.
And I think it's in part because there's this disconnect between the expectations and priorities
of a Linux enthusiast and the priorities of companies or foundations that are utilizing
Linux for a product.
The foundation, like you say, really just wants something for a kid.
They want something that is approachable by schools, that's approachable by everyday people
and that's a really noble goal.
But it's not quite in line with what Linux users want.
Linux users want an OS that frees people from surveillance, among many other things.
I think that's why this strikes such a chord.
Yeah, it's always tricky when diverse communities come together on the same thing but with somewhat
different objectives.
But thankfully, the Pi is an excellent general purpose Linux device and it's quite easy to
switch out your distro.
Indeed, I've had a lot of success with Ubuntu and Arch on my Pis, but this week, another
flavor of Ubuntu was updated that features Pi support.
Canonical released Ubuntu Core 20.
Ubuntu Core is a trimmed down version of Ubuntu and with this latest version has been rebased
to Ubuntu 2004 LTS.
This release also adds support for full disk encryption, secure device recovery and other
features particularly focused around increased security.
And of course, it's all built on SNAPs.
Even the root file system itself is actually a SNAP that can be upgraded or downgraded.
You could see how useful that is in the context of a product OEM.
It's a version of Ubuntu that's more interchangeable.
It's designed to be supported by hardware manufacturers for many years.
Helped by that is that Canonical provides 10 years of maintenance for Ubuntu Core, which
as you can imagine, pairs nicely with the SNAP system because you can keep running the
same underlying operating system, SNAPs ship their own dependencies built right in so you
can run the latest applications and keep the underlying base the same.
For their part, Canonical says that there are already tens of thousands of industrial
and consumer IoT devices from the likes of Dell, Bosch and others.
Yeah, I know when I was down at Dell a couple of years ago even, they were working on an
earlier version of this for their devices.
They seemed to like it a lot and I think they've just doubled down on that effort.
And speaking of that Pi support, Ubuntu Core also supports the new Raspberry Pi compute
module to boot.
Linode.com slash LAN.
You go to that URL to get a $100 60-day credit towards your new account.
Linode is the largest independent cloud and they're our cloud provider.
We run all of our new infrastructure on Linode.
They are perfect because they have a good mix of technology and price.
They're about 30 to 50% less than the other major cloud providers like AWS or Google.
Have you ever thought about maybe setting up a desktop in the cloud?
Linode totally embraces that.
They have a document on their website that I'll link in the show notes that walks you
through setting up the HTML5 Guacamole VNC client and XFCE and even doing the whole thing
with an SSL certificate.
So everything is nice and secure.
On top of that, on their YouTube channel, they have a video that walks you through how
to do it.
They're on board with you using Linode however you want.
And that's great for me because I have a custom VPN system I had to set up that works with
my LTE router.
It's this crazy custom Linux OS and Linode has a guide on how to get something like that
on a Linode server.
They make it really easy to use if you've never managed a server before with their cloud
dashboard.
But if you know what you're doing, if you want to get under the hood, they're totally
are on board with that.
They're Linux users themselves.
That's how they got into the business back in 2003 before it was even called cloud computing.
They saw where Linux was going.
They use the technology.
They love the technology.
And I really click with that.
On top of that, they've been a big supporter of the community for a long time, making events
like Linux Fest Northwest possible.
And now they're sponsoring independent Linux media and making it possible for a whole group
of creators to give their content away for free.
And you can get in on that action with a $100 60 day credit.
Just go to linode.com slash LAN.
See what Linode is capable of.
With that $100 credit, you can try out object storage, dedicated CPU rigs, GPUs, etc.
I mean, there's a lot you can mess around with and learn and then deploy as your back
end infrastructure, linode.com slash LAN.
This week, Docker announced it has contributed its Docker distribution project to the Cloud
Native Computing Foundation.
If you're not familiar, Docker distribution is the open source code behind the Docker
hub container registry and many other registries.
Yeah, that Docker hub back end became known as the Docker distribution after the team
took on a major rewrite of the original registry code, which was written in Python and it was
a really early rough design.
This new version is written in Go and it's been designed to be an extensible library
of sorts.
Different back ends and subsystems could be designed to use it.
So why is Docker doing this?
Well, they address that in their announcement and say that there are now many registries
out there with a lot of different companies and organizations providing registries internally
or offering them as a service.
Essentially, it's just become a confusing mixture of different Docker registry forks.
Many of these are based on the code in Docker distribution, but have small forks and changes
that they were not contributing back upstream.
Docker hopes to make the project an industry wide collaboration with this move.
This is also why they chose to host it in the Cloud Native Computing Foundation, known
as the CNCF and is sort of a neutral place for cloud tooling used across different companies.
Yeah, and you also kind of have to assume this is a play to stay relevant, right?
We're looking at a lot of different ways to run containers now.
It's sort of become table stakes for a lot of different application deployment scenarios.
So what do you do when your secret sauce is now built into every sandwich Linux sells,
right?
Like, what do you do as Docker?
I think this is sort of a continuation of the next phase of Docker.
They're no longer necessarily leading the pack and are just trying to stay relevant
and are one of the many players.
I think hoping that this way, at least the code that they've designed can be out there
and useful, even if it's not just in their sandbox.
Well, and you could see how they still have some influence in this capacity.
They still have name recognition, they're still recognized for having some really good
thought and ideas in this space.
So I think this is something that plays well to that strength.
The project is going to drop the Docker moniker and just be called distribution and it's going
to be available on GitHub.
Of course, we'll have a link in our show notes.
And as of now, the distribution project has been accepted into the CNCF sandbox, but they
know what they're doing and they consider the project to be pretty mature.
They're hoping it just moves to incubation shortly.
It's got these stages it has to go through.
We learned this week that Google is funding a project at the Internet Security Research
Group, the folks behind Let's Encrypt, to port a crucial component of the Apache web
server from C to Rust.
Oh yeah, of course.
This plan here is it's looking pretty good.
They're going to develop a module called mod TLS, nice and simple.
It'll be based on Rust TLS, which is an open source Rust library developed as an alternative
to the C based open SSL project.
Yeah, Apache strikes me as a pretty good candidate for this work since it's got a modular design
already and that should hopefully make it possible to just easily replace those individual
components and modules without having to rewrite the whole dang thing.
But all the Rust hype aside, this actually does seem like a great fit.
Rust was designed as a memory safe programming language and it comes with protections against
memory management issues that have often resulted in security flaws, which that sounds like
a that sounds like great protections for a TLS library.
And memory safety vulnerabilities have dominated the security field for the past decade.
They've been no joke.
They've been used to take over entire systems, desktops, IoT devices and so on.
I mean, a great example of that is the pseudo vulnerability we were just talking about last
episode.
Yep.
And just to put a little more data behind that, Microsoft said in 2019 that the percentage
of memory safety issues patched in its own software had hovered around 70% of all security
bugs for the past 12 years.
That's just huge.
That's damning.
Overall, this is a pretty interesting experiment.
I mean, it's a good test candidate.
See how it works.
TLS is a good fit.
And if it goes well, I mean, I would expect to see more module conversions in the near
future.
I mean, I'm starting to wonder if this isn't some sort of slow rolling Rust takeover.
There's even some great arguments for just writing kernel modules in Rust.
And of course, it can never seem to get enough love over on the Coder Radio show.
It also seems borne out by the data.
Last year, Rust broke into the Teobi index top 20 list of the most popular programming
languages, and it's already been voted the most loved programming language on the Stack
Overflow Developer Survey for the last five years.
Linux.ting.com.
Today's episode is brought to you by an all new Ting.
Ting is now a smarter choice for everyone.
I've been a customer forever, and it's never been a better time to switch to Ting.
It's a smarter choice.
You can get talk, text and data for just $10 a month.
Limited plans starting at $15 and unlimited plans at $45.
And if you use two gigs or 20 gigs, there's a perfect Ting plan for you and the fam.
And no need to worry, even with all these changes, the only thing that's really going
to be noticeable to you is a lower monthly phone bill.
You still get access to Ting's award winning customer service with nationwide LTE and 5G
coverage, and no contracts ever.
Ting mobile customers can now choose from three different plans based on your data needs.
And with three nationwide networks to choose from, I'm currently on Verizon, it's never
been a better time or simpler to switch to Ting.
Pretty much any phone will work with Ting with those different networks.
So start by going to linux.ting.com, check out your current phone, create an account
and pick the plan that's just right for you.
Ting will then send you a SIM card that you pop in your phone and activate within minutes.
Cutting your phone bill in half has never been easier with Ting's mobile plans.
Get talk, text and data for only $10 a month.
If you're like me and you can use Wi Fi to sync your podcasts and your music, this could
be an incredible way to save time.
The next generation of Ting mobile is here.
See how much you could save.
Go to linux.ting.com.
And a big thank you to Ting for sponsoring Linux Action News.
In a milestone of sorts, Alma Linux, the open source enterprise level Linux distribution
created as an alternative to CentOS, has been released in beta with most Red Hat Enterprise
Linux packages available.
Indeed, this is one of the first ones of these out of the gate.
And Alma Linux is binary compatible with RHEL.
You'll recall that they're backed by Cloud Linux.
They've infused the project with a $1 million annual sponsorship.
As promised, the main development and maintenance were done by the Cloud Linux team.
And now we appeal to the community for its contributions, writes Cloud Linux's founder
and CEO, Igor Seletski.
Yeah, I can understand.
I mean, it's a big undertaking.
This is named after the project, is named after the Latin word for soul.
So it's Alma Linux, is that what it is?
I believe so.
Yeah.
And they're kind of just coming, they're just kind of saying now to the community, come
on in.
We've created this thing.
Come on in and let's work together on it.
I think that's probably a good message.
It's interesting, though, because Rocky Linux is starting from that position, that before
there's even a distribution shipping of any sort, they're starting with the community
position.
Right.
Whereas this effort is more of a let's hurry up, get some up over the wall, and then we'll
slowly let the community in.
And there's probably valid reasons to do both of them.
But as we watch these different CentOS alternatives emerge, these are going to be the interesting
differences.
And like you said, it is a milestone of sorts that they were the first.
We should note that the finalized source code is not yet available, though that's expected
to arrive once there's actually a stable release after some feedback from the community.
I'm actually thinking I'm going to throw this in a VM.
I almost did it before the show, but I think I'll throw it in a VM afterwards after we
get done tonight and then just get a sense of it.
I'm not expecting much difference, but might as well start looking now.
Well, you know, you should write this down.
They have their own bug tracker set up.
So if you do run into any issues, there'll be a link in the show notes and you can go
report them and hopefully get them addressed before that stable release.
While you're going through those notes, check out a link to Hector Martin's patch.
It's a series of patches that bring up initial support for Linux on the Apple M1 SoC that's
used in the mini MacBook Pro and MacBook Air models in 2020.
Something that really strikes me about this, Wes, is there was some esoteric hardware challenges
he had to solve for.
And that's really what is worth reading if people are interested at all in this.
Yeah, I mean, it's both a patch set to try to get this stuff working and also something
of an explorer's notes, right?
He's got his hands on this hardware, started playing with it and figuring out what is Apple
actually doing in there?
Yeah, and there's some interesting discoveries.
This is going to be a long process.
But what struck me about the way Hector Martin has put all this together is he knows what
he's doing here.
He knows how to send a series of patches and information upstream to the maintainers.
And he's got a lot of source documentation.
He talks a little bit about their project and it's just packaged really well.
Nicely broken down.
Yeah, you can tell he's gone through this process before.
And of course, part of the battle is actually making the stuff work.
But the other half, maybe more than half sometimes, is getting that working code into a shape
that's acceptable to the upstream kernel community because they're going to be the ones helping
to maintain it in the future if it gets merged.
Yeah, and we wonder where Corelian will fit in with all of this.
But ultimately, this is the beautiful thing about free software is it's all going to go
upstream.
And that's where the support will land, ultimately.
They can stage this in their downstream projects, that's something Hector's doing, and they're
doing that pretty successfully.
But ultimately, the goal is for mainstream support.
And we've waffled a bit on this show.
Is it a good idea or a bad idea?
Like I think a lot of our listeners have.
But at the end of the day, I'm always a fan of Linux being able to breathe life into older
hardware.
Even if this becomes something where he's chasing support, although Hector thinks once
he gets the M1 figured out, it's not like the M2 is going to be some radically different
CPU.
It's probably going to be a lot of similarities there.
Hector's actually feeling very positive about future support.
But even if we could just get legacy support and pump life into an old machine or get somebody's
data off of a failed box, I feel like it's a worthwhile project.
My only question is how many of these patches have to get merged before we see you picking
up an M1?
It's got to be a pretty smooth experience.
Although I am somebody who really does appreciate performance and a smooth system, and that's
like the number one thing people say when they write into the speaking to Koda radio,
when they write into Koda radio, they're always telling me about how smooth and fast everything
is.
I could really get down on that.
I think that'd be wonderful.
For me, I think I can get most of the way there with technology available to us today.
You know, Ryzen system, maybe a 120, 140 Hertz screen, dedicated GPU, a fast disk.
And I can have pretty good performance.
I'm not really feeling the need to get an M1 box to have ultimate performance.
That's one of the things I kind of like about X86 and that ecosystem is you could today
build a box that's faster than an M1 Mac.
You could do it for a couple thousand dollars.
Most laptops and desktops are maybe just, maybe not like these performance machines,
but you could really build a monster X86 system.
I always liked that capability.
Yeah, you can put whatever you want in there.
Choose how much RAM.
No, no limits.
Yeah.
Isn't that crazy?
Isn't that crazy?
So it's not really calling to me just yet, but I could definitely see one day loving
the fact that system rescue CD boots on this thing and I can recover files from my mom.
That actually is the totally reasonable use case for me.
The other part I like about all this is that Hector Martin's been doing some coding live
streams over on YouTube.
And if you're at all interested in seeing the setup, you know, he's using a lot of plasma
technology in his day-to-day dev flow.
So that's just neat to see and it's an interesting, you know, access to this sort of stuff that
only happens in open source.
It's an interesting journey that we've been following and that probably is the other reason
why I would maybe suggest checking out the link in the show notes because we follow this
from concept now to upstream patch submission.
That's pretty cool.
That's one of the things I love about covering these stories and covering open source in
general is we can actually follow something like that.
So go to LinuxActionNews.com slash subscribe for all the ways to get new episodes every
single week.
And don't be a stranger LinuxActionNews.com slash contact.
And if you're looking for your next podcast to listen to check out JupiterBroadcasting.com
for all the great shows.
We'll be back next Monday with our weekly take on the latest Linux and open source news.
Thanks for joining us and we will see you next week.
