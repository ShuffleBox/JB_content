Hello, and welcome to Linux Action News, episode 209, recorded on October 3rd, 2021.
I'm Chris.
And I'm Wes.
Hello, Wes.
Let's do the news.
On Tuesday, Linus Torvalds sat down for the now-annual check-in at the Open Source Summit
in North America.
He took questions from Dirk Hondl, an early Linux contributor, and the chief open source
officer and vice president at VMware.
It was clear from the beginning, the recent milestone of Linux's 30th birthday would be
a theme for the night.
How could it not?
It's a major milestone.
It started, and it's a good chat, we'll have a link to the full details in the show notes,
but it started with Linus's memories of the first hours of the operating systems release.
In fact, he recalls that he intended to call it Freaks, but that was changed by a stern
FTP admin.
Linus says, quote, I had already been told that the FTP site that I put it on, that I
did not maintain, had a directory called Linux.
So I had actually changed the name in the kernel main makefile to be Linux at that point.
Sometimes it's easier to change the name of your project rather than convince that mean
FTP admin.
He went on to say, I am eternally grateful for two other people having much better taste
than I did.
I got to say, I do like the name Linux better than Freaks.
No kidding.
I would hate to go around saying I'm a Freaks user.
Among a range of other topics, longevity of the Linux community itself came up with Torvald
saying, I think that's a testament to how good the community on the whole has been,
and how much fun it's been.
Yeah, it's pretty wild, too, when you consider that when the project started in 1991, it
was a tiny collection of people.
And now that's just exploded to this worldwide phenomenon.
It's making history every day that it exists.
And Linus made it clear that he still considers that one of the building blocks of Linux and
its community is, quote, just for fun.
He said it's part of what he still strives for.
And just think of that for a moment.
If you're a Linux user out there that just uses it for fun, that's literally one of the
intentions that Linus has in mind.
And it was within that context, too, that the topic of Rust in the kernel came up.
And addressing that, Linus said, quote, from a technical angle, does it make sense?
Who knows?
That's not the point.
The point is for a project to stay interesting and to stay fun.
You have to play with it.
As for where it's going, Torvalds noted that people have been talking about Rust in the
kernel for a long-ish time now, and it's still not done yet.
So we'll see.
Probably next year, we'll see the first Intrepid modules being written in Rust and maybe being
integrated in mainline.
Yeah, he still seems slightly skeptical.
He notes that people have been talking about this for a long time.
We haven't seen it yet.
That seems like a reasonable timeline to me.
He then ended the chat looking forward a bit, and when he was asked to sort of project out
for the next 30 years, Linus joked and said that, like the Linux kernel, he doesn't make
plans more than six months out.
And although he said he's been very happy working on the Linux kernel for the last 30
years, he somehow doesn't see himself working on the kernel when he's 70.
This week, the Pipewire team updated us on their next major series of goals.
Pipewire from the start was designed around handling the needs of both audio and video
streams on Linux.
And while Pipewire is already in use for screencasting and recording, especially now with Wayland
and recently we even saw OBS add support, recently much of the development's actually
been focused on addressing audio issues.
But now that the audio support is in reasonably good shape, Red Hat will be working on a fresh
round of video feature work for Pipewire, led by its founder, Wim Taymans, and working
on improving video capture support on Linux in particular.
Yeah, now this is interesting, and man, do we just love watching this team go.
They understand that getting community buy-off is the difference between a smooth transition
and fighting uphill for every single win.
And instead of reinventing the wheel, they're going to work with existing software stacks
where it seems to make sense, like maturing GStreamer elements to use Pipewire appropriately.
And then also integrate Lib Camera into Pipewire.
Now Lib Camera is an open source stack and framework for Linux, Android, and Chrome OS.
That means we could see a lot of developers that already understand how to work with that.
Now the current state of video capture on Linux is not great.
Usually you're using a webcam, but it can be professional devices in some cases.
And that will use the Video for Linux 2 Kernel API.
Now this Linux for Video API is only meant for a single application at a time, much like
how also audio only used to allow a single application to use the audio device at a time.
But more so than audio devices, making things even more challenging with cameras is they
are just getting more and more complex and feature rich at a really rapid pace.
And that means that the bar for an application developer just trying to use these APIs to
write a desktop Linux app is pretty gosh darn high.
They have to worry about working with other apps on the system and about the security
issues of direct kernel access.
No thanks.
Christian Schaller made this point on a blog this week about these new goals, saying, quote,
there's no need to rely on the kernel API anymore.
We have GStreamer and Pipewire plugins for Video for Linux 2.
You can just use GStreamer or Pipewire's API instead.
And really that's the point here.
Most of what is needed for Pipewire to solve these problems is here or close to here, or
we have some ideas, just need some more work from Wim and for developers to be willing
to adopt the Pipewire API or the GStreamer API as a better solution going forward.
Yeah, that's it, Wes.
So the idea here really to take the big takeaway from this is developers will start targeting
Pipewire and GStreamer to get access to video devices on Linux.
Under the hood, Pipewire will use Lib Camera and Video for Linux.
It'll manage that aspect of it.
The end result is you get the ability for many applications at once to use a video device
and you prevent these applications from using a kernel API directly, both for end users
and developers.
It's really just going to be Pipewire and GStreamer that we're interfacing with.
Just like how these days, for the most part, I only worry about PulseAudio.
I don't have to think about also.
This new focus is also important because when you look at the trends going on in the Linux
desktop right now, adoption of this Pipewire approach for access to video devices should
also allow both sandboxed and non-sandbox applications to share those resources in a
way that doesn't blow up your system.
Yeah, it means we will have, regardless if it's a snap, a flat pack or an application
installed from your repository, we are going to have a truly unified video and audio API
for Linux, something honestly that before the Pipewire project launched, I truly thought
we would never have.
And I kind of was a little sad about it as a content producer.
I really felt like this was something truly missing.
And to see the project refocus now on video and come at it in a way where they're looking
from the very beginning for community adoption and they're going to reuse components where
it makes sense today, I just think this is a fantastic approach.
Developers should be able to get started pretty quickly too because they'll be able to take
advantage of the well-established and documented Gstreamer, which will just now have a Pipewire
plugin.
And just doing that, developers will be able to take advantage of a modern, fully unified
multimedia subsystem that runs on a free operating system.
And for us end users, hopefully this all just means that our audio and video applications
just work as we expect.
And for AV dorks like you and I, Chris, hopefully it means our modern and professional gear
will also find some better support.
Docker Compose, a tool that lets you simply set up one or many applications in a container
via an easy to read YAML file, got its first major update in quite a while.
The big change with version 2.0 is Docker Compose is no longer a standalone package
or application.
Compose has been entirely rewritten in Go and it's now just a Docker plugin.
For end users, not much is really changing.
When you execute it now on the command line, instead of executing docker-compose, you're
now just going to run docker-compose.
As for how you're going to get it, well, you can't install it from pip anymore, but we
do expect distros to package it with Docker or maybe change their existing Docker Compose
package to install the new plugin.
I don't know.
It's not yet fully clear exactly how they're going to pass this change on, but we should
note that Docker has a transition tool out there, a little helper alias script that kind
of bridges the gap between 1.x and 2.0.
So maybe distributions will just ship that.
You know, as somebody who doesn't use this tooling on the daily, and I have to figure
there's a few in the audience out there like this too, I maybe use it a few times a month.
From where I sit, my perspective as a Docker user is it feels like they're constantly changing
and messing with the tooling a little bit.
I know you use it a lot more.
So do you get that sense as a daily user?
Yes.
And no, I would say no for the core technology, you know, like the actual Docker daemon and
command line.
Yes, there have been some changes, but from an end user perspective, nothing too crazy.
But for Docker desktop, you know, that Mac and Windows application and for some of the
Compose stuff.
Yes.
I mean, I think we kind of had to expect some of it after they sold off their enterprise
business and started focusing more on the developer experience side of the story.
And Compose has kind of been an awkward stepchild from the start, you know, it was this third
party extra edition on the side, sure, you know, it was from the same people, but I never
really got a great sense of just how integral the project really treated it.
So in that sense, I think this is a step forward.
But you're right, it is a change.
Yeah, I totally agree.
It never really made sense as a separate package.
And it felt like this isn't like a core part of the project.
And so even a change like this that makes sense, and I'm glad they're making this change,
actually it seems sensible, it leaves me feeling like maybe long term Podman is in my future
because that tooling is using a lot of established stuff already built into Linux that doesn't
change as much and it feels like long term.
For my use case, that's a more stable tooling trajectory, I guess we'll just have to see.
Just a quick note for you Raspberry Pi four fans out there, self hosted guest and active
YouTuber Jeff Geerling recently noticed that the Raspberry Pi 400's very slightly upgraded
CPU has now made its way to the standard Pi four.
This low key spec bump changes the amount of memory addressable by the eMMC and the
PCIe bus, and that means extra memory addressing space, a big boon for those eight gig Pi four
users.
Also, the CPU revision is known for running slightly cooler.
Jeff suggested the Compute Module four remains the best option for those wanting to mess
around with PCIe peripherals, however, we'll have a link to his post in the show notes.
Hello listeners, please do excuse this out of time clip from a very near future.
I'm out here working on a physical box and it reminds me of how I got started in IT.
When I deployed a server, that meant working with a vendor to spec a box and getting a
PO quote, taking that to management and having them approve that purchase, and then I would
go back to my representative and order it, a couple of weeks would go by and I'd get
a chassis, an empty chassis, and then I'd get the CPU and the memory and the disk and
the network controller and the SCSI controller and all of it in separate boxes, then I would
assemble that server, like it might have been an HP Proliant or we did some IBM servers
for a while and Dells, the power edges, lots of Dells, and then we would put them in a
rack and that meant carrying this heavy box and installing rails into a rack and then
mounting the thing into those rails, which there was different rail systems all the time
so each one was unique, in fact some of the racks couldn't even be compatible with other
rails, it was very frustrating, then you had to run the power and the KVM so you could
get access to the console and the networking and then you had to provision that hardware
and it would basically do one application.
And today when I think about how easy it is to deploy infrastructure on Leno, it just
completely blows my mind, like I in just a few clicks can have access to power that even
if I had an unlimited budget as one guy, I couldn't have access to the networking infrastructure,
they are their own ISP and I just would never be able to do that, and you know the great
thing about Leno too, it's like what they focused on forever, so now they are the world's
largest independent cloud provider and they're just investing in making it better and better,
they just recently upgraded their block storage to have faster PCIe NVMe drives and here I'm
sitting here thinking like how am I going to get all these disks in this physical box
and of course I can't transition these disks to the new server I'm going to be putting
in here because they're a totally different size, not to mention I'm not even sure if
the interface is the same, oh man it's so much simpler with Leno and you know they're
30 to 50% cheaper than the major hyperscalers that are just trying to lock you in all the
time, Leno has things like S3 compatible object storage, cloud firewalls, DDoS protection,
a powerful DNS manager and then on top of it they just run really great servers, if
you want it to run on Linux and you want it to run great, just put it on Leno, I don't
know man, these old physical boxes, that's for the blokes working in data centers now,
linode.com slash LAN, go there, get $100 in 60 day credit on a new account and you support
the show, linode.com slash LAN.
And thank you to Ting for sponsoring this episode of Linux Action News, linux.ting.com,
Ting is a mobile virtual network operator, in short what that means is instead of spending
their money digging holes in the ground to stand up a tower, they can focus on building
out excellent customer service and figuring out where they can add true value and differentiate
from the duopoly out there.
And that means you get better prices but get access to the big carrier networks, Ting Mobile
has nationwide coverage, a great mix of plans and they have LTE and 5G data.
And something I really appreciate since being a customer since 2003, Ting's plans are simple
and easy to understand, they're just a smarter choice, I would encourage you to go to linux.ting.com
and see how much you could save and then take $25 off by visiting linux.ting.com.
They have plans that literally start at $10 and top out around $55 and they have great
family plans where you can share a pool of data and have unlimited calls and texts and
of course all of the plans, all of the plans get access to LTE and 5G where it's at.
But I think maybe the best thing about Ting and ironically maybe one of the reasons I've
stayed with them as long as I have, they have no contracts, no contracts ever.
So not only is it super simple to switch to Ting and they have multiple network support
so most phones work but you also never get locked into a contract.
Just go see how easy and simple it is to switch by going to linux.ting.com, check out your
current phone, create an account, you pick the plan that's just right for you.
Once everything looks good to go Ting will send you a SIM card, you pop that in your
phone and then you're going to get activated in minutes and like I always say, they have
a clean beautiful dashboard that sets the bar for the industry.
So much of what you're going to want to get done when it comes to your account and your
service you can do it through the dashboard.
So go check out the Ting website by going to linux.ting.com and while you're there visit
the blog, that's where they do giveaways and kind of tech tips and all that kind of stuff.
Cutting your phone bill in half has never been easier with Ting's brand new plans.
Go to linux.ting.com, it's the next generation of Ting Mobile, it's never been a better time
to become a customer.
So go to linux.ting.com, see how much you could save and take 25 bucks off that, linux.ting.com.
It caught our eye this week that an experimental open-source self-hosted SnapStore server
with the name LOL has been released for testing on most distributions that support Snap.
In addition, back in June, Project Kebe was announced another open-source SnapStore implementation.
Yeah, not just one but two of them, I mean that's good to see, I suppose the question
I have is why now, this has always been possible and I just wonder if it isn't just more buy-off
on the concept of universal sandboxed applications.
We touched on this recently when we mentioned Firefox would be shipping as a Snap on Ubuntu
and there just wasn't a big reaction to it.
Now we see maybe another data point that these are just becoming a little more accepted as
a way to distribute software and I've also heard from folks that are using universal
packages in production and enterprise environments that snaps tend to work, well just do work
in headless and server environments but major enterprise providers are going to want to
host these repositories locally, something they can't really do with Snap today but perhaps
that's driving some of these projects.
I'm not sure but if you have an idea out there, dear audience, let us know, linuxactionnews.com
slash contact.
Why are we just now seeing self-hosted SnapStores?
We like to keep a close eye on major moves in the cloud market, after all, the cloud
is powered by Linux and if we're being honest, it's also what pays the bills for Linux.
So that's why we need to talk about Cloudflare's recent announcement of R2, an S3 compatible
object storage service that has no egress cost and can sit in front of Amazon S3.
And while Amazon S3's prices have come down over the years, it's definitely not free.
R2 can really sit in front of any S3 compatible object storage, which that could have some
seriously interesting ramifications when you're looking at using multiple cloud providers.
But going to the Amazon example here because that's clearly going to be the big impact,
Cloudflare has been pretty clever.
You can layer this in front of Amazon S3, pull the files out of S3 once, store them
on R2 and then serve them there with no egress costs ongoing besides the service costs.
That's really going to shake up the lower end of the market and AWS has made a bit of
an empire on being cheap to get files into S3 and then costly to get them out.
Yeah, it seems very clear that R2, notably one less than S3, is going directly at a cost
sensitive market, an area their CEO has been focused on since their launch in 2010.
Here's Matthew Prince at TechCrunch Disrupt 2010 during a Q&A that asked him how he could
prevent Amazon from eating his lunch.
So from a competitive standpoint, obviously, you're intruding on some of the stuff that
the bigger boys are doing and they've been at this for a long time.
What's to stop them from coming in and sort of replicating your model?
Yeah, there are companies that are doing things at the high end of the market and they make
very fat margins doing them.
I'm really a big fan of Clay Christensen, he was a business school professor of mine
and I like the idea of businesses that come in from below.
So the big incumbents have an innovators dilemma trying to come down and deal with a company
like ours but we welcome the competition, we think we make a really great product, it's
designed for a certain type of users that's very different from the users that a larger
company might be trying to attract.
Just as Amazon leveraged their infrastructure that they were building to support their e-commerce
business, Cloudflare can leverage the DDoS absorption network that they've built out,
the boxes they have at ISPs and all the network links that they've already paid for.
Since their launch in 2010, Prince has said that their goal is to become the number four
cloud provider.
No matter how you slice it though, the cloud market has massive ramifications for Linux
and Linux adoption.
We'll keep an eye on everything and all the stories in the world of Linux and open source
so be sure you go to linuxactionnews.com slash subscribe for all the ways to get new episodes.
And linuxactionnews.com slash contact for ways to keep in touch.
And be sure to check out Linux Unplugged 425, we cover our recent server crash and the big
upgrades we have planned.
As for us, we'll be back next Monday with our weekly take on the latest Linux and open
source news.
Thanks for joining us and that's all the news for this week.
