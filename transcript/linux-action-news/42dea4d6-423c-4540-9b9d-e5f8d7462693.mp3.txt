Hello, and welcome to Linux Action News, episode 147, recorded on March 1st, 2020.
I'm Chris.
And I'm Joe.
Hello, Joe.
Good to be connected with you.
We start with Collabra, who's brought smooth editing to Android and iOS this week.
Yes, finally, a full feature office suite on Android and iOS that is open source.
Yeah, that was the part I was going to underscore, is that's what's so great about this.
The Android and iOS apps are fully open source, and you get full editing features, rich copy
and paste support.
And I decided to load it up on the old iPhone to give it a go.
And to their credit, it immediately populated with recent documents I've edited on this
iPhone that I've saved to the local files, file stores, whatever they call it on iOS.
And it reads them all, I can open them right up, it recognizes this is a markdown document,
I can go in here and edit.
I don't love the way it displays on the phone screen, but if I was on a tablet, this would
be fully usable.
Yeah, I tried it out on Android, and I didn't have any local files for it to open.
So I just was starting from scratch.
And it's okay, it works reasonably well at this stage.
It's nowhere near as good as Google Docs.
So there's a way to go here.
But this is a very important first step.
Because although we've had Collabra as an app before on Android, it was pretty much
read only.
The editing features were very much experimental and just didn't work very well.
Whereas now, it does work.
It's a little bit clunky.
There's a lot of room for improvement, but I'm very happy to see it.
Yeah, clearly competing against Google Docs or Apple's pages is going to be a challenge.
It's an uphill battle to take on the very own platform creators apps.
It's just always a rough fight.
But in the open source ecosystem, this is playing an important role.
But additionally, you can see the network effect and value that Collabra Office users
on the desktop would have with a mobile app now.
And so they'll likely be the top users of this.
It might not skyrocket to the charts on Play or on Apple Store, but it will be very useful
for the established Collabra Office user base.
Yeah, and they're not trying to make this some open source island.
It will work with things like Nextcloud, but it will also work with Dropbox and proprietary
services.
So they are making it easy to switch over.
And I think the most important thing here is that they're rolling this into their paid
for product, Collabra Online, as a very important part of that product.
And they have a clear business model here, which will hopefully lead to this being successful
long term.
As you can probably imagine, this was no small task.
We'll have a link in the show notes at linuxactionnews.com slash 147 to Michael Meeks' blog where he writes
about the process of taking this desktop application, prototyping it for mobile, and then eventually
shipping it as a mobile application.
It's full of really cool geeky details.
Yeah, and the very first screenshot in it shows you how long this has been going because
it looks quite old school.
Those of us in the States have a new default rolling out to Firefox.
Yes, DNS over HTTPS.
We've talked quite a lot about this, but now it is default in the United States.
It's not going to be default anywhere else for the foreseeable future.
But now if you're a Firefox user and you don't change any of your settings, all your DNS requests
are going to Cloudflare.
I've been using this since we first started talking about it just so I could get some
real hands on experience.
And I have not noticed any difference except the one time we rebooted the pie hole here
at the studio and DNS was down for all of the machines, it was still working in Firefox.
And I thought, oh yeah, right.
But I was thinking maybe I'd see a performance improvement or something.
I don't notice any difference.
So they write on the Mozilla blog, which we'll link to again, DNS is a database that links
a human friendly name such as www.mozilla.org to a computer friendly series of numbers
called IP addresses.
By performing a lookup in this database, your web browser is able to find websites on your
behalf because of how DNS was originally designed decades ago.
Browsers doing DNS lookups for websites, even encrypted HTTPS sites had to perform these
lookups without encryption.
At the creation of the internet, these kinds of threats to people's privacy and securities
were known, but not yet exploited.
Today we know that unencrypted DNS is not only vulnerable to spying, but is being exploited.
And so we are helping the internet to make a shift to a more secure alternative.
We do this by performing DNS lookups in an encrypted HTTPS connection.
So I read two things from this, Joe.
Number one, they're kind of making the case of maybe we should have always done it this
way.
And number two, I still feel like the Mozilla organization as a whole is trying to signal
to us that they know about something going on with DNS, but they don't want to publicly
say what it is because they kind of sneak these lines in here.
Like DNS is not only vulnerable to spying, but is being exploited.
They sneak these lines into a lot of these posts, like they know about something that
they're not publicly sharing.
Are you talking about the Snowden revelations?
Maybe or maybe through Firefox metrics or conversations with ISPs, they've learned of
a lot of DNS tomfoolery going on out there for advertising purposes, spying purposes.
I honestly get the sense this is almost a move to protect against government he's dropping.
I don't know if that's going way out there with the old conspiracy bacon, but it almost
feels like that's what this actually is.
When you change a default like this for this many users at this aggressive of a pace, but
you also are constantly making your case, I start to think maybe there's a bigger picture
here that Mozilla is trying to very cleverly communicate to us.
Much like this is an odd comparison, but much like how Canonical is trying to say, hey,
these 32-bit libraries and software are not being properly tested and the backports may
not even be working properly.
We can't guarantee this is secure.
They can't say that, but they try to say it in their own language in the post when they're
talking about deprecating 32-bit support.
They try to communicate it without sounding alarms.
It feels like that's what Mozilla is doing here.
But this is not without its controversy because what they're doing is centralizing DNS.
They argue, and I don't buy the argument, that DNS is centralized anyway because in the US
you have so few ISPs, but at least you have four or five, whereas this by default is pushing
everything through Cloudflare, a company which they say they trust and they have a legal
agreement in place saying that Cloudflare are not going to sell this data or do anything
nefarious with it.
But detractors are saying this is centralizing something which is not centralized at the
moment.
Yeah, and that is, I think, a valid argument.
Of course, the counter argument could be, well, it's software.
They could add 100 DOE servers and then they could have Firefox randomly pick them every
time it does a query.
That could be done.
But Burt Herbert, the founder of PowerDNS, he's one of the vocal opponents of Mozilla
turning this on.
He told the Register in an interview, quote, I find it highly disappointing that Mozilla
has decided on behalf of all users, it deems American, that this was a good idea.
So while encrypted DNS is great, it matters a great deal who you encrypt your DNS to.
Mozilla has dark patterned the choice, so almost everyone will take the new default.
Essentially, they're saying, we decided it's best that you send all your DNS queries to
Cloudflare.
He goes on to add, it's far easier to defend DOE the protocol than to defend DOE the land
grab.
Which is a pretty valid point, isn't it?
I don't think anybody disagrees with it in principle as a technology.
It's just the implementation.
And yes, they have added Next DNS as an alternative, but defaults are king and people will stick
to them.
And so the vast majority of Firefox users in the US will now be using Cloudflare.
And I can see why that makes people uncomfortable.
But that said, anyone who really cares about it, and if you are the admin for an organization,
it's not impossible to completely disable this, or to switch to your own provider, there's
nothing stopping you rolling your own.
So I can understand why Mozilla have done this with Firefox, but I also understand why
some people are annoyed about it.
I think it's quite telling that whilst I have experimented with this, I checked to see if
I'd left it on and I hadn't yesterday.
And I don't think that I will be enabling it.
I don't know why that is.
I just have this strange feeling about Cloudflare.
And I have no evidence for it, but I just feel it's telling that I'm not using this.
And if I was there, I would probably turn it off.
So I think that's fundamentally what it comes down to, is who do you trust less?
And that's why this is such a hard conversation, because my ISP, my upstream DNS is Comcast.
I may trust Cloudflare more than Comcast, at least in 2020.
In 2025, I may feel differently.
But I guess because I put faith in Mozilla's ability to lock them down on this agreement,
which we have talked about in the past, we have reviewed, and does look very legitimate.
And both organizations publicly claim that they have agreed to this.
As long as that remains in place, I trust Cloudflare more than I trust Comcast.
I do kind of see this, I guess I could buy, I should put it, that this is a land grab
by Cloudflare.
But because it took us a total of 45 seconds for us to set up our own Doze server, and
then plug it into Firefox, it's not that big of a deal.
They would just take something like PFSense, and PyHole, and OpenSense, and Linksys routers
to build in Doze servers that would be handed out to clients over DHCP, and this centralization
problem starts to go away.
Yeah, so I suppose the argument is they needed a centralized way to push it out on a big
scale.
But this will hopefully drive people to accept this as the way forward.
And hopefully, as you say, start building it into routers and make it a universal thing
that isn't centralized.
It's one of these technology problems that is easily solvable, but not until you get
people actually using the protocol.
And I think your point there about scale is the one that we should probably end this on.
Fundamentally, who else could handle that scale of users?
There's got to be only a few companies in the world, and Cloudflare is built to be one
of them.
Yeah.
I think you said something.
When you said, it took us 45 seconds to do it, you mean it took Wes 45 seconds.
Yeah, it totally did, because I think that container took 20 seconds to download to begin
with.
But a bit of Firefox news that got lost in all this this week is that they have rolled
out for Mac and Linux users a new sandbox system.
Get out your open source bingo card and cross off Rust, because we've got another Rust solution
for you.
I saw on the post here that they continue to make extensive usable sandboxing and Rust
in Firefox, but each has its limitations.
You have process level, and that works well for sandboxing large, pre-existing components.
But it consumes a lot of resources.
You see this in Chrome, and you kind of see a more sparing approach in Firefox today.
Now Rust is lightweight, but rewriting millions of lines of code in C++, it would take years
and set Firefox back competitively.
They make an example here on their website about the graphite font shaping library.
Now this guy's tricky, because Firefox uses that to render certain complex fonts.
It's too small, though, to put in its own process with all that overhead.
And yet it can be a bit of a memory hazard.
It sort of gets around the process isolation as it is right now.
So that's why they've launched Joe RL Box, which is WebAssembly meets Rust to break off
like individual library type components to isolate those little bits and pieces that
could be exploited even with process isolation.
Yeah, and as you say, this is initially just being used for the graphite font library,
but it is generic enough that it can be used for other stuff within Firefox and potentially
in other applications as well.
And obviously it's all open source.
Yeah, powered by WebAssembly and Rust.
So you know this thing's going to get a lot of attention.
And the beautiful thing is, is it lands on Linux first in Firefox 74.
We get it on Linux and Mac users get it in 75 with Windows support coming sometime soon
in the future.
They're all using Edge anyway, so, you know, all right, well, while we're talking about
the web, Tim Berners-Lee has got this crazy new idea that most of us didn't think was
going anywhere.
But this week, he got Bruce Schneier on board.
Well, that's kind of the tricky thing.
I think Schneier has been working with Inrupt for a little while, but this week was his
public outing.
Inrupt, he writes, think of that as basically the red hat of Solid.
So Solid is the base, and then they are going to package it up and implement it.
Solid was that idea that was Web 3.0 or whatever it would be, like it'd be a replacement to
the web we have today, where your data lives in a pod that's totally controlled by you.
And that could be a pod that's hosted on a cloud pod hosting provider, or it's running
on your LAN, Schneier even daydreams that potentially one day routers would come with
little pod receivers on them.
But the idea is you grant granular access to the data in your pod to whoever you want.
So instead of it being in a bazillion different databases all over the web, just totally disperse
everywhere where you have different security issues and whatnot, you control it all.
So if your insurance company wants to have access to your fitness data, you grant it
through your pod.
And if you don't want them to have access, you don't grant it.
Similarly, like you want to share vacation photos with friends, it'll all be pod access.
And then you'd need something that sits on top of this and kind of makes it all connected
together.
The idea is to be completely distributed if possible.
And I guess this in rep company is bringing together a bunch of big names to try to really
lend some credibility to this whole solid idea.
Yeah, Bruce Schneier joined as chief of security architecture, but there's some other people
who are involved with this now who've got a lot of experience in the industry.
But you know, the more I read about this, the more I feel that I don't get it.
I just feel a bit dumb, maybe it just it feels like this great idea that is just beyond me.
Whether they're not explaining it properly, or I'm dumb, at least that was what I was
feeling until I read a post on the Greater Manchester Combined Authority site.
This is a huge metropolitan area in the north of England, where they talk about using this
technology to safeguard the data of young children of around two and a half years.
And suddenly all kind of clicked into place for me.
That is probably one of the ideal examples.
So good on them.
My issue here is if this were to be successful, it would, it would require a lot of big corporations
getting on board and acting in a way that would reduce value.
For example, the data that Facebook owns, or the data that Google owns is truly their
primary value.
They build everything around that.
Why would they not want to keep that?
Even Schneier writes on his blog, quote, I believe this will fundamentally alter the
balance of power in the world where everything is a computer and everything is producing
data about you.
What we're talking about here, and I'd love to see this happen.
I want to make this clear.
I mean, I would be, I would be happy to see this happen, totally 100%, but we're talking
about a fundamental shift where things go away from being controlled by corporations
and back to being controlled by users.
And it just doesn't seem like that's the general trend line of the internet these days.
Well, Thomas Carburn writing on the register nailed it as far as I'm concerned.
He wrote, all it will take is to win over the businesses that profit from the current
status quo and the internet users who have consistently traded privacy and control for
free services and delegation of responsibility.
I mean, the register is always quite snarky.
And so it kind of goes with my sense of humor, but that really just sums it all up.
Yeah, that's all it'll take.
That's all.
Well, something else that didn't get a huge amount of coverage and I think should have
got more is that Azure Sphere has hit general availability now.
It's been a little while.
Microsoft initially introduced Azure Sphere, which is that whole Microsoft services paired
with Linux running on a system on a chip with different OSes and security models.
They did that back in 2018.
The technology evolved out of a research project that was started back in 2017.
You remember, it's essentially a three component system.
It's got a processor storage and memory, and that connects back to an Azure service that
guarantees 10 years of support and metrics for failure analysis and log retrieval and
ideally a service package that makes it possible to keep IoT devices out in the marketplace
and out in the field, secure, up to date, and even potentially detect possible security
issues or failures.
I think why this is really important is that this is a Linux first product from Microsoft.
Can you even have imagined that five years ago?
Right.
And the other thing that's kind of fascinating about it, it's their Linux.
It's a Microsoft distro running on this thing.
I kind of would love to see that.
Does it count as a distro if it's only on their microcontroller?
I don't know.
Did we ever consider Linux running on the TiVo A distro?
I'd say no, but it's something.
I'd still like to see it.
Microsoft's distinguished engineer and managing director of Azure Sphere, that's actually
the title, said, quote, IoT is in the science fair stage.
Every enterprise is doing at least one experiment here, but security is really keeping them
from going to scale.
And that's what Microsoft's trying to solve.
The adults have come to the room and they're now going to sell the components for you to
build a device, either be a washing machine, which is what some of their preview customers
built were clothes washing machines that just keep track of health and power usage and all
of that that some reason need Wi-Fi.
You want something like that to be supported for at least 10 years, and you want it to
be solid.
You don't want to be the brand that brought some malware into your home because you bought
a Wi-Fi connected washing machine.
Not good for the brand.
So Microsoft's customers are those people.
Mary Jo Foley has a really good write up and she covers a bunch of the things that have
happened, including they've added support for the IMX8 ARM CPU, which is fascinating
because they think it's well suited to artificial intelligence, graphics, and richer UI experiences.
Hello Librem 5.
I thought that was interesting.
So you could get the same CPU and an Azure device running Linux.
Yeah, I hope it's got a good heat sink.
And probably a pretty good power source, I would hope.
Yeah, but the IMX8 is designed for this kind of installation, something that's mains powered,
and it's got great Linux support, so it's no surprise that Microsoft have added support
for it.
Well, speaking of small device surprises, there was a birthday surprise for Raspberry
Pi users.
Yeah, to celebrate the eighth birthday, they've lowered the price of the two gigabyte version
of the Raspberry Pi 4 to only $35.
They will keep the one gigabyte version around for industrial customers, but that's going
to stay at $35, so it makes no sense for a consumer to buy the one gigabyte version and
the four gigabyte one is going to stay at $55 as well.
So really what's happening here is the default $35 standard priced Pi is now really fast
and with two gigabytes of RAM.
Yeah, it's not bad.
And if you consider inflation, when the Raspberry Pi originally launched, that $35 is nearly
$40 today.
So you're effectively getting all the improvements for a $5 price cut, as they point out on the
post.
So the one gigabyte model sticking around for industrial and commercial customers, as
they put it, and staying at the $35 price is kind of a surprising move.
I'm curious to know why a two gig model wouldn't work where a one gig model works today.
There must be applications where too much memory is bad.
I mean, it totally seems possible.
I can only think that possibly the extra power draw of two gigabytes, it's only a tiny bit,
but if you have done a lot of calculations based on exact numbers, then maybe you might
want to stick to the one gigabyte one.
And if you've certified it, maybe you'd have to go through a whole new process.
So maybe it's that.
That's what I was thinking is maybe it's just how this business works.
You got a product, you certify it and you know, you build it to spec and if you deviate
from that spec, it has to be recertified.
They write though that we expect most users to opt for the larger memory variants since
there's no price advantage.
So we'll see how long it sticks around.
But I think it opens up a real nice spot in the lineup for an eight gigabyte Pi.
I know that's what I was thinking.
If only they'd drop that four gigabyte price slightly and then bring in the eight one,
but it doesn't look like they're going to do that anytime soon.
I'd pay 80 bucks for an eight gig Pi, I think.
For me, it'd be a slam dunk upgrade.
I just pop out the SD cards and pop them in the new one because my installs are already
64 bit.
Yeah, that's the beauty of the Pi's.
You could take that same SD card and put it in my original one that has 256 megabytes
of RAM.
It reminds me honestly of the good old days where I would have a boot disk, a boot floppy
disk to get my OS up and then I would have to switch over to the game and I would have
to eject the OS disk and then put in a floppy disk for the game until I became a real baller
and I got two floppy disk drives and then I could have an OS drive and a game drive
and don't even get me started on how fast I can install an operating system.
Yeah, and it meant you could copy those Linux installation disks really easily as well.
Don't copy that floppy Joe, unless it's open source.
But one quick thing to mention is on the FAQs, the question is, is this a permanent price
cut?
And they say yes.
If a lot of this is to do with the fluctuations in RAM pricing, it's not going to go back
up again.
I love it.
Yeah.
They're passing the savings of the drop in price to us and they must look at it from
a business standpoint and figure, okay, this is going to stay this way for a while.
We can make this price change.
They're really conscious about that $35 price point.
Yeah, massively.
So that's why they were reluctant to change their system on a chip and make it possible
to do the four gigabyte one.
That looks like they've finally found a way to have two gigabytes of RAM for $35.
Imagine that five years ago.
I know and imagine this in another five years.
Yeah, very exciting.
Well, that just about wraps us up.
So be sure you go to linuxactionnews.com slash subscribe for all the ways to get new episodes
and linuxactionnews.com slash contact for ways to get in touch with us.
We'll be back next Monday with our weekly take on the latest Linux and open source news.
I'm at Chris LAS.
I'm at Joe Rissington.
Thanks for joining us and we will see you next week.
See you later.
