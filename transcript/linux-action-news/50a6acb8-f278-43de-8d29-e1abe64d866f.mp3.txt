Hello, and welcome to Linux Action News, episode 116, recorded on July 28th, 2019.
I'm Chris.
And I'm Joe.
Hello, Joe.
Good to be connected with you.
We start this week with the introduction of Fedora Core OS.
The core is back, and this time it's better than ever, Joe.
Yeah, this is the first preview release, and it's very much a successor to Fedora Atomic
Host and also Core OS, which Red Hat bought last year.
And we've been wondering where Core OS was going to fit in with CentOS and Fedora, and
now we know.
Fedora Core OS combines the provisioning tools, the automatic update model, and the philosophy
of containers with packaging technology, with SE Linux baked into all of it.
That's essentially the Fedora Core OS proposition.
The idea is the isolation that's provided by those containers means that the host OS
can be really tiny and potentially, like Fedora Core OS, automatically updated without regular
admin interaction.
So the idea being that you don't actually run anything on the host system, you run everything
in containers.
Which is becoming more and more of a common practice as it is.
It's essentially for 99% of everything we now host here at the studio, it's all in containers
with NetData being the only application we run on the physical host.
And in the case of Core OS, even the installation process of the OS is pretty different.
It provisions itself with a tool called Ignition.
It runs containers with Red Hat's Podman and Mobi and updates itself automatically with
RPM OS tree.
Core OS is really the stab at an immutable operating system.
Immutable infrastructure is this idea that you could blow away stuff and rebuild without
any real losses.
And containers get you close, just like VMs do, that both are really good solutions.
This is for folks that are focusing more on the container side of those solutions.
So when it first boots up, it uses Ignition to provision the system, and then it reads
a config file from a remote URL.
It uses the information in that config file to create the disk partitions, the file systems,
the users, creates a system to units, and then you have a box that you SSH into and
start standing up containers and running applications.
And then the host operating system just sort of runs itself, self-updates.
You can roll back if there's an issue.
It's a really solid proposition for workloads that work well in containers.
When you need a solid base OS that is secure and maintained to run those, it's a really
solid product.
And I'm really glad to see we're kind of getting the results of that acquisition of CoreOS.
I don't think this is really targeted for anyone on the desktop yet.
That's going to be more of Silverblue's role.
But on the server, I could see within a year or two this being something that we migrate
our Fedora servers over to.
I don't know.
It seems to me that it's designed to run more at scale rather than on individual machines.
It's supposed to be sort of managed by Kubernetes or Ansible or something, rather than something
that you're going to install on one physical box in your garage like you.
I would agree, with the exception that managing systems with maybe not just one system, but
if you have more than one system, managing them with something like Ansible is becoming
the common practice.
And that's how Red Hat's really directing a lot of future trainees is to manage their
systems with Ansible and deploy with these tools.
So it's becoming more common to use those tools even for really small infrastructure.
And in our case, we've got a Fedora server here in the studio that does quite a bit of
work for us.
And then we've got a couple of Fedora servers up in the cloud.
And you could use one tool to manage all three of those systems.
And Fedora Core OS would be a perfectly good solution for what we do, with the exception
of NetData right now.
But that's a solvable problem.
So they're going to have three editions of this, testing, next, and stable.
And they've only got testing available at the moment.
And they don't recommend it for production.
The idea is that you'd have a few instances of testing so that you can then find the bugs
and report them.
But the majority of your workloads would be running on the stable edition.
And something jumped out at me.
They said that Fedora Atomic Host will be maintained until the end of life of Fedora
29, which is expected in late November.
But they also say that they expect the preview period to continue for about six months for
Fedora Core OS.
So there's not going to be much time to migrate your production systems over if you're running
Atomic Host.
However, assuming you've set up things correctly, moving containers to a new system shouldn't
be that risky because the state should be saved outside the container and the configuration
as well.
And then you just have the application and its resources inside the container.
You move them, reconnect them with the configuration of state, and you should be good in theory.
Of course, I get to say that.
I'm not the one that have to do it.
But yeah, that is really tight.
And it seems right now it's not just Atomic Host, but also Core OS container Linux users
also have to migrate.
Six months after Fedora Core OS is declared stable, which who knows when that is.
But they also will be facing that same transition.
Yeah, but they'll have six months of stable to play with, whereas Fedora Core OS users
won't.
But then I suppose Fedora generally has always been aimed at more agile shops.
If you've got real infrastructure at scale, the chances are you'll be using Red Hat rather
than Fedora.
Yeah.
It seems like agile shops like ours where we can just say, well, let's go fix it, please.
It makes more sense to use Fedora.
So it's probably not going to be a huge problem for that many people.
But I know it might trip some people up.
You know, I'll tell you, Joe, there was a time in my career where I really took every
opportunity I could to migrate every RHEL and CentOS server to Debian or Ubuntu.
If the client was even passively interested, I would seize that opportunity because I hated
working on RPM based distributions as servers.
Drove me crazy, especially ones that got to be about three or four years old, or the ones
that were so old, they were no longer under support at all, which is a really long time
for RHEL systems.
I hated it.
And now I'm the guy on the team that's saying, let's put Fedora on everything.
And that makes the sysadmin in me just go crazy thinking about it because Fedora is
a short term distribution, et cetera, et cetera, et cetera, moves too fast, new, blah, blah,
blah.
I don't like new.
But the reality is it's become a very viable platform.
And that group has worked on upgrades and they have worked on making the packaging system
fast and tidy and safe.
It is a stellar distribution now amongst many great distributions.
If they could ease the pain of automatic updates so that we are always rolling to the next
version of Fedora, that could be brilliant.
And if down the road you could pick the pace, it's what you want.
Maybe you wanted something that was more stable.
Maybe you wanted something that's a little more cutting edge.
That could have a lot of potential.
But now imagine what this will be like when it's manifested as RHEL.
Because if it's Fedora now, it's likely upstream to a RHEL project in the future.
We could end up having a version of RHEL where you can turn the knob and have a really hyper
current almost Fedora style RHEL, but it's still genuine RHEL.
Or you turn that knob down and it's more like a traditional enterprise Linux.
And that could be a really nice improvement for RHEL.
It really would ease those old pain points I used to run into in systems that were three
or four years old and people are trying to deploy modern web applications.
Yeah, and other applications as well.
You can really see Red Hat's kind of long term play here, can't you?
Becoming more modern and keeping up with the market.
There's a reason why they're the market leader, because they are forward thinking like this.
And they're testing this all out now with Fedora.
And yeah, it's almost certainly the best of it is going to go into RHEL at some point.
It might be a year or two, and they'll be in a very good market position there once
they do it.
But even the whole IBM Red Hat acquisition and their focus on the hybrid cloud right
now, I could also see a scenario where this is realized as a RHEL product, but not as
a traditional RHEL Linux release, but something that's perhaps folded into OpenShift or something
that they expand upon with the universal base image.
Fedora Core OS to me feels like a very important chess piece in a wider strategy that Red Hat
is playing towards with their quote unquote hybrid cloud strategy that involves OpenShift
containers and infrastructure management deployment across on premises and public clouds.
And they are now really focusing on multiple public clouds.
Core OS will likely be that very important software glue that ties everything together
along with OpenShift pulling all of the strings.
Yeah, I can't disagree with that, but I can disagree with some of the reporting of a VLC
story this week.
I'm really glad we took the time to look into this one and didn't run with your VLC is busted.
Your VLC is busted, but widespread reports of a quote critical security issue that supposedly
impacted users of VLC media player was debunked this week by the VLC developers.
It all started when a German computer engineer response team, part of the Federal Office
for Information Security, pushed out an advisory warning network administrators and other users
of a high impact vulnerability in VLC.
It seems the advisory can be tracked back to a ticket that was opened on VLC in their
public tracker about four weeks ago.
It was allegedly a heat based buffer overflow flaw that the user discovered and then reported
not through the traditional channels.
And then these folks picked up and sounded the alarm.
And then Videoland, the makers of VLC, hit back saying, well, hang on, no, we patched
this over a year ago.
And there's no problem.
This is someone who's obviously running an old version or something.
But it's actually a little bit more nuanced than that.
Right.
The issue really exists in one of the many under lying libraries that VLC depends on.
In this case, it was lib ebml, which is a library to parse ebml files, which kind of
comes into play with MKV files often.
The flaw existed there in the distribution the user was on.
So thus it impacted VLC, but it wasn't actually VLC's fault.
And it turns out that I guess upstream developers didn't really know that there was a security
issue in this library.
I mean, they knew there was a bug, but it wasn't clear that it was a security issue.
Yeah, because the version number got ramped by the upstream.
But Ubuntu didn't patch it in 1804 because it was considered a new version and there
was no security advisory.
And so it sat there unpatched with this potential security vulnerability.
And so it was only when it came to this week that Ubuntu patched it, added a dash two to
the version number, and the problem is solved.
And so really it comes down to the maintainer of that library not flagging up that there
was a security issue there.
Right.
And VLC is a tricky one.
It touches a lot of different things on the web.
It has a lot of power, I guess you could say, and it does depend on a lot of libraries.
In this case, though, it wasn't really VLC's fault.
In fact, Videoland president John Baptiste was having a bit of a riff on this issue.
This is kind of like the funny side tangent story here.
I like this poll quote right here.
If you report a security issue, at least update your Linux distribution.
That was good.
That was good.
And also they were quite annoyed that they submitted to the public list and not the actual
private security list.
Yeah, that wasn't done properly.
But to be fair, you could have been on a fully up to date 1804 and had this issue when it
was reported.
Yeah.
Because Ubuntu didn't know about it.
It does kind of show you how tricky it is to do legitimately good security reporting
in an open source ecosystem with lots of libraries like this.
You need to be really careful where you're assigning blame and all of your due diligence.
You have to make sure you're up to date.
You have to make sure your distribution has properly packaged those things.
Then once you've got all of those things checked off, you can then move upstream to the project.
And then you need to figure out how to work with that individual project.
Because if you do it in the wrong way, you're going to aggravate them.
And because this was on a public list, an entirely different entity, one that is well
respected in the space, was able to take this and run with it.
And it really caused quite a bit of stink for VLC and some really aggressive headlines
were out there, like bad ones, VLC plagued by unpatched critical remote execution flaw.
And that's bad PR for a project like VLC that has a lot of competition in this space now.
I can understand why they got frustrated and annoyed.
What's funny is that I actually switched back to VLC this week because GNOME MPV was starting
to drop frames later on in videos.
And I thought there was something wrong with the hardware at first.
I was like, let me just try VLC and it works fine.
So I think I'm converted back.
But there's a video of a talk that I watched recently from Jean-Baptiste.
And he really goes on about the nightmare that it is to be VLC, having so many people
reporting bugs to them.
Things as stupid as your source code is available, as if people think that's a bug and it just
it does sound like a nightmare to run a project of that size.
And he talks about some of the numbers, I just could not believe that the number of
people downloading it, I suppose mostly for Windows.
But it is still a very, very popular, I mean, is the de facto for a lot of people.
I know we've moved over to GNOME MPV, a lot of us in the Linux world, but yeah, VLC is
still massive.
I've been shopping around a little bit, trying out different media players like Parole.
I like Dragon Player on Plasma, but VLC is a classic go to.
And at this point, I know it so well, like I know all the different config options.
I just I love it.
It's such a great project.
MPV is also really fantastic if you are looking for an alternative.
When Certbun, the people that published the report from the public posting, I was contacted
asking for some clarification.
This is what they sent back, quote, as far as we know, initial attribution to us was
by Softpedia, who wrongly referenced the public post.
So they're kind of throwing Softpedia under the bus and not taking responsibility for
double checking themselves in that statement.
But I'll pass it along.
I thought that was interesting.
Well, another security issue that came up this week is that Android phones might well
be vulnerable to something called spear phone.
This might be my favorite vulnerability in the last couple of years.
So we'll link to a post by Terra Seals on threat post that goes into more details.
But yeah, it's called spear phone.
Researchers discovered that essentially any audio content that comes through the speakers
when used in speakerphone mode.
So say you're playing a podcast or you're on a speakerphone or you're talking to an
assistant, that audio can be picked up by the accelerometers in the phone as a sound
wave.
It picks up on the reverberations and is able to reproduce sound waves.
And because the accelerometers are always on and don't require any special permissions
in Android to access, any app can read them.
Even web pages can listen to the vibrations going into your phone.
And there's a surprising amount of data.
They can get not only can they put words back together in entire sentences, but they can
even in some cases detect the gender of the speaker.
And it appears to impact multiple Android devices.
Yeah, they tested some LG and Samsung phones, but there's not really any reason why this
wouldn't affect any Android phone unless the accelerometers and speaker properly separated.
But these days, phones are so small and so tightly packed in that seems a virtually impossible
task to me.
This is really something.
It's the vibrations of the phone's body housing being picked up by these sensors, which are
incredibly sensitive.
It's actually a great form of recording because you're not actually capturing live waveform
data, but you're just capturing accelerometer data and then recomposing it.
So it's actually very transmittable, very portable data.
Well, good luck trying to do this to me because even trying to turn my screen to landscape
mode, I have to really shake it sometimes and then to get it back, I have to bang it
on the table and stuff.
So yeah, I don't think mine's too accurate.
I wonder if maybe you got some dust in there.
I was thinking too, how are you going to mitigate this?
The researchers say the Android platform could implement some stricter access control policies
to access the sensors.
That would definitely help, but probably bypassable.
And then the other solution they had was the internal build of the smartphone could be
such that the motion sensors are better isolated from all of the vibrations.
But then the question is, is that even possible with, like you said, how thin and small phones
are now?
And the other question is, would that make them less capable, less sensitive?
They'd be more like your phone where they're really hard to detect movement.
It seems like a lose lose here.
This is a bad one.
And also at the same time, great.
I love it because it's so low tech and so high tech and potentially really hard to solve.
Yeah.
I think there will be a solution to it, but as usual, that's going to be in a future version
of Android probably.
And so most people will never get it.
Motion dampening.
You know what they ought to call it?
They ought to call it the internal dampening field.
Just for me, I would really appreciate that.
Samsung and LG did not immediately respond to a request for comment on the findings.
I would imagine my assumption is they're sitting there going, holy crap, we didn't even think
about that.
Somebody go test that.
It seems so obvious in retrospect, but it's just one of those things.
It's like I heard that using a laser on someone's laptop screen, even through a window of a
hotel or whatever, you can tell what they're typing because each key kind of vibrates the
screen that tiny bit differently and stuff.
No way.
Oh yeah.
That was years ago that I heard that.
Wow.
That is really something.
Well, I guess the other alternative could be switch to Sailfish.
Yeah.
And they had a big release this week, version 3.1.
And it's the first release for about a year.
And there's nothing hugely exciting about it, but it's kind of a spit and polish release.
It shows to me that Sailfish is really starting to mature at this point.
It looks really good.
It looks good enough that if I could get my hands on an Xperia XA2 for a decent price
right now, I found it for about $130 on Amazon, I'd be pretty tempted to pick it up so I could
have a flagship Sailfish OS device.
They've redesigned a lot of their core apps, their people app, phone messages, the clock
app, email calendar.
They now implemented dual SIM card information so you can actually manage dual SIMs in one
of these devices.
They've rolled out file system encryption, which seems huge.
They've implemented a sane and functional API for fingerprint reading.
This is like really taking it to the commercial operating systems now.
And you've got to wonder if maybe there's some motivation with all the recent Huawei
stuff going on that perhaps there's been a new found interest in getting Sailfish OS
up to snuff.
Well, yeah, that and Russia.
There's a lot of Russian money going into Sailfish OS now.
And that's because, well, in that country, there's tensions with the US and they don't
want to be completely dependent on US technology.
Like Chinese companies like Huawei found themselves suddenly cut off.
What strikes me about this story is if you go back in time, you would have seen them
most likely implement their own wacky, totally homebrew OS, maybe be a fork or a rip off
of some commercial operating system even, loaded up with backdoors and government tools
and then it would never go beyond their borders and it would be an irrelevant operating system.
You look back at history, that's been the case.
But something has shifted in the last few years.
And now it's like the free software versions are too good to be ignored.
Even if it's something they have to implement their own solutions on top of, the hardest
problems now are being solved by open source software.
And that's a big shift.
And it might mean that all of these weird geopolitical things that are going on right
now could leave us with an operating system that's better for all of us to use.
We'll see.
I'm really not going to switch to Sailfish anytime soon, but this update, 3.1, this really
looks like from a graphical perspective and from an app functionality perspective, they're
getting on par now with the commercial operating systems.
Their people's app, I feel like is better than anything that's on Android or iOS.
Well, don't forget that although it is based on Linux Sailfish OS, it does have some proprietary
elements so they could probably slip those backdoors in quite easily if they wanted to.
I'm a bit of a pragmatist with it when it comes to this kind of stuff.
I don't know if we're ever going to have a perfect solution for mobile.
Yeah.
And it does seem that of all of the alternatives to Android and iOS, Sailfish is by far the
most well-developed and mature at this stage.
Mozilla caught our attention again this week when they debuted their implementation of
the WebThings Gateway, which essentially is an open source router firmware.
Yeah.
This is based on OpenWRT and it's available on GitHub, but it is still an experimental
build at this stage.
Yeah.
Very much so.
It really is only supporting one mainstream router that's kind of hard to get your hands
on and there is a test build that's compatible with the Raspberry Pi 4, which will probably
be how I try this out.
So for those of you that don't recall, we've talked about it a little bit, but the WebThings
Gateway is like Joe said, an OpenWRT Linux based operating system that's aimed at embedded
devices and is intended to support off the shelf consumer routers and will act as a Wi-Fi
access point.
But there's another bit of this that is really cool.
Internet of Things devices are really great when you get to automation.
That's really where they get nice.
Turn lights on when you're not home, turn a water heater on or off.
There's all kinds of things that once you get into automation are really quite fantastic,
but you need a system to orchestrate all of that.
So maybe you have a temperature sensor and when the room gets to 75 degrees, you want
it to automatically turn on the fan.
Everything has to trigger those actions and that's to take in the sensor data and then
trigger an event based on that data.
WebThings will play that role.
It'll have sets of rules and outputs that you can use to manage devices that trigger
different actions or send emails and reports.
It'll be a central hub to manage all of this stuff that we really don't have right now.
Home assistants allow you to turn lights on and off and they have some rudimentary routine
support but this sensor driven data where event A triggers event B and then notifies
person A, that kind of thing is not really fully here yet without implementing something
more advanced.
This promises to not only be backed by Mozilla, but just run it on your router, which kind
of makes sense anyways for a lot of home users.
They don't have anything really server wise or infrastructure wise in their home.
They got a router and they got some devices.
This seems to me like a solid open source foundation on which people will then build
other ways of interacting with it and interacting with all of your devices.
It feels like the beginning of the dream of being able to use Internet of Things devices
and do home automation without relying on proprietary third party services and sending
all your data off to the cloud.
That's the idea.
They're working, Mozilla is working with the W3C to write this all up.
They're drafting these as standards for interaction and methods of communicating with devices
that don't require proprietary cloud services and don't require going off your land.
And I am just so all about that because there is real utility, especially for those that
have disabilities or have special needs, perhaps are motion limited.
There's a lot of advantages to some of these types of tools in those situations.
I have a family member in that situation and it's been a game changer for them.
So I would love to see the ability to do that stuff, but to keep it private.
Yeah.
And for all of the criticism that we throw at Mozilla from time to time, they are the
organization that I would trust most to do this because they've got the resources and
they've got the fundamental mission to actually do this properly.
And so I've got real high hopes for this.
Right.
And not to mention, they have relationships established with the standards bodies that
are necessary to make this an industry wide thing.
And they're producing real results that end users can start messing around with soon.
I can't wait to try down a Raspberry Pi 4.
I've got lots of old IoT devices here that I use for testing and security auditing and
for automation.
And it'd be a perfect, perfect thing to try with this.
So I'm very excited about it.
I'll be watching it because just like you, I think they're the right group to do it.
Well, something that I've been watching for a long time is RISC-V and we've seen various
advances there.
But Alibaba this week announced a 16 core RISC-V chip that could actually be pretty
powerful.
Yeah, they're claiming it runs at 2.5 gigahertz on a 12 nanometer processor node.
And they're claiming it's 40 percent more powerful than any RISC-V processor produced
to date.
They're going to offer it as its own system on a chip as well.
Get ready for this.
They plan to release the code as open source on GitHub, they claim around September of
this year.
Yeah, at least some of it will be open source.
So we'll have to see about that.
But what jumped out at me is that this is potentially the first RISC-V processor with
an out of order operation pipeline.
So it might even have a bit of the old Spectrum meltdown with it.
I mean, it is what they're claiming to be part of their secret sauce.
They say it enables up to eight instructions to be loaded into each cycle, which is massive.
Of course, they do have to do some caching to store that information between cores.
So it could be.
Would that be a feature, Joe?
Well, for certain workloads, Spectrum meltdown don't really affect you.
So I don't know what we'll have to see on that one.
But this does illustrate something about RISC-V that is an open instruction set architecture.
However, it is permissively licensed.
And so you can just take that and make your own proprietary thing, add your secret source
to it, and then never release it, or at least charge people to license it.
And so it's something to be wary of.
And I don't know, I'm not hopeful that we're all going to get the source code for this.
Yeah, I'm a little skeptical too.
We might get bits of it, which would be beneficial, but not perfect.
The other interesting element of this story besides that will we get all the code element
is how the Chinese media is covering it.
They're reporting this as a major step in enabling local companies to get easy access
to quote, locally developed processor architectures, end quote.
And they know that's especially important in light of restrictions that were put on
again, Huawei, and the uncertainty that's created for the market down the road.
There's a real market opportunity over in China for this type of processor.
Well, yeah, in a lot of places as well, as people realize that being completely dependent
on one country isn't a great idea, because that country's leadership can change.
And that can be unfavorable to you.
And it might change back again or whatever.
But I think this has been a real wake up call, this whole trade war thing.
And it's making people invest in stuff like RISC-V, which ultimately, hopefully will benefit
us all.
But at least it makes things a bit more interesting.
Well, here we are, not even really intentionally, but here's another story where geopolitics
are pushing forward an open solution for the market.
Open source specs and open source software are kind of a great enabler.
I guess Richard was right after all.
Well, I stopped Clocksprite twice a day, eh?
Oh, no.
Oh, no.
Anyways, it is a very fascinating story.
And maybe you'll finally get your RISC-based desktop sooner than later, thanks to Alibaba
Joe.
You never know.
Now check out linuxactionnews.com slash subscribe for all the ways to get new episodes.
It's a quick way to get the open source on Linux news in 30 minutes or so every single
week.
Linuxactionnews.com slash subscribe.
And you can go to linuxactionnews.com slash contact for ways to get in touch with us.
And did you know that Linux Academy is hiring?
Come work with us.
Go to linuxacademy.com slash careers and go check out all of the positions, many of which
are remote, full time, full benefits, some of which are based out of the office.
They have a big selection.
The company's growing fast and it's a great team.
Go check that at linuxacademy.com slash careers.
We'll be back next Monday with our weekly take on the latest Linux and open source news.
I'm at Chris LAS.
I'm at Troy Wessington.
Thank you for joining us and we will see you next week.
Until then, have a great day and I'll talk to you later.
