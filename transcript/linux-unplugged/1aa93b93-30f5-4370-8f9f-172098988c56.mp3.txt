Before we start talking about Linux today,
we have to talk about storage.
Brent found this story about optical media
making a strong comeback.
Researchers at the University of Southampton
are showcasing a new nano-structured glass disk
that has the ability to store data for billions of years
with a laser writer.
So it's like optical media is back, Wes.
Well, I mean, of course Brent would be promoting optical media.
Not only can you store it forever,
but you can put a lot on there, like 360 terabytes of data.
I mean, we all want more data all the time, right?
Yeah, and not only that, but how about them temperatures, too?
1,000 degrees Celsius, that's 1,800 degrees Fahrenheit.
It'll remain stable in up to 1,800 degrees Fahrenheit.
That's a good archival media right now.
13.8 billion years at room temp.
It's like the next evolution of microfilm, but way better.
Maybe they'll be able to fit a single 8K video on there.
I assume they're eventually going
to get to where the original Superman movie where
they have the crystals that they put in and have
all the data in the Fortress of Solitude.
I think they're eventually going to get there.
That's absolutely what these are,
isolinear chips, Superman crystals, they're glass data.
And they show up a bit, like in the show notes,
we'll have a link to a video they have in there.
And they're holding it up, it's translucent.
It just looks like a piece of glass
where they've etched a title onto it, but they didn't need to.
That's just for looks.
I'd like all of Star Trek just on one
of those sitting on a shelf on the wall.
I think you'd have no problem fitting all of Star
Trek on one of those.
No.
You know what, actually, kind of ironically,
in like 15 years from now, somebody
could listen back to this and be like, oh, that's so quaint.
It'll be like the Computer Chronicles is to us now.
No doubt.
This is Linux Unplugged, episode 307 for June 25, 2019.
Hello, friends, and welcome into your weekly Linux talk show.
My name is Chris.
My name is Wes.
Hello, Wes.
Looking very dapper today.
And are you ready to talk about Mixcloud?
Oh, I'm excited.
Yeah, we have an episode that we've been
prepping for in the most real sense.
You may recall we've recently reloaded our main server here
at the JB Studios.
And we have put it to good use.
Immediately.
Yeah, we will tell you about those adventures today.
But before we can get that far, we
got to do the one, the only introductions.
Time appropriate greetings, Mumble Room.
Hello.
Hello.
Hi there.
Hello.
Hello.
And of course, Alex, Drew, and Cheese are here as well.
Hello, hello.
Good day, everybody.
Hi there.
Hi there, everyone.
Well, we have a real panel of knowledge experts on this one.
We're going to need them today.
There's a lot going on.
You just heard from a whole bunch of Nextcloud users there.
This is really going to be, I don't know,
I feel like the one where I finally nailed it.
I am so excited to talk about this.
After many years of failed attempts
to move away from Dropbox to implement Nextcloud,
only to go back and have to eat crow right here on the air,
I feel like we have finally nailed it.
And I cannot wait to share our setup with you.
Dropbox, they finally just did it with this last update
where they want to take over everything
and they want to be like your online file system
and ship Electron as part of the Dropbox sync client.
I thought, OK, all right, maybe not.
Let's look somewhere else.
But before we get there, let's start
with some outrageously great community news today as well.
A round of applause to the Raspberry Pi folks
for tricking us all.
The rumors were we wouldn't see anything this cool
until like next year.
And so we kind of just thought, all right,
well, we got what we got.
And well, wait.
But yesterday, the Raspberry Pi 4 Model B was launched.
And it's actually a pretty decent upgrade
from the Pi 3, which they're saying this new device can now
provide desktop performance comparable to an entry level
x86 PC system.
Oh, that's exciting.
OK, but how does it perform as a server, right?
Could we replace this new box we've got?
Could we have just waited a little bit
and got this new Raspberry Pi?
No, it basically, while it's a huge upgrade,
it's still a Raspberry Pi at its heart.
Although, there's a lot to like, right?
So DDR2 RAM, that's been upgraded to DDR4.
And the new Cortex A72 CPU is anywhere
from double to quadruple the speed of the older A53.
Plus, there's gigabit ethernet that
isn't hamstrung by that darn USB 2.0 bus
that everyone's been ragging on for years.
You can actually saturate some traffic now.
Yeah, genuine gigabit ethernet is a big deal.
And keep in mind, all of that is still
at the starting price of $35, which is still
a pretty good deal.
One of the things they're doing differently now, though,
is they're including different SKUs, three different SKUs,
up to 4 gigabytes of RAM, which is the model I purchased.
Did you get the 4 gig?
I got the 4.
I mean, you're going to say no to that.
And it's only, I mean, what was it, like $55?
Not that much more.
Cheese, did you pull the trigger?
I haven't purchased one yet, but I plan on it.
Alex, you were probably too busy on vacay to pull the trigger.
I was.
I use a Pi to run my 3D printer.
I have a 3B plus right now.
I think a 4 will be in my future.
I'm going to see if this 8 gig rumor shakes out, though.
There is a rumor in the manual.
They talk about an 8 gig model, which
that might be what we see in, oh, boy, 2020.
Drew, are you a Raspberry Pi fan?
Yeah, I've got a few lying around the house,
but I think they're all 1s and 2s.
I didn't actually buy any 3s.
Thinking about the 4, but I don't know.
I mean, I don't have a project for it yet,
except that it's cool.
Yeah, I bought it because I'm going to try it as a desktop.
I'm going to test this claim.
You know, I'm going with reasonable expectations.
I'll tell you one thing it will be really great for.
It has x265 hardware decoding.
So we've all had h264 for years and years.
And now x265 offers significantly smaller files,
and this thing has hardware decoding.
So it will make an absolutely great media box.
They're also saying potentially 4K video out.
Early benchmarks, seems like some of the reviewers
have had issue getting that 4K video to actually work,
but jury's out in my opinion on that one.
I think something else they did with the announcement
is they had a little bit of a,
is it fair to call it a publicity stunt
or would it be a proof of concept?
Proof of concept, I don't know.
I think it's just a fun thing to do.
The launch site for the new model
is mostly running on a cluster of 18 Raspberry Pis themselves.
So why not self-host it with the product you're selling?
14 handle PHP code execution, two more serve static files,
and then two run our old friend memcached.
Of course.
Of course, that's all sitting behind Cloudflare.
As you do. Which is caching.
And also, I think the databases aren't on that cluster either.
But if you're serving cached content, it still worked.
It's legit.
I think the way to look at it is not really proving anything.
It's just saying, look, these are capable little devices.
Maybe don't use them for production,
but they're great for small little tasks.
Yeah, I see quite a few people on Home Lab
and all that kind of thing on Reddit using three, four,
five different Pis to run small Kubernetes clusters.
So there are definitely use cases for that kind of thing.
I'm kind of feeling that the Raspberry Pi, as time goes on,
is becoming sort of the universal Linux computer.
The Raspberry Pi 4B now kind of makes
it competitive with a lot of the other boxes
that are out there at a pretty competitive price point.
This isn't the fastest.
But Raspberry Pi is a brand.
It's an ecosystem.
It is a standard now.
It's integrated in production equipment.
And it's also in classrooms with students tinkering.
And it's at an accessible price that a massive range of people
can afford.
And the sort of pop a disk in, like a floppy disk,
and boot off of an operating system approach
where you can flash it from a Windows or a Mac
and then just pop it into a little Raspberry Pi
is approachable.
It's just so easy, and you get such leverage
for such a low entry fee.
And it brings something that we so rarely
have to the Linux desktop.
And that's why I'd really love to see as the Raspberry develops
when the 5 and the 6 come out, maybe one day
we'll even have a Raspberry Pi 10.
Could you only imagine what that thing's going to be for $35?
I just think it's something that Linux doesn't typically have,
and that is a single problem domain for the desktop.
It's the opposite of fragmentation.
It's one company, one product, and we're not locked into it.
That's what's so great about it.
It's available to us.
And because of its brand, and its ecosystem, and its user
base, it's becoming this common platform.
It's not the Apple style where only one company makes
all of the hardware, and only their operating system
runs on all of that hardware.
It's like it's Linux still available to everybody.
I really like where the Raspberry Pi is
going as a broader product.
When you look at it individually with the different boards
and stuff, you can argue about which one's technically
superior.
But when you zoom out and look at it
as a platform and an ecosystem, it's
creating something for Linux as it becomes powerful enough
for the desktop.
It's creating something we've never had, in my opinion.
Very excited about it.
So that's why I ordered what they call their desktop kit.
You get the Pi 4B with a keyboard and mouse,
and you get that micro HDMI adapter.
Wish they hadn't done micro HDMI.
That's the one thing I'm not really a big fan of.
And they've added two micro HDMI outs.
Yeah.
I would have loved some kind of a SATA interface
instead of a second HDMI port, but maybe that's just me.
You know, it's going to give it ethernet.
There's a lot of ways I can get storage onto that Pi now.
I think one of the original issues, too,
is that with the original CPU, you were tied down
to USB 2.0 throughput, so that's why you could never really get
gigabit speeds out of it.
And now that it's upgraded, with USB 3.0,
obviously, yeah, you can get that speed out of it,
no problem.
And I mean, there are tests that have been run out there
that actually show that they're able to get their full gigabit
speed out of it.
Are you happy with this?
You follow these types of boxes, or I guess boards,
pretty closely.
Are you happy with this release?
Yeah, I mean, I own several of these.
I own Raspberry Pis, Orange Pis, Nano Pis,
you name single board computer.
I probably have one of them from one company or the other.
I think it's a great step for the Raspberry Pi foundation.
This is obviously a response to the popular RK, the Rockchip
3399 that's in some of the other boards out there,
and set top boxes and stuff.
It seems to stack up pretty well.
I will say, though, that that cluster that they set up,
the photo that they tweeted out of that
just drove me crazy because of the cable management and the.
Don't look, man.
Don't look.
Yeah, just don't look.
It's a nightmare.
But no, I mean, I think this is going
to be a really good board, and it's going to be something,
like you said.
I mean, Raspberry Pi already has an ecosystem built around it.
There are so many companies that produce Raspberry Pi hats that
are continuing to do that.
And I think that this is just going to be a continually
growing project from them, where I think they focus more,
kind of take that Debian approach,
and focus more on reliable tried-and-true hardware
before they just step off into the next thing.
Like you have some of these other single-board computer
manufacturers, OrangePod to name one,
that just iterate so quickly on these boards.
And they're all over the place.
They've saturated the market with these different boards
and stuff.
Some of them are good.
Some of them aren't.
But it just kind of dilutes the whole thing.
So it's really nice to see Raspberry Pi just chugging
away, doing what it does, coming out
with what's sure to be a great board once it gets out
on the scene and everybody starts using it.
So I'm really looking forward to seeing what they do.
Man, I can't even imagine a Raspberry Pi 10,
what that's going to entail.
A couple of use cases that I haven't super seriously
considered the Raspberry Pi 3 or previous Raspberry PIs for.
Number one now is, to me, this seems like a clear contender
for a Codibox.
You've got real performance on this thing now, real ethernet,
real HDMI out, and it just seems like a clear contender.
So that's something I'm going to have to try.
And then also, you and I were just
talking about this a couple of days ago.
We kind of need a computer that is low noise, low heat,
doesn't have to be super powerful.
We just need to show graphs and different inputs
and different meters and just readouts of different data
that we have.
And a Raspberry Pi that can run two monitors,
that would be perfect.
That could be exactly what we need.
And it could take the job of a louder, more power hungry
x86 PC easily.
That's kind of cool.
You know, I just thought of another use case,
too, is considering that these are fanless,
this would not be a bad choice for something
to run Reaper on in a little mini recording booth.
Yeah, that's got to be tried, I think.
They do have an ARM version.
They do.
And also, our mixer makes the mixer's GUI software
for the Linux ARM as well.
Yeah, that's why I said we could get meters right
off the mixer on a Raspberry Pi.
It's pretty great.
There's also a link in the show notes
to CNX Software, which got one of these units
and did a series of benchmarks on it.
I don't really know what to take from this.
A couple of temperature benchmarks.
Remember, there's ambient.
There's other things you have to consider.
But at idle, it ran at 62 Celsius.
And then when the benchmarks were being downloaded and loaded,
you know, accessing disk and network, 64 Celsius.
And then when John the Ripper was ripping, 73 Celsius.
And he appears to observe some CPU throttling
as temperatures have gone up.
He kind of describes it as a particular type
of peak performance, bursting performance, I guess.
Yeah, it'll operate perfectly fine.
You don't need extra cooling.
It's designed for sprint performance, right?
You're going to get a few extra workloads done here and there,
but you're not loading it all the time.
If you are keeping the CPUs ramped up,
you'll probably want some extra cooling.
Yeah, yeah.
Otherwise, passive cooling does seem to be sufficient.
And he also did test that gigabit ethernet
and was getting pretty reasonable transfer speeds,
you know, between 95 megabytes a second,
93 megabytes a second on the send and receive,
which is perfectly, perfectly good.
And did run into some 4K video playback issues.
But again, early days on that.
So I'm going to wait and see on that one.
I'm tempted to pick one up, 3D print a little housing
for a USB hard drive, and then just send it
to my parents' house, plug it into the network,
and then just use BitTorrent Sync as a remote endpoint.
Oh, yeah, great little management,
whenever I'm having something in someone's house.
I could definitely see it being a little Sync endpoint,
or a little Nextcloud endpoint, potentially, as well.
Well, we have to move on because, of course, there
was some dramatic breaking news.
Everybody freak out.
Ubuntu is dropping I-386 architecture support.
Oh, no.
Yep, if you are on a Intel series processor,
no longer will there be 32 support.
Steam is pulling out all support for Ubuntu.
And Valve developers are upset.
And Wine developers are, wait, what's that?
It's the Linux gaming apocalypse?
I'm sorry.
I'm sorry, I was just going off an article that was posted
by PC Gamer just 20 hours ago.
Hold on, Wes, just one second.
I'm getting, I'm getting, I've got to get, I'm sorry.
I've got to get pulled aside.
Apparently, there's an update.
The Cone of Silence.
OK, what's going on?
So I'm getting word here that all of this
is a complete overreaction.
Oh, yeah, there's some updates.
Oh.
We should get into them.
Oh, OK.
All right, well, maybe we'll just cut all that out, OK?
All right, let's go.
Yeah, so this week, there was a big upset
when it appeared that Canonical would be removing support
for i36 packages in Ubuntu 19.10, and subsequently, 20.04 LTS.
So kind of the series of events is,
this is something that's been discussed for quite a while.
We covered the timeline on LAN a little bit.
But from what I can kind of see, it
seems like you really kind of see the conversation kick off
around May of 2018.
And we've seen other distributions
make similar moves already.
Yeah, in fact, OpenMandriva just announced they're dropping.
Now, arm architecture, you've got arms keeping 32-bit.
That's not going away.
This is just for the Intel x86 side of the camp.
And so you see conversations.
And the message really seems to be from the Ubuntu developers
is, our sense is, kind of I'm paraphrasing for them,
our sense is that these packages are not well-maintained.
They don't really have a lot of eyes on them.
There is security issues that they're
known to be vulnerable to, like Spectre and Meltdown,
that they're not receiving mitigations
for, amongst other issues.
And we kind of think to protect the overall platform,
moving forward, we should remove these packages
because they're likely a security risk.
Although we can't prove it, they likely are.
Conoco didn't want to come out and scare everyone.
But in their own way, that's essentially
the message they were relaying is, hey, nobody's
really looking at these.
I mean, we're shipping them.
And we're putting the work to package them and make
sure they're built against your current distro.
And we're making sure they're in the repo.
And that's all good, but they might not
be very well-maintained.
And that conversation sort of went off from the mailing list
onto the web and then onto podcasts.
We've talked about it a little bit.
And then this last Tuesday, just after the show wrapped up,
Ubuntu desktop lead developer or manager, Will Cook,
went onto their community Discord and our discourse
and said, here's the plan.
We're going to start officially deprecating support for I36.
A couple of days go by, nobody really
says much except for a few other people
from Canonical chime in on the conversation.
And then on Friday, a staffer from Valve
posts on Twitter that the removal of I36 support
would be untenable and that Steam
would have to look at dropping support for Ubuntu moving
forward in 1910 on.
Yikes.
That was the moment that really set the internet on fire.
Because all of a sudden, it looked like Steam
was dropping support.
So then you immediately had half a dozen articles go live saying
that Valve was officially dropping support for Ubuntu.
And within a short time after that,
a change.org petition was created to force Canonical
to keep 32-bit support in the distribution.
And there was sort of a back and forth
about if this was good or not for the longevity of Linux
desktop.
And by Sunday, Canonical had decided
they would walk it back a bit.
And then on Monday morning, they made an official announcement
that they would be keeping the 32-bit I36 packages for Ubuntu.
They wrote, thanks to a huge amount of feedback
this weekend, that's how they start the blog
post on their Canonical blog.
And then later on, they say, community discussions
can sometimes take unexpected turns.
Ain't that true?
And then they later write, we also
look forward to working with Wine, Ubuntu Studio,
and gaming communities to use container technologies
to address the ultimate end of 32-bit libraries.
And that's important right there.
Yeah.
So I want to continue this conversation.
But I thought maybe what we should do
is not a lot of people are really
discussing what it takes to run a 32-bit x86 program
on a 64-bit x86 Linux system.
So suppose that you have a modern 64-bit x86 Linux system
and that you want to run an old 32-bit program on it.
I do.
Maybe a game or something like that.
It's got some libraries.
So what does it require from an overall system,
both from the kernel and the rest of the environment,
to actually run a 32-bit application on a 64-bit box?
All right, well, at a minimum, that
requires your 64-bit kernel to support programs
running in 32-bit mode and making 32-bit kernel calls.
OK, so there's some kernel stuff.
Oh, yeah, of course.
And supporting those calls is not always easy
because there's all kinds of different structures involved.
And those structures have to be translated back and forth
between the native 64-bit version
and the emulated 32-bit version.
Wow, imagine when you're playing a Windows Proton
game that's 32-bit on Linux.
There's so many layers of translation happening there.
Oh, yes.
And a lot of those, they're not always pretty.
I'm sure there's many in the kernel community who
wish that it would just be kind of simpler.
It'd be cleaner not to do this.
There's more complications around eye octals
and dealing with pseudo terminals
and other sorts of devices like that.
And more complications with the VDSO,
where you have stuff that gets mapped into just
about every program running, even
for statically linked programs that don't necessarily
make use of nice sorts of optimizations
for dynamically linked programs.
System doesn't know, so it's just got to load it.
Yes, and that means the kernel has
to carry around another 32-bit ELF image, which
has to be generated somewhere.
And that's where a lot of this stuff
gets the most complicated, I think,
because you have to get all of these programs.
You have to make all of these shared libraries.
You have to generate and compile all this stuff,
and that's never going to be simple.
Well, and it gets even significantly more complicated
because it's a little simpler if the 32-bit application,
like you were saying, is statically linked.
However, if it's a dynamically linked 32-bit application,
then there needs to be 32-bit libraries all the way down,
and they're all loading all of this extra baggage and crap
all the way down, including if you
have an application that needs to do a name lookup.
There needs to be a 32-bit name lookup library.
Oh, yeah, all those NSS modules you have in nsswitch.com.
Yeah, you've got to take care of those.
There's lots of complications with glibc.
Maybe there's some other libraries
you have, stuff like Curses or X11,
or the standard library for, say, C++.
There's just lots of stuff lurking all over,
and a lot of it might need a 32-bit compiler or toolchain.
Now, in theory, it's often possible
that you can do cross-compilation,
but it's just a lot of infrastructure
that actually has to be supported
to make all of this work at the level
that you expect on a modern Linux desktop.
Well, if you try to file a bug against a 32-bit application
that was built on a 64-bit system,
they'll tell you to go screw yourself.
You're supposed to, if it's going to work in production,
you build a 32-bit app on a 32-bit system.
I mean, it is possible, but in practice, there's problems.
On top of that, many build systems just
don't really even support it, so there's that as well.
So forgive my perhaps ignorant question,
why do Valve care so much about the 32-bit thing?
Is Steam a 32-bit app or something?
Keep in mind, this was an employee at Valve.
This wasn't an official Valve announcement
about a Valve partner.
This was an employee venting on Twitter.
But it's the games.
It's the games.
The games need the 32-bit libraries,
because a lot of the games are 32-bit.
And everybody that has played a handful of Steam games
has definitely run into this.
I mean, I know I have 32-bit libraries on my system
right now just for Steam games.
Canonical's Insight data that they
get from the surveys that people opt into in 1804
shows that less than 3% are getting the 32-bit version
of the ISO.
That's the ISO.
Doesn't tell you how many of them
are 64-bit systems running 32-bit libraries,
but that tells you how many people are going directly
to 32-bit.
It tells you, really, as a whole,
it affects your entire system to run a single 32-bit
application.
We'll have a great breakdown linked in the show notes
if you want to dive into just everything that has to go on.
There's a real technical cost.
There's a genuine technical cost.
And sure, we all have memory to burn,
and we all have processor to burn these days.
But I don't know.
I like my system to be as efficient as possible.
Right.
As always, supporting this stuff,
it's a cost-benefit analysis.
And if people are using it, and it's useful to have,
then you should do it, but it's never free.
Somebody tell me if I'm wrong.
But let's just pretend we live in a world where
internet outrage didn't drive immediate reaction
by corporations.
And Canonical really stuck to their guns on this.
And they said, no, no, no, no.
You don't understand.
We're going to maintain the libraries in 1804.
There'll be ways to get those libraries on 1910.
It's not a big deal.
The libraries aren't changing that much.
We'll be backporting security fixes.
You just get it that way.
What if they had done that?
Because to me, it seems like, because that
was the original proposal, to me, I
see a few ways that problem could have been solved.
We could have had our cake and eat it, too.
We could keep 1910 64-bit pure, and then wrap all this stuff up
in a snap.
One Steam snap that has all the 32-bit libraries you need.
Or how about the flat pack that literally already exists?
There is a flat pack already that you
can install on a pure 64-bit system,
and you can play 32-bit games.
It exists today.
Of course it does.
What about a PPA?
How about a PPA that just installs the 1804 libraries?
Could have done that.
Valve could update the Steam runtime
to just simply include the now missing 32-bit libraries.
They have chosen certain libraries to include already,
but then opted to allow the OS vendor
to supply the rest that they consider to be less important.
Perhaps Valve could reconsider.
It seems like the community does think they're important.
Perhaps they could just package them up
like they've been already packaging a lot of libraries.
So what I don't quite understand,
and I'm genuinely seeking feedback here,
is what was the risk?
It seemed like there was always going
to be a pretty solid solution.
In fact, some that are already working today.
So I don't quite understand what the upset was about,
why we needed a change.org petition,
why we harassed canonical employees over the weekend
about this.
PC Gamer just posted an article this morning,
even after canonical clarified, just
trying to get clicks and drive controversy.
You had people that were trying to get others
to switch distributions.
Come to Manjaro, it'll solve your problems.
I mean, that was all over the place.
And I don't understand why we, as a fairly technical community,
are susceptible to this, because it seems like,
and I guess maybe I'm just idealist,
but it seems like the immediate conversation should be,
all right, well, how do we solve this with technology?
How do we get together as a community
and collaboratively solve this problem?
Right, or at least maybe the beginnings of, hey,
this could cause problems for us.
What are our avenues?
Do you have, how can we migrate our software?
Because usually, and I know this is true for canonical,
people care about others running their software,
and they want to help, right?
You could be a part of this in a positive way
and engage and have help from canonical
to find the best solution.
It's harder to do when you start in such an aggressive tone.
Well, what I don't understand is that people
are so up in arms over this now, where
this is something that's been talked about for quite a while
now.
Yeah, I think it was that developer at Valve's tweet
that sort of, that kicked off the headlines.
Why are we taking faith in this guy's statement
when it's not a direct statement from Valve?
I mean, I've gone to Valve's blog.
They've stated nothing about any of this.
I guess to be fair, there was also
a conversation going on on the wine development mailing list,
too, about some serious, right?
But yeah, again, I come back to there were ways,
there always was going to be ways to solve this.
And at some point, we do need to move forward.
Just for funsies, was it you that
went and dug up the original announcement
about transitioning to 64-bit?
Yeah, way back in the day over at Linux.com.
And it's quaint, because it's written in a way where they
clearly don't think we'll be.
The 32-bit support's like a transitional technology.
These will hang around for a while and then be gone.
Yeah.
Like, no, here we are in 2019.
Yeah, almost like 15 years later.
Yeah.
You do kind of wonder, when is it time to go?
And I know the elephant in the room here
is that Mac OS Catalina drops in the fall, and it's 64-bit only.
And Apple's like, just shut up and make it work.
Deal with it.
And they don't even have the same technical.
They don't have the Snaps, Flatpaks, Docker images.
They don't have Chirrut environments.
We have a lot of ways to solve this problem.
I think what kind of gets me, I'm
going to just take this moment to remind everybody,
currently a very happy Fedora user on my daily driver,
and just moved my server over to Fedora.
However, you have to acknowledge that Canonical takes it
in the face on this stuff all the time.
They move something forward like this
that maybe is a little bit earlier than anybody else,
or because of the size of their user base,
it's genuinely the first time it matters.
They take the brunt of the reaction.
And then in a year when Fedora announces
they're going 64-bit only, crickets.
The day OpenMandriva announces it, crickets.
Canonical's reputation constantly takes it
as they move this stuff forward.
And people love to trot out old wounds like Mirin Upstart.
It even came up on the YouTube comments.
Pulsatio.
Yeah.
And at what point do you go, you have
to weigh the good with the bad.
They make a decision, they listen to the feedback,
and they change it.
That seems to be the balance.
That seems to be the right balance.
And in this particular instance, I
kind of wish they'd stuck a little more to their guns,
because you've got to think about the long-term picture
here.
These distributions, like 2020, the LTS version,
are going to be supported until, what, 2025?
And then and now they're doing 10 years,
so it could be even longer.
So when they commit to shipping these 32-bit libraries
in these LTS distributions, they're
committing to paying staff to maintain those libraries,
even if they import them from Debian once a cycle.
Yeah, that's still a lot of money, time, and effort.
For years.
So they have to cut at some point,
because there is a knock-on effect for years.
A long tail, yes.
A very long tail here.
And somebody's got to do it eventually.
That's what makes it so funny, too.
We had this discussion in such a rapid, quote, unquote,
discussion, in such a rapid-fire manner,
when this is all stuff happening over months and years
of timeline.
So there's plenty of time to figure it out.
The fervor really kind of started Saturday morning,
London time, which is where Canonical's offices are.
Saturday morning.
And they had provided clarification
by Monday morning, PST, Pacific time.
That's a very, very small window of time
to respond, to come to a consensus,
and then issue a statement.
Which is funny, because a lot of people
are getting on them for not responding faster.
What do you expect?
You need to go watch our video on burnout,
because this is getting ridiculous.
Anyways, so yeah, the end result is essentially
they're backpedaling to a degree.
And they do leave a line in the second to last paragraph
in their update, they write, there
is a real risk to anybody who is running
a body of software that gets little testing.
Again, they're trying to tell you in a professional,
non-fearmongering way that they don't think, in their opinion,
that these are safe libraries, that this stuff is not
getting enough attention, and they really
recommend that we stop using it.
And we're kind of forcing them to continue to ship it.
That sucks.
Yeah, it must be pretty frustrating.
We've been thinking about it.
It's not like they rushed into this.
You might say that they did, or that they
could have done some things better,
or communicated more, or whatever.
It really wasn't rushed into you.
It's just silly.
All right.
I know by the time people are listening to this,
it's probably been talked to death.
So if you have thoughts in the Mumble Room you want to chime
in, let's do it in the post-show,
because I know people are probably sick of the topic.
But that's where it stands of us right now, at least.
Any other thoughts?
Yeah.
Maybe just go play some games on your sweet gaming Linux rig.
I'm going to go play some 32-bit games while I still can.
I actually thought it would be kind of a laugh
to start a change.org petition that demanded that Canonical
goes all 64-bit.
Do the opposite.
Get that gross 32 off my system.
Promote it here in the show, and get quadruple
the amount of signatures.
But I thought, you know what?
Canonical's probably just ready to move the F on.
They don't need me trolling them.
I am too.
Yeah, me too.
All right, and with that, let's do a little housekeeping.
We've got some good stuff.
We won't be doing the Friday stream.
I mention that here because a lot of times
the crew here on this show is who's on the Friday stream.
So it's like a sneaky second Linux Unplugged.
But this week we're going to be in Texas,
so no Friday stream this week.
But Friday 8 just came out.
Best thing about a Monday, Friday stream comes out.
Random access memories.
Wes is there.
Drew's there.
Cheese is there, I'm there.
And we share the stories of our very first PCs.
Yeah, it's a retro show.
It's all sentimental.
It's a lot of nostalgia, a lot of fun.
We solve some problems for the world.
Of course we do.
And I won't spoil it, but there is a competition
to give away a game in the show.
And one of the hosts here on the show wins.
We won't say anymore.
We won't say anymore.
Fridaystream.com is a really, really fun episode.
Go check that out.
And then catch up while we have an off week.
Why not, right?
Oh, yeah, that's a great tip.
We're doing it at 1 PM Pacific time.
Check out meetup.com slash Jupiter broadcasting, too.
We have a security meetup coming.
We just had our burnout, understanding burnout,
I should say.
What's actually the name?
The understanding burnout meetup.
And it was really good.
Really good.
Fantastic.
I mean, Drew's got a heck of a file he's got to go through.
But once we get a chance to process it and all that,
we'll get it up on the YouTube channel.
It's a lot, Drew.
I'm sorry.
It's a long one.
But it's so good.
It's really good.
This is an instance where the Mumble Room showed up.
We had just a really great participation.
And just, ah, so good.
And organic, helpful discussions.
We'll have a link to that in a future Linux Unplugged.
Or just keep an eye out at meetup.com slash Jupiter
broadcasting for that.
Also, a little sneaky tip.
This is a little birdie.
This isn't an official feature, but I
think you guys might like to know.
Linux Academy is going to be soft-rolling out
transcripts on the platform over the next couple of days
or the videos.
Killer feature when you're trying to study.
Now, it's just going to be initially in some videos.
They're working out the tech.
You know, they want it to be really solid.
Of course.
I've seen the demos, though.
It's nice.
The implementation is solid.
The tech is good.
But they're going to be measured about how they roll it out.
But a little sneaky little insider tip.
I notice that's rolling out soon.
I know people have asked in the audience.
Yeah, that's nice to have.
It's really nice to have that.
All right.
Anything else we want to mention in the housekeeping?
All right.
I'm calling it then.
So you probably heard in the last few episodes,
if you've been listening, that we
did a hot swap, a live swap, of our free NAS
box to a Fedora box.
At the time, we said, don't do what we did.
But it actually worked great.
So maybe you should.
Jeez, it's been good.
And we'll keep you, you know, if it goes wrong, we'll tell you.
Of course.
Of course.
Oh, my gosh.
It's been so good.
Because we really took the philosophy
of put all of the applications we're running
on this thing in a container.
Is NetData in a container, or is that actually
installed on the host system?
That's on the host.
All right.
So that might be the one app that is actually
installed locally.
But otherwise, it's pretty much just a pure Fedora server
install.
And then we have begun standing up back-end services
that I am already so elated over.
But the one that was sort of number one
with a bullet on our list was replacing Dropbox.
So it's not a, you know, as a business,
it's not the end of the world to pay $1,000 a year for storage.
But it's a big chunk of change.
And it's sizable.
You have to think about it.
It's not on the accounting.
Yeah.
Well, you know, $1,000 a year in hard drives
would be a much better investment, in my opinion.
And this is so silly.
Because now, I disagree with past Chris.
Past Chris is such an idiot, you know?
Future Chris, you don't know about that guy.
But present Chris is really, he's got to figure it out.
I'm hopeful for future Chris.
He's going to come through for us.
Right.
But present Chris, I really feel like he's
got everything figured out.
You know, he really knows what's going on.
And he had came to a realization that the reason why
he was sticking with Dropbox was really
because I just wanted something that was fundamentally
really good file sync technology that I could generate a web
URL so people that don't have an account
or don't have the software installed
could still get access to resources.
It's a combo, right?
It's this everywhere file system that follows you around.
And you can easily get stuff from it whenever you want.
And sort of sneaking up on me, I didn't even
realize what's happening, Wes, is it turns out
like a good mobile app matters.
Yeah, it does.
When I started file sync, it didn't matter.
But now here in the 2019s, I want a good mobile app.
You do stuff on your phone.
And the better it is, the more you can do.
So when Dropbox announced their latest update where
they would be packaging the Electron Runtime
and integrating with Google Docs and Google Sheets
and trying to connect all of your different online systems
together in one activity dashboard
that you could share with your colleagues,
I realized if the dropping of everything but Extended For
wasn't enough, this was it.
It's not your Dropbox anymore.
It's just not a product for me anymore.
It's not.
And I thought Nextcloud wasn't for me
because I didn't want the overhead of a whole website
and all of the collaborative stuff.
I just wanted file sync.
So I was convinced I would go with Sync Thing or Resilio
Sync.
I tried them all out.
And I thought about it from a collaborative space.
And I realized how important some of the mobile app features
were and the web interface was.
And in the end, Nextcloud won out.
So I came to you, Wes Payne.
I said, Wes, bestow upon us a Nextcloud instance.
That's how he talks all the time in real life, yeah.
It's weird.
And Wes, grant us with the performance
that only a dedicated database can provide.
These are the words that I conveyed to you.
And you came back to me with a multi-container setup
on our fake net.
So can you walk us through our setup a little bit?
And then we'll get into everybody else's setup.
Yeah, actually, Nextcloud provides
some pretty handy Docker Compose YAML files ready to go up
in their repositories on GitHub.
We'll have that linked, of course.
So you can get it set up.
There's all kinds of ways.
It's a blessing and a curse with Nextcloud.
There's also a really handy Linux server I.O. image
we also have linked.
And there's a snap.
So there's all kinds of ways to get Nextcloud installed.
Well, to that end, we'll hear from each of those
in a moment.
But you went with the official.
Yeah, I thought we might as well give it a try, right?
See what that road was like.
And we ran into a few snafus, because I
wanted to use Postgres, which is my database of choice.
And we ran into an issue with using
some of the automated setup to sort of populate our stuff,
set up an admin user right out of the box.
Had to do a little tinkering with the database.
Were they expecting you to use a different database?
Well, no, it just seems like that bug
was only affecting Postgres, because you can also
use MariaDB or SQLite.
But we wanted good database performance,
as that's supposedly quite important for doing
heavy file syncing of large media files.
Once we got through that, through all the configuration
stuff, it's working great.
So we have Redis in there.
We have a Postgres container, the actual web container
that has all the app and has PHP FPM running,
and then some proxies in front of it that are handling
lots of encryption, all the rest.
Yeah, and it works beautifully, and the performance
seems to be actually pretty wonderful.
While we're talking the container method, though,
Alex, do you know what would be the advantage of going
with the Linux server IO container over the project's
main container?
This is an aspect of Docker that bewilders me.
So full disclosure, Linux server IO
is a website that I helped found several years ago.
But the reason that I run the Linux server
image instead of the upstream Nextcloud image
is the permissions mapping that the Linux server images do.
So the user that is running inside of a container
will probably have a different UID and GID
to that on the host.
So we allow, through environment variables,
which are specified at runtime, you
can specify a specific user and group
ID that matches that that's running inside the container.
So it just makes permissions a lot more simple.
All right, very good.
A lot of the other things, almost all
of the other things we're running
are Linux server IO images.
Oh, yeah.
There's, I don't know, 10 of them on there.
Yeah, that has been really good.
Well, I expect nothing less, Chris.
Well, that way, I know who to yell at if it doesn't work.
I'm going to treat that.
OK, so Cheesy, you did the snap method, right?
Yeah, I did the snap method in super simple snap install,
setting up your let's encrypt certs.
I'd spoken with Wimpy at Linux Fest Northwest,
and I told him how snaps are just kind of freaky to me
because I'm used to being a little more hands-on
with my setup and going through and editing files
and setting up the configuration and everything.
So it's just weird that literally just one command can
pull it down and get it working.
So that's what I've done.
But one thing that I did notice with that particular snap image
is that it's based on Nextcloud 15.
I wanted to run 16, and you can do that.
It took a little bit of a Google foo getting around
and just reading some of the documentation
there for the snap.
But doing a snap refresh package name,
tac-tac channel equals 16 forward slash candidate,
then you can pull down the latest 16 and give it a go.
You can even pull down edge versions or nightly builds.
Yeah, and you can revert as well if it screws up.
I've played a little bit with the snap as well.
It's pretty nice.
Yeah, it's super painless.
And then what I've done is I've gone through and connected
to some block storage elsewhere so that all of my data
basically resides in that storage
so that if I need to nuke or do something different
with the snap or if I wanted to flip it over
to being a Linux server.io Docker container,
I could do that.
Right, the data's still there.
And my data's still there.
Yeah.
Chris and I, we were talking about how good a time
it is to be running some of these server side
apps on Linux because it's easy to reason about
and it's safe because all of them
have these nice mechanisms for protecting your data.
Yeah, I mean, multiple times yesterday,
we would just completely destroy a container
and then stand it back up and just reattach it to the data.
And it's like nothing changed.
It's just incredible.
Several years ago, I was pretty anti-Docker.
I thought it was going to be a flash in the pan
and a waste of time and just the next kind of noise
in the industry, really.
And then I started using, this was when I was using Unraid,
so it was probably five years ago now.
Docker was like 0.3 or 0.4 or something.
It was a while ago.
And I blew away probably 15 apps worth of configuration
by mistake.
However, it turned out that Unraid
had mapped my volumes to a persistent storage somewhere.
And I hadn't really realized what
I'd done to set it up because I was a complete novice.
I reinstalled Unraid and came back and had all of my apps
exactly as they were within five minutes.
And I'll tell you what, that single thing alone
was just like a light bulb going off
in my head of just to say, yes, containers.
But storing the state outside of the application runtime,
I am 100% on board with this.
This is fantastic.
So Drew, we haven't asked you yet.
So we've got a couple of container-based installs.
We've got a Snap installed.
Have you done a traditional install for your Nextcloud?
Or are you also a container guy now?
Container all the way.
Mine is set up very similar to the way
Wes set up the Studio instance in that I've
got Docker running Nextcloud, Redis, and PostgreSQL all
behind a reverse proxy.
So very, very similar.
Wow.
None of us have done a traditional package
install or anything.
I don't want to.
I mean, especially for a PHP sort of app like Nextcloud,
unpleasant.
I literally never would.
After this experience, I just never would.
And I think my takeaway advice is,
if you're running an instance for a group of people,
like a small office, or a couple, or a family,
the Snap version is really solid.
I've experimented with it, used it myself.
And if you're going for a larger team instance,
like we're using here, then something
with a traditional Postgres database
is a better setup using a container, where
you have a little bit of flexibility,
because it is a fast-moving project that needs updates.
And that's kind of the nice thing,
is we can just track now.
And if things go wrong, we'll just go back.
Go back.
Go back.
Just a heads up for other listeners,
MySQL works just fine as well, if you
prefer that for your database.
There you go.
Yeah.
Yeah.
So there's a lot you can do with Nextcloud besides file sync.
But that's how I came at it.
I've always really been primarily interested in that.
But I started now to look at it more as like a team
collaboration piece of software.
And when I changed my perspective to that,
I think I started, that's really for me
when I started to click.
I started to see in a different light.
And I've been watching how Drew and Cheese
use Nextcloud quite a bit.
You guys have really kind of integrated it quite a bit.
I have a sense maybe you're one of the longer Nextcloud
users on the show.
I'm not sure, though.
I get that sense, though.
Six months, maybe.
OK.
Cheese, how long have you been running Nextcloud now?
Probably three, four months.
OK.
So we're all kind of new to it.
What about you, Alex?
So I guess maybe, Alex, you're probably the longest term.
Over a year.
Before I emigrated, I digitized every single document
in my house.
And I have to say, when I was emigrating across the ocean,
just being able to pull up a scan of this bill
or that piece of ID at a moment's notice on my phone
was so unbelievably useful.
That sounds wonderful.
Yeah.
So when I started looking at it, and we're obviously the newest
because we've only been running it for a week.
So it's still very new to us.
But I've ran Nextcloud on and off for years now.
And now that I'm looking at it, it's
more of like a team collaboration tool.
These apps are starting to make sense.
One that I'm looking into right now
is called Workflow External Scripts.
I'll put a link to this in the show notes.
And this just allows you to pass files through this.
And depending on the type of file and set of defined rules,
it will take different actions.
So I was imagining we could have a Nextcloud drop
folder where this would be set up.
And it would read the file name and then take actions based on
and what type of file it is.
Where it needs to go.
Oh, that's a great idea.
Absolutely something we could use, right?
So that I put in there.
The other one that I installed, but it sounds like maybe
isn't so great, is there's an audio player
that I thought would be great for us
because we have a lot of media content in there.
That's hit and miss.
Doesn't seem to be working, does it, cheese?
Well, I tried out the latest version of that audio player
plug-in or app, as they call it.
And on my particular instance, which
is just a DigitalOcean droplet with one CPU
and two gigs of RAM, there was this crazy runaway PHP FPM
process that I can only associate with that player.
That I guess, I don't know exactly what it was,
but it pegged the CPU at 100%.
So I mean, I think that there are still,
maybe on the particular instance that you have set up,
it might not be a big deal.
You're the guy trashing the server now.
Yeah.
No, that's always me.
I'm always the guy that's just totally soaking up
the CPU.
Yeah, some of these apps are a little hit and miss.
And you do have to kind of check how they went.
Sometimes you have side effects.
There's something that Nextcloud doesn't do by default
that seems sort of necessary for what our use case is,
because we'll often be inviting people in,
is it doesn't have registration on by default.
When you go to Nextcloud, you just
kind of have to have an account or someone
has to have made one for you.
And again, that's nice for like a small team.
Yeah, private, right?
So I turned on the registration app,
and I said, if your email is this domain,
you can register, essentially, is what I did.
And that works.
That's a great little feature for small teams.
And you can make sure it goes into this group.
You can turn on two factors as well.
But here's, I wanted to find something kind of cool
that you could never have in Dropbox.
I turned this on for the whole team, because I'm that guy.
It's called Keep or Sweep.
Keep or Sweep.
It's Tinder for your files, essentially.
When you get bored and you're in Nextcloud,
you hit this little random button that's up in the toolbar,
and it'll just bring up a random file in your Drop,
in your Nextcloud, almost a Dropbox.
It'll bring up a random file in your Nextcloud,
and it will ask you if you want to keep it or delete it.
Oh, that's kind of handy.
And you can sweep it or keep it.
And I sat there, and I went through.
And I started going through some old pictures.
I don't need that picture anymore.
Sweep it.
Yeah, yeah.
Speaking of pictures, I'm looking forward
to when we go to events, turning on the phone auto upload
feature in the app.
And just have it like, as I'm taking pictures at events,
just put them all up on Nextcloud automatically.
But this one is a game changer.
Now, this, I admit, is not applicable to all teams,
or even probably anyone else on our team.
But this one I'm very excited about.
It's PhoneTrack.
People might be familiar that I have a Rover tracker in the RV.
And he likes everyone to know where he is all the time.
I really only turn it on when I'm on trips, typically.
Every now and then, we went somewhere in Oregon,
and I turned it on.
And somebody noticed.
That was kind of cool.
Because it is always up at jupyterbroadcasting.com slash
Rover, at least the tracker is.
But I pay for that.
And it uses a proprietary service.
And it has a hardware device.
You've got like a dongle hanging around you.
So PhoneTrack is an application for Nextcloud
that will generate links that you can plug
into corresponding apps on a phone or hardware device.
And it will supply location data to this Nextcloud
application.
And then it will generate a real time map
using OpenStreetMaps.
And it looks really good.
And you can get history.
You can create an entire tracking session.
You can display location history if you want.
You can check distances and points.
You can display session statistics.
And of course, you can also make it public if you want.
It'll generate a public page that I could make available,
which is kind of neat.
So we wouldn't necessarily have to use the proprietary tracking
service.
We could use this.
That is a great application.
I tried it out.
I loaded an app on the old iPhone
and said allow background tracking.
And sure enough, it's accurate.
It's very accurate.
And I was able to then share the link with the team.
It could be useful, too, though, right?
If you're trying to coordinate or maybe we're
all going to the same meetup or event
and keeping track of one another.
I actually think it'd be a great way for a meetup
to tweet out, here, this is where we're at.
Especially if we were at some sort of large event
where you're moving around.
Oh, right.
Or if I'm on the road.
The main reason I've used the Rover tracker so far
is because people watch where I'm going.
And they send me emails.
And they're like, hey, man, if you're in my town,
stop by and say hi.
Let's go get dinner or something.
And so that's really neat because people
can kind of anticipate where I'm going to go next if they're
looking at the map and stuff.
Right.
I'll be in this in two hours.
Yeah, right.
Now it's just on Next Class.
Just on Next Class.
I think the thing that we have now
is something like it's close to $90 a month for the service.
Shh.
It's meant for fleets that have trucks
that they want to track and stuff.
It's not really meant for the industrial grade solution.
It's not meant for a podcaster in his RV.
Cheesy, you threw in a news app.
Oh, it looks like a feed reader app.
Yeah, and that's exactly what it is.
So I've previously been using an instance of fresh RSS,
basically just to have my own little RSS news reader
that I can log into and pull down feeds from.
Just switched over a couple of days ago to using the news app
that's in Next Cloud.
And it's super solid, really good.
It's a lot quicker than fresh RSS was
as far as updating the feeds.
It will give you kind of like a little notification
if you're on that tab letting you know that it's been updated.
It's a really solid, solid news reader.
I'll tell you where they could improve it.
If they made it so it was a group RSS shared set of feeds
so we could have like a one feed dashboard
that we could go into, that would be great.
The way it is now is each user has their own app data.
Right.
And then another thing I would recommend that an app that you
install, and maybe one of the first,
is the app order app, which will allow
you to reshuffle the icons along the top header.
So you can place those wherever you want.
You can do that from an admin level.
So you can choose what to show all of the users, which
icons to show them, which ones not to show them.
You can reorder them.
Or you can just set it up so that the user basically
has the ability to reorder and show and hide whichever icons
that they want to see.
So it's a really nice little feature
that I'd like to see built in.
Another one is right click.
At first, whenever I installed Nextcloud and I got it going,
and I couldn't right click and get that context menu,
it kind of drove me crazy.
So I would highly recommend that you also install the right
click app.
Right click is pretty much necessary in 2019.
So I have a couple of recommendations as well.
One of them is a two factor authentication for login.
So that supports like Google Authenticator and Authy
and all that kind of stuff.
As I mentioned, I've got a lot of personal information
in this one.
So I wanted to make sure it was safe and secure.
The other one is QoNotes.
I don't know if you've used this desktop app.
I think it's Qt app that allows you to take notes.
And it allows me to use Nextcloud to sync my notes
like I used to use Evernote to do.
I think it's text only.
But I mean, that's good enough for most things.
Supports markdown.
Yeah, it supports markdown.
And yeah, that's all I need.
So QoNotes API is the name of the plug-in for Nextcloud.
Yeah, we should probably set that up.
I love QoNotes.
That's a good one.
Drew, you got any pro Nextcloud tips?
Yes.
So you can actually use it as a full PIM suite PIM,
personal information management.
It does have a mail client and a calendar.
And you can even get your contacts in there
if you want to set it up so that you have all of your stuff.
Is it good though?
It's OK.
It's not great.
It's not going to be challenging Gmail and Google Calendar
anytime soon.
But as far as a simple implementation
of each of those things, it's all right.
So really, if you're going the route of Nextcloud
because you want the enhanced privacy
aspect of self-hosting it and not having somebody controlling
your stuff, and say you also have your own mail server,
this is a good way to have a web-based interface
to your various services like mail, contacts, calendars
that isn't being snooped on.
So using it that way, you can get a little more off the grid,
as it were.
I have one more pro tip, if I may.
And that is something called Rclone.
Now, Rclone allows you to mount a directory using
Fuse on your Linux system.
And then use Google Drive or any other kind
of quote unquote limitless back end.
So if you wanted to host Nextcloud on a droplet,
for example, it's only got 20, 30 gig of disk.
Without paying for digital ocean spaces or something,
you could use your pre-existing Google Drive account.
And it's all encrypted.
Rclone handles the encryption.
And it's completely transparent to Nextcloud using Fuse.
So that's another top tip.
Nice.
My newbie tip then would be take a look
at the desktop client's Selective Sync options.
And I'm very happy to report that I
feel like they've given really good data on the sync process.
In fact, much better data than Dropbox gives.
Really?
Yeah.
I'm really happy with the desktop sync information.
You were skeptical of sync.
Very happy.
And for us, because we have archival information,
we have large productions that some people just
aren't even involved with, Selective Sync
is such a killer feature.
And I'm very happy to report that it seems to be working
quite well.
So my early tips are to play around
in the settings of the Nextcloud client and take a look at that.
I'm excited to switch to this already.
Yeah, I've moved three or four folders out of Dropbox already.
That's how far I've gone.
And then I feel like after the company event,
we're going to just do the big switch.
Another quick tip related to that,
for any GNOME users out there, GNOME Online Accounts
does support Nextcloud.
So you can add it, and it shows up as a file system
in your files app, Nautilus, on the left-hand side,
where you can click into it and directly access
all of your folders, not even the ones that are just synced.
And it just grabs everything over web dev.
Yeah, so you don't even have to.
If you just need a couple of files,
you don't need to bother with sync,
which is probably a fair number of people on the team, really.
Always on connection.
Yeah.
Oh, it's so great.
Yeah, I really think that's pretty neat.
We've been experimenting with all kinds of different ways,
too, to make sure that, like, I have a couple of automation.
I don't want to go into a lot of details,
but I have a couple of automation things
that kick off when certain things enter folders right now
in Dropbox.
And Wes and I yesterday were experimenting
with replicating that Nextcloud with the extra layer
of challenge that you then have to mount that path to where
that is in Nextcloud into the Docker container.
Like, there was a little extra layer
of work we had to do there to kind of figure that out.
So there's some instances like that.
We're still sorting a couple of things out.
But I think for us, the other area we're going to use this
out of, and I don't even know how you do this yet,
is there's that feature where you can request
somebody sends you a file.
And we're going to use that for production purposes.
When we record, we can then send a guest a link,
say, hey, please upload what you just recorded to us here.
Give me those bytes.
And otherwise, we were actually just,
we were discussing building that.
We'll just, I guess we'll just build this ourselves for free,
because that'd be great.
And then we look at the Nextcloud and go, oh, yeah,
like, that's just a feature built in.
It's pretty nice.
Hey, Chris.
I don't know if you remember, but for our Portland trip,
I sent you a link like that, and you gave me all your photos.
Yeah.
All the photos.
That's incredible.
It was awesome.
It was the first time I'd used it.
It worked really well for us.
I was going to ask you, have you been finding success there?
Do you sync a lot of photos?
You're like, what's the, like that?
A lot of little files, big files, that kind of.
What's the Brent breakdown?
Yeah, what's the Brent experience?
Yeah, so I've been recently moving towards Nextcloud
for syncing photos to clients as like a method
of delivery because almost everything I
produce in my photography stuff is digital,
as you might imagine.
So I've been testing that a little bit,
and it's been working really, really well for clients.
I've gotten no bad feedback, and it has mostly the features
I need, not all, but mostly.
And it's worked great.
I haven't had anybody complain about it.
I think after the last week, Cheese and I
were talking about raw support for giving previews of raw
photos.
That's something he ran into as a problem,
but I don't ever serve raw photos sort of to a public place.
So it was fine for me.
But it's been super solid.
I've been running it for a year now,
and also moved all of my contact and calendar
syncing from the phone to the computer.
Wow, you must be feeling good about it.
Oh, yeah, that I've been doing for a few months now.
And I was testing it alongside the Google services,
just because I think, as you learned last time,
you tried Nextcloud, you should sort of test it for a while.
And sometimes you run into some strange bugs.
But it's been really, really solid.
And I've been loving it that all of my syncing of almost
everything is in the same place.
So one log in, and I get files.
I get calendars.
I get contacts.
I get notes.
I get all that stuff.
So I would encourage you to consider that as well.
It's been really powerful.
And I've helped a bunch of people move to that.
And they've loved it.
That does sound appealing, especially because Hedi and I
right now are trying to sort out,
what's the best way for us to share that kind of information?
Nextcloud, dude, definitely is.
Brent, there is a Camera Raw plugin
that you can install, which I have used successfully
for a while.
Yeah, it requires an iMagic plugin that has been deemed.
Some people said it wasn't so secure.
So I would love to get your take on that at some point.
But that's for that reason I considered not going for it.
I didn't know that.
I just installed it.
Honey badger.
I didn't care.
Ironic honey badger.
What's your nextcloud address again?
Yeah, let's give that out on the air.
Yeah, let's do that.
Can we talk about business applications
for just a moment?
Please do, sir.
So there are a couple other cool things
that you can do with it.
There is the ability to white label,
where you can put in your own logos, your own color
themes, and all of that.
JBcloud.
That's right.
And I want to do that.
I definitely want us to do that.
There are some other neat features in the app store,
like a DICOM viewer for people working in HIPAA sites,
that actually works.
I don't know if you guys have ever had to deal with DICOM
files, but essentially they're files that are typically
somewhat encrypted.
And you end up shipping them on a CD from one hospital
to another.
And then they've got this included viewer
that only runs on Windows.
And some of them only run on Windows XP.
Like for x-ray images and whatnot?
Yeah, I think I have.
It's been a long time.
So for HIPAA, where you are trying
to self-host all the things as much as possible,
Nextcloud is a really good option.
OK, so now, Brent, you brought it up, Federation.
I got to ask how Federation is going
to fit into our future setup.
Because I envision, and I want to get your feedback
on this, all of you, I envision our on-premises Nextcloud box
being like a source of authority for all of the data,
like terabytes of data on this thing.
But then it seems like the more frequent access data,
the less historical stuff, should be available up
on a droplet or something that's fast and available
for everyone.
And I'm wondering if that's where Federation plays
into this, or how Federation plays into this.
And if, maybe down the road, I could
have a setup where some data is on some Nextcloud servers
and the rest of it's on another, how that could work?
Or do you get where I'm going?
Is there some sort of future where
I could have some sort of glorious on-premises cloud
hybrid Nextcloud setup?
I think the future is here, Chris.
Is it?
Have you played with Federation?
So I have two servers, because I basically
wanted to play with Federation, but for a few other reasons.
So I have two servers that I am running
with a few different users on each individual server.
And so that way, I can have a certain URL
when I share files to clients, for instance,
and then another sort of internal URL, if you will,
that contains almost all of my internal files.
So you can see it another way, which
is I have one for business and one for personal.
Oh, that's a good idea.
It's been really nice, because I can have certain content
in one place, and I can share it to myself from one server
to the other.
I've been really impressed with the Federation.
It isn't instant when you do a share,
but it's pretty darn close.
And so I would say if you've got that sort
of decentralized notion in your mind of where you want to go,
I'd say go ahead and try it.
It's been super solid and really easy to share files.
They have sort of a nomenclature that's
easy to understand for sharing files
between different servers from user to user.
And I say go full steam ahead.
Couldn't be easier to test that kind of thing in Docker,
though.
You just spin up another container and test it out.
That's true.
And we have a system up on DigitalOcean
that we kind of use for those very kinds of purposes.
So we could definitely try it.
And I think I could also see myself setting up a personal
next cloud, probably on DO.
I don't need nearly as much storage.
If I separated my personal stuff from all the JB stuff,
I'd probably need a couple of gigs at most,
because it's mostly documents and backgrounds that I like.
It's all the boring stuff, yeah.
Don't forget that high res photo sync, though.
That does eat up a lot of space.
It does, yeah.
But I'm going to be sending those to the JB one,
because my intention there is to just turn that
on while I'm at events, and then turn it off after.
Because otherwise, I've got built-in platform solutions
for syncing the photos and backing them up.
But I guess I feel like one of the things that
has made me feel like this is the time to go with NextCloud,
besides just the project getting more mature,
and Dropbox getting crappier, is the setup
that we now have, where we have at multiple layers what I feel
like are solid sketch hatches.
If the container goes bonkers, we can go back,
or we can switch to a different container.
It's isolated from the config and the data.
The data is not the application.
The two things are not at all intertwined,
and they're completely safe to separate and move
independently.
And then the operating system, also, as we have now proven,
can be swapped out to anything that supports ZFS.
I mean, we have gone from a FreeNAS install that
was maybe one release behind or current, and went to Fedora,
and were able to just successfully mount those CFS
pools.
And did we end up doing these command to bring it up to date?
We've not upgraded the pool to the latest stuff.
Yeah, we did look, though.
There is an option to essentially upgrade
the pool to the most current, but then there's
no going back to FreeNAS if we do that.
Does Alan know that you've dropped BSD?
I haven't really said anything, no.
I'm pretty confident that at this point, if for some reason
we were like, oh, screw Fedora.
It's so horrible for some reason,
even though we've been running it on other systems on DO
now for a while, I'm pretty confident we
could put Ubuntu LTS on there, spin those containers up,
and mount that ZFS pool, and we'd be back in business
in an hour.
And so at each, all these different levels,
we're abstracted and we're safe.
There's like, next cloud can screw up, we're good.
The ZFS pool can have problems.
It doesn't mess it.
And at the same time, it's not making it
like it's some horrible thing.
In fact, it's easier to manage, if anything.
It is.
It's way easier.
It's great.
I'm giddy with how simple it is, really.
Once you've gotten the most basic Docker understanding,
you compose files that are easy to read,
and the Docker command line is very simple.
Or you can use, even on top of that, you can use cockpit.
And it has a front end, a graphical front end.
And if you don't need to do anything fancy, it's fine.
I'm so pleased you guys are doing this.
It's been the way I've been running my servers now
for two, three years, or whatever.
Just 100% Docker containers for everything
I can get my hands on.
And it just makes life so, so easy.
It's great.
I guess I feel like, I don't know,
a lot of times when you do something like this,
you've got to compromise.
You've got to give something up when
you go with the free solution that's self-hosted.
But this time, it's like, I'm gaining a bunch of stuff,
and I'm going to be saving money.
And I also got privacy that I didn't have before.
So I've gained on every front.
I think the application is better.
The suite of tools is better.
The platform it's on is better.
And it's private.
And, oh, by the way, it's under my own roof.
And it's kind of fun.
Yeah, and it's really fun, too.
That, too.
Because it's not a huge hassle.
Hell yeah.
That, too.
I just, oh.
This is just, I love open source.
I love free software because this is the right.
This is when it's at its best.
It really is.
So good.
So I really encourage you to go out there and check it out.
However you want to install it, it's
worth the time to just play around with it a little bit
and see if maybe it would work for you.
Because I was resistant, legitimately.
And I'm just really happy we gave it a go.
And maybe some of our fine audience
have some tips we should be hearing
about how they have their Nextcloud instances set up.
How do they tell us, Wes?
Oh, linuxunplugged.com slash contact.
Absolutely.
Hey, by the way, next week, we're
going to have a special on PCI pass-through for virtualization
and our general VM setups, how we do them,
and maybe some of Wes's tricks for running operating systems.
We decided to just make one authoritative episode
that we can refer people to.
So it's going to be our comprehensive setup
on how we do virtualization.
I have been all in on PCI pass-through and eGPUs.
And so has Drew.
And I feel like we need one definitive spot.
And so while we are traveling, we thought,
let's go ahead and release a special.
So we're going to do a PCI pass-through special next week.
So make sure you grab that.
Should be really good.
All right, Mr. Payne.
Well, I'm so excited I could talk about Nextcloud
for another hour.
But we should probably get to these pics,
because they're crazy good.
And one of them is rather relevant to us that you found.
And I love the way they talk about this project.
So it's called Lexicon.
And here's how it describes itself.
Manipulate DNS records on various DNS providers
in a standardized way.
It's just, what does that mean, Wes?
Well, OK, I think Lexicon is actually a great name, right?
Because we're dealing with names.
We have all these different terms.
DNS helps you look things up.
Unfortunately, different providers
do things in different ways.
Some have very nice APIs.
Some have complicated APIs.
Some have unofficial undocumented APIs.
All of that's been combined in Lexicon,
which is a handy Python library that has
a great little command line.
So it was built with Let's Encrypt in mind, primarily,
as well as being a general utility.
So you can do stuff like do their DNS challenge,
where you have to go add a specific thing to your DNS
update.
With Lexicon, you can do that programmatically.
Yeah, and it supports a ton of providers.
Route 53, Cloudflare, Hover.
Hover, DigitalOcean, EasyDNS, GoDaddy, obviously,
Google Cloud DNS.
I mean, Linode, just essentially all of them.
I'm looking at namecheaps on there.
All of the ones I've ever used are on there,
including ones I haven't.
So in theory, as long as it's on this list,
regardless of who your DNS provider is,
it's the same set of command syntax
to update DNS records, modify DNS records.
Yeah, there's a few little variances here and there
as they're acquired by the platforms.
And it's one PIP package in Python,
but a few of them have a few extra packages because they
have extra dependencies, maybe.
So look into the details for your specific provider.
But in our case, we were doing some stuff with Hover.
Really easy.
You just had to go.
You could get your credentials, pass that in.
They have a couple different options for how you do it.
And then you can make queries and stuff.
So we needed a dynamic DNS updater
now that we're using a whole bunch of stuff at the server,
obviously, here in the studio.
We want to know where it is when we're on the road.
That made it really easy.
I just wrote a little batch script.
You could go and query Hover through lexicon
to find out what the current IP address is.
And you can easily delete it or change it.
And does that just become like a script that gets cronned or?
Yeah, I stuck it in a Docker container on the server here.
And then it's just running all the time.
Of course it is.
All right, well, how about something
you're not going to run in a Docker container this week?
It's called Arandr.
I realized that I never really followed up on how I solved
this super obnoxious problem I had,
where I had two vertical screens on the side
and a horizontal screen in the middle.
So I have three ASUS 27-inch monitors,
two of which are vertical, one of which is standard 16
or whatever it is.
You get what I'm saying.
And I have this really obnoxious problem
that has plagued me across all the different desktop
environments.
And I eventually just decided I had
to come up with a solution.
I thought the desktop environments would solve it.
I thought maybe switching to an AMD GPU from an NVIDIA GPU
might solve it.
A dramatic switch, by the way.
It did not solve it.
In fact, now it just manifests in different, weirder ways.
So I had to make a script.
And Arandr or Arandr or whatever you.
A-ran-der.
Yeah.
It's essentially a front end to X-randr,
which is a way to script the layout of your screen, windows,
et cetera.
And the nice thing about this is it just
looks like a regular display manager window
where you arrange your monitors like you
would in a multi-monitor setup.
And then you hit Save.
And it generates the script for you.
I put that into a desktop launcher.
That's in my menu bar.
As soon as I wake up my screens, if they're screwed up,
I click that within a second.
Everything's laid out perfectly.
You know, it's funny.
I discovered this a few years ago
when I was still running Unity.
And then I had three screens on an NVIDIA GPU
over, like they were DisplayPort daisy-chained.
And it was just the worst.
I had to deal with fighting with the built-in desktop
environment display manager.
It just wasn't as good.
Plus, as you say, you can easily add it to startup scripts
or, you know, just restore scripts.
Yeah, didn't you just use it recently here in the studio?
Oh, yeah.
It's on the Reaper machine now, because I
wasn't happy with how XFCE was doing it.
Yeah, we have like a stand-up, the jack system,
patch everything in, because we have so many things.
And so Wes created a script to standardize it.
And part of that is you just run this so that way,
every time the monitors, you just
know they're always laid out correctly.
Exactly.
Yeah, it's so nice, because you can run this once,
and then it solves the problem for you forever.
And I don't know if you've ever experienced some of the ones,
I think, in the Gnome Heritage, in particular,
where the screens just don't move where you tell them.
Yes.
It doesn't happen, right?
It's just it's the perfect amount of simplicity.
It's not flashy.
It gets the job done.
I will have a link in the show notes.
I encourage all of you, at least as long
as we're still running X. That was one of the stories that
did not make it in, but I want to talk about soon,
is we now kind of have a end-of-days date for X.
Maybe we'll try to cover that next episode,
because that's kind of a big deal, too.
And maybe we should spend some time,
switch totally over to Wayne.
Let's see how, you know, evaluate.
Yeah, we really should, especially
since we've got the ThinkPads with the full Intel set up.
Yes.
We really could.
I have done it a little bit.
The issue is, once I switch back to XFC,
I basically kiss that goodbye.
But you know, it's not bad to check in on Gnome or GATEV.
I think I need to.
Yeah, and we could do that.
We could definitely do that.
Well, I want to say thank you, everyone, for listening.
We really appreciate you listening to these shows,
sharing them with your friends, sending in feedback.
Or even if you've just been a longtime, silent listener who's
never said anything, we're thinking about you,
and we appreciate you.
We do this for you.
That's the whole reason, right?
Yeah.
We very much appreciate your time
and choosing to listen to us, so thank you very much.
We will not be live next week, because I'll
be traveling back.
I'll be doing a road trip back from Texas,
so we'll have the prerecorded PCI pass-through episode.
But we'll be back after that, the next Tuesday,
the following Tuesday.
We do it at 2 PM Pacific time, but you
can go to jupiterbroadcasting.com.
I'm at Chris Ellis.
The network's at Jupiter Signal.
Thanks for being here.
See you next Tuesday.
I'm Chris Ellis, and I'll see you next week.
All right, jbtitles.com, let's go boat.
I'll tell you where else I use A Render,
and that's when I'm switching between two virtual machines
with PCI pass-through.
I can use that script on a hotkey
to rearrange my monitors to have just one as Windows 10
or just one as Linux.
That's a nice way to do it.
Yeah, hotkeys are a good way to go, too.
I should hotkey it.
I don't know why I don't.
I should, I really should just hotkey that.
Control-F10 is now Windows 10, so I mean, it just makes sense.
That way, when you try it at Wayland again,
you'll have to figure out how to make that work.
You really are in this now.
It's the future, Chris.
I use XClip and A Render.
I'm heavily all in on X.
Yeah, I know.
I know when I was reading this post about sort of the plans
for wrapping up Xorg, I was thinking to myself,
well, I don't know if I'm ready.
I don't know if I'm ready, but you know.
You're going to be installing the weird 32-bit Xorg snap,
and that'll be your system.
I'll have XWayland 32-bit.
No, but seriously, though, you joke,
but like we were talking about with the 32-bit stuff,
at some point, you do have to make a transition.
And like it or not, we're running
an engineer's workstation operating system here.
We're not really running Mac OS for end users, right?
And so sometimes the stuff's a little bit of a pill
to swallow, but that's sort of the agreement,
I think, when you run desktop Linux, like it or not.
Maybe I'm wrong, but it sure seems to be the case.
See the upside if we do some playing around there.
That's a good excuse to play with pipeline again, too.
Yes, yes, yes.
I like the way you're thinking, Wes Payne.
We're going to have to make that happen.
What are you thinking?
Are you thinking like Fedora?
Yeah, that's what I was thinking, yeah.
How'd you know?
Oh, I know.
